{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3156a16",
   "metadata": {},
   "source": [
    "## Introduction to Text Mining and NLP: Term Paper\n",
    "\n",
    "---------------------------\n",
    "Ramón Talvi\n",
    "\n",
    "David Vallmanya\n",
    "\n",
    "Irene Villalonga\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc928607",
   "metadata": {},
   "source": [
    "In this term paper we will use Reddit's API to extract a huge number of users opinions in response to the prompt \"Donald Trump has not made a single lasting positive impact on the USA during his term as president\", which is included in a very popular reddit community called \"Change my views\". \n",
    "\n",
    "The project aims to analyze how the public, as represented by Reddit comments, perceive the performance of Trump's presidency in various political areas such as the economy, foreign affairs, COVID, gender, etc. The goal is to propose a logistic model that measures the relationship between a given political topic (for instance, gender) and the overall sentiment of the public on that specific topic: the idea is to measure Trump's perceived presidency performance on different political areas (topics). The model would use the topics as a covariates and the overall sentiment as the dependent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d0387",
   "metadata": {},
   "source": [
    "### 1) WEB SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2e828890",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseException",
     "evalue": "received 401 HTTP response",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResponseException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[165], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m submission \u001b[39m=\u001b[39m reddit\u001b[39m.\u001b[39msubmission(\u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39miq41dt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m submission\u001b[39m.\u001b[39mcomment_sort \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbest\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m submission\u001b[39m.\u001b[39;49mcomments\u001b[39m.\u001b[39mreplace_more(limit\u001b[39m=\u001b[39m\u001b[39m1500\u001b[39m)\n\u001b[1;32m     22\u001b[0m count_comments\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[39mfor\u001b[39;00m comment \u001b[39min\u001b[39;00m submission\u001b[39m.\u001b[39mcomments:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/praw/models/reddit/base.py:34\u001b[0m, in \u001b[0;36mRedditBase.__getattr__\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the value of ``attribute``.\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m attribute\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetched:\n\u001b[0;32m---> 34\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch()\n\u001b[1;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attribute)\n\u001b[1;32m     36\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m     37\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m object has no attribute \u001b[39m\u001b[39m{\u001b[39;00mattribute\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/praw/models/reddit/submission.py:634\u001b[0m, in \u001b[0;36mSubmission._fetch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fetch\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch_data()\n\u001b[1;32m    635\u001b[0m     submission_listing, comment_listing \u001b[39m=\u001b[39m data\n\u001b[1;32m    636\u001b[0m     comment_listing \u001b[39m=\u001b[39m Listing(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reddit, _data\u001b[39m=\u001b[39mcomment_listing[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/praw/models/reddit/submission.py:631\u001b[0m, in \u001b[0;36mSubmission._fetch_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m name, fields, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetch_info()\n\u001b[1;32m    630\u001b[0m path \u001b[39m=\u001b[39m API_PATH[name]\u001b[39m.\u001b[39mformat(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfields)\n\u001b[0;32m--> 631\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reddit\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m, params\u001b[39m=\u001b[39;49mparams, path\u001b[39m=\u001b[39;49mpath)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/praw/util/deprecate_args.py:43\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     arg_string \u001b[39m=\u001b[39m _generate_arg_string(_old_args[: \u001b[39mlen\u001b[39m(args)])\n\u001b[1;32m     37\u001b[0m     warn(\n\u001b[1;32m     38\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPositional arguments for \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m will no longer be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m supported in PRAW 8.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCall this function with \u001b[39m\u001b[39m{\u001b[39;00marg_string\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     40\u001b[0m         \u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m     41\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     42\u001b[0m     )\n\u001b[0;32m---> 43\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mdict\u001b[39;49m(\u001b[39mzip\u001b[39;49m(_old_args, args)), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/praw/reddit.py:941\u001b[0m, in \u001b[0;36mReddit.request\u001b[0;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[1;32m    939\u001b[0m     \u001b[39mraise\u001b[39;00m ClientException(\u001b[39m\"\u001b[39m\u001b[39mAt most one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    940\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 941\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_core\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    942\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    943\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    944\u001b[0m         json\u001b[39m=\u001b[39;49mjson,\n\u001b[1;32m    945\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    946\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    947\u001b[0m         path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m    948\u001b[0m     )\n\u001b[1;32m    949\u001b[0m \u001b[39mexcept\u001b[39;00m BadRequest \u001b[39mas\u001b[39;00m exception:\n\u001b[1;32m    950\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/prawcore/sessions.py:330\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[1;32m    328\u001b[0m     json[\u001b[39m\"\u001b[39m\u001b[39mapi_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    329\u001b[0m url \u001b[39m=\u001b[39m urljoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_requestor\u001b[39m.\u001b[39moauth_url, path)\n\u001b[0;32m--> 330\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request_with_retries(\n\u001b[1;32m    331\u001b[0m     data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    332\u001b[0m     files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    333\u001b[0m     json\u001b[39m=\u001b[39;49mjson,\n\u001b[1;32m    334\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    335\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    336\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    337\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    338\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/prawcore/sessions.py:228\u001b[0m, in \u001b[0;36mSession._request_with_retries\u001b[0;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[1;32m    226\u001b[0m retry_strategy_state\u001b[39m.\u001b[39msleep()\n\u001b[1;32m    227\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_request(data, method, params, url)\n\u001b[0;32m--> 228\u001b[0m response, saved_exception \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    229\u001b[0m     data,\n\u001b[1;32m    230\u001b[0m     files,\n\u001b[1;32m    231\u001b[0m     json,\n\u001b[1;32m    232\u001b[0m     method,\n\u001b[1;32m    233\u001b[0m     params,\n\u001b[1;32m    234\u001b[0m     retry_strategy_state,\n\u001b[1;32m    235\u001b[0m     timeout,\n\u001b[1;32m    236\u001b[0m     url,\n\u001b[1;32m    237\u001b[0m )\n\u001b[1;32m    239\u001b[0m do_retry \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    241\u001b[0m     response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m codes[\u001b[39m\"\u001b[39m\u001b[39munauthorized\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    243\u001b[0m ):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/prawcore/sessions.py:185\u001b[0m, in \u001b[0;36mSession._make_request\u001b[0;34m(self, data, files, json, method, params, retry_strategy_state, timeout, url)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_make_request\u001b[39m(\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    175\u001b[0m     data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m     url,\n\u001b[1;32m    183\u001b[0m ):\n\u001b[1;32m    184\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rate_limiter\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m    186\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_requestor\u001b[39m.\u001b[39;49mrequest,\n\u001b[1;32m    187\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_header_callback,\n\u001b[1;32m    188\u001b[0m             method,\n\u001b[1;32m    189\u001b[0m             url,\n\u001b[1;32m    190\u001b[0m             allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    191\u001b[0m             data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    192\u001b[0m             files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    193\u001b[0m             json\u001b[39m=\u001b[39;49mjson,\n\u001b[1;32m    194\u001b[0m             params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    195\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    196\u001b[0m         )\n\u001b[1;32m    197\u001b[0m         log\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    198\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mResponse: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mcontent-length\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m bytes)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m         )\n\u001b[1;32m    201\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/prawcore/rate_limit.py:33\u001b[0m, in \u001b[0;36mRateLimiter.call\u001b[0;34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Rate limit the call to request_function.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[39m:param request_function: A function call that returns an HTTP response object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelay()\n\u001b[0;32m---> 33\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m set_header_callback()\n\u001b[1;32m     34\u001b[0m response \u001b[39m=\u001b[39m request_function(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     35\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(response\u001b[39m.\u001b[39mheaders)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/prawcore/sessions.py:283\u001b[0m, in \u001b[0;36mSession._set_header_callback\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_header_callback\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_authorizer\u001b[39m.\u001b[39mis_valid() \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_authorizer, \u001b[39m\"\u001b[39m\u001b[39mrefresh\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     ):\n\u001b[0;32m--> 283\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_authorizer\u001b[39m.\u001b[39;49mrefresh()\n\u001b[1;32m    284\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mAuthorization\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbearer \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_authorizer\u001b[39m.\u001b[39maccess_token\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m}\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/prawcore/auth.py:314\u001b[0m, in \u001b[0;36mDeviceIDAuthorizer.refresh\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m     additional_kwargs[\u001b[39m\"\u001b[39m\u001b[39mscope\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_scopes)\n\u001b[1;32m    313\u001b[0m grant_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://oauth.reddit.com/grants/installed_client\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 314\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request_token(\n\u001b[1;32m    315\u001b[0m     grant_type\u001b[39m=\u001b[39;49mgrant_type,\n\u001b[1;32m    316\u001b[0m     device_id\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_device_id,\n\u001b[1;32m    317\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49madditional_kwargs,\n\u001b[1;32m    318\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/prawcore/auth.py:155\u001b[0m, in \u001b[0;36mBaseAuthorizer._request_token\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    151\u001b[0m url \u001b[39m=\u001b[39m (\n\u001b[1;32m    152\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_authenticator\u001b[39m.\u001b[39m_requestor\u001b[39m.\u001b[39mreddit_url \u001b[39m+\u001b[39m const\u001b[39m.\u001b[39mACCESS_TOKEN_PATH\n\u001b[1;32m    153\u001b[0m )\n\u001b[1;32m    154\u001b[0m pre_request_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 155\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_authenticator\u001b[39m.\u001b[39;49m_post(url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdata)\n\u001b[1;32m    156\u001b[0m payload \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m payload:  \u001b[39m# Why are these OKAY responses?\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/prawcore/auth.py:38\u001b[0m, in \u001b[0;36mBaseAuthenticator._post\u001b[0;34m(self, url, success_status, **data)\u001b[0m\n\u001b[1;32m     30\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_requestor\u001b[39m.\u001b[39mrequest(\n\u001b[1;32m     31\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     32\u001b[0m     url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     headers\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mConnection\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mclose\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m success_status:\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mraise\u001b[39;00m ResponseException(response)\n\u001b[1;32m     39\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "\u001b[0;31mResponseException\u001b[0m: received 401 HTTP response"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from praw.models import MoreComments\n",
    "\n",
    "secret_token = \"\"\n",
    "client_id = \"\"\n",
    "username = \"\"\n",
    "password = \"\"\n",
    "\n",
    "reddit = praw.Reddit(client_id=client_id,\n",
    "                     client_secret=secret_token, password=password,\n",
    "                     user_agent='MyBot/0.0.1', username=username)\n",
    "\n",
    "df = []\n",
    "\n",
    "submission = reddit.submission(id='iq41dt')\n",
    "\n",
    "submission.comment_sort = \"best\"\n",
    "submission.comments.replace_more(limit=1500)\n",
    "\n",
    "count_comments=0\n",
    "\n",
    "for comment in submission.comments:\n",
    "\n",
    "    if isinstance(comment, MoreComments):\n",
    "        continue\n",
    "\n",
    "    if comment.stickied == False:\n",
    "\n",
    "        c_author = comment.author\n",
    "        if c_author == None: \n",
    "            c_author = \"NA\"\n",
    "        else:\n",
    "            c_author = c_author.name\n",
    "\n",
    "        c_replies = 0\n",
    "        for reply in comment.replies:\n",
    "            c_replies += 1\n",
    "\n",
    "        c_awards = 0\n",
    "        for award in comment.all_awardings:\n",
    "            c_awards += 1\n",
    "\n",
    "        df_comment = pd.DataFrame({\n",
    "            'post_id': submission.id,\n",
    "            'comment_id': comment.id,\n",
    "            'subreddit': submission.subreddit.display_name,\n",
    "            'post_title': submission.title,\n",
    "            'post_ups': submission.ups,\n",
    "            'post_upvote_ratio': submission.upvote_ratio,\n",
    "            'post_num_comments': submission.num_comments,\n",
    "            'post_time': datetime.datetime.fromtimestamp(submission.created_utc),\n",
    "            'post_text': submission.selftext,\n",
    "            'post_downs': submission.downs,\n",
    "            'post_score': submission.score,\n",
    "            'post_awards': p_awards,\n",
    "            'comment_awards': c_awards,\n",
    "            'comment_author': c_author,\n",
    "            'body': comment.body,\n",
    "            'comment_replies': c_replies,\n",
    "            'comment_ups': comment.ups,\n",
    "            'comment_downs': comment.downs,\n",
    "            'comment_time' : datetime.datetime.fromtimestamp(comment.created_utc),\n",
    "            'comment_score': comment.score\n",
    "        }, index=[count_comments])\n",
    "\n",
    "        df = pd.concat([df, df_comment])\n",
    "\n",
    "        print(\"Post: \"+str(count_posts) + \"    Comment:\"+ str(count_comments) + \"    Inpost comment:\" + str(i))\n",
    "\n",
    "        count_comments += 1\n",
    "\n",
    "df.to_csv(\"cmv_extract.csv\", sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad99a606",
   "metadata": {},
   "source": [
    "### 2) PRE PROCESSING \n",
    "In this section we are going to first load and clean the Data Frame. Then we will carry out the usual steps in Text Mining preprocessing: tokenization, lemmatizing, remove punctuation, unify, remove stopwords, stemming. Finally we will obtain the Document Term Matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c74f9480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "42606686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 849 entries, 0 to 848\n",
      "Data columns (total 21 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Unnamed: 0         849 non-null    int64  \n",
      " 1   post_id            849 non-null    object \n",
      " 2   comment_id         849 non-null    object \n",
      " 3   subreddit          849 non-null    object \n",
      " 4   post_title         849 non-null    object \n",
      " 5   post_ups           849 non-null    int64  \n",
      " 6   post_upvote_ratio  849 non-null    float64\n",
      " 7   post_num_comments  849 non-null    int64  \n",
      " 8   post_time          849 non-null    object \n",
      " 9   post_text          849 non-null    object \n",
      " 10  post_downs         849 non-null    int64  \n",
      " 11  post_score         849 non-null    int64  \n",
      " 12  post_awards        849 non-null    int64  \n",
      " 13  comment_awards     849 non-null    int64  \n",
      " 14  comment_author     647 non-null    object \n",
      " 15  body               849 non-null    object \n",
      " 16  comment_replies    849 non-null    int64  \n",
      " 17  comment_ups        849 non-null    int64  \n",
      " 18  comment_downs      849 non-null    int64  \n",
      " 19  comment_time       849 non-null    object \n",
      " 20  comment_score      849 non-null    int64  \n",
      "dtypes: float64(1), int64(11), object(9)\n",
      "memory usage: 139.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load the dataframe\n",
    "readin = '/Users/irenevillalonga/Desktop/Text Mining & NLP/Term paper/'\n",
    "filename =\"cmv_extract.csv\"\n",
    "\n",
    "df = pd.read_csv(os.path.join(readin, filename), sep=';', encoding='utf-8')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f35740e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 736 entries, 0 to 735\n",
      "Data columns (total 21 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Unnamed: 0         736 non-null    int64  \n",
      " 1   post_id            736 non-null    object \n",
      " 2   comment_id         736 non-null    object \n",
      " 3   subreddit          736 non-null    object \n",
      " 4   post_title         736 non-null    object \n",
      " 5   post_ups           736 non-null    int64  \n",
      " 6   post_upvote_ratio  736 non-null    float64\n",
      " 7   post_num_comments  736 non-null    int64  \n",
      " 8   post_time          736 non-null    object \n",
      " 9   post_text          736 non-null    object \n",
      " 10  post_downs         736 non-null    int64  \n",
      " 11  post_score         736 non-null    int64  \n",
      " 12  post_awards        736 non-null    int64  \n",
      " 13  comment_awards     736 non-null    int64  \n",
      " 14  comment_author     647 non-null    object \n",
      " 15  body               736 non-null    object \n",
      " 16  comment_replies    736 non-null    int64  \n",
      " 17  comment_ups        736 non-null    int64  \n",
      " 18  comment_downs      736 non-null    int64  \n",
      " 19  comment_time       736 non-null    object \n",
      " 20  comment_score      736 non-null    int64  \n",
      "dtypes: float64(1), int64(11), object(9)\n",
      "memory usage: 120.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Remove NA \n",
    "df = df.loc[df['body'] != '[removed]']\n",
    "df = df.dropna(subset=['body']) \n",
    "\n",
    "df = df.reset_index(drop=True) \n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c4f09f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Title: \n",
      " CMV: Donald Trump has not made a single lasting positive impact on the USA during his term as president.\n",
      "   \n",
      "Text 1 : \n",
      " How about his signing into law of \"right-to-try\" legislation, allowing gravely ill patients access to experimental drugs?\n",
      "   \n",
      "Text 2 : \n",
      " I haven't seen this posted yet - but by far his most important accomplishment has been keeping America out of any new foreign conflicts. [According to this article](https://www.theelders.org/news/only-us-president-who-didnt-wage-war), the only other president who managed to keep the US out of new foreign conflicts was Jimmy Carter. Thus if this holds through the rest of his term, Trump will be only the second to manage that.\n",
      "\n",
      "For all the talk about Trump being an idiot, and dangerous, and in over his head, I'd submit that keeping the US out of other people's wars is not an easy thing to do. I think he deserves an enormous amount of respect for having pulled this off.\n"
     ]
    }
   ],
   "source": [
    "print(\"Post Title: \\n\", df.post_title[0])\n",
    "print(\"   \")\n",
    "print(\"Text 1 : \\n\", df.body[0])\n",
    "print(\"   \")\n",
    "print(\"Text 2 : \\n\", df.body[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37c2646",
   "metadata": {},
   "source": [
    "Note that from a unique post title: \" Donald Trump has not made a single lasting positive impact on the USA during his term as president\" we have extracted 736 user's opinions. \n",
    "\n",
    "First, we are going to preprocess this texts and obtain the document term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0b2e826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk           \n",
    "\n",
    "# Natural language Toolkit\n",
    "from nltk.stem import SnowballStemmer                                   # Porter's II Stemmer\n",
    "from nltk import word_tokenize                                          # Document tokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "porter=SnowballStemmer(\"english\")\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    clean_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(clean_words)\n",
    "        \n",
    "def abbr_or_lower(word):\n",
    "    if re.match('([A-Z]+[a-z]*){2,}', word):\n",
    "        return word\n",
    "    else:\n",
    "        return word.lower()\n",
    "\n",
    "def tokenize(text, modulation):\n",
    "    tokens = re.split(r'\\W+', text)\n",
    "    stems = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        lowers=abbr_or_lower(token)\n",
    "        if lowers not in stop_words:\n",
    "            if re.search('[a-zA-Z]', lowers):\n",
    "                if modulation==1:\n",
    "                    stems.append(porter.stem(lowers))\n",
    "                if modulation==2:\n",
    "                    stems.append(lmtzr.lemmatize(lowers))\n",
    "                if modulation==0:\n",
    "                    stems.append(lowers)\n",
    "                stems.append(\" \")\n",
    "    return \"\".join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ab70564b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 736/736 [00:00<00:00, 6866.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# stop words\n",
    "stop_words = set(stopwords.words('english')) \n",
    "my_stopwords = ['com', 'reddit', 'http', 'wiki', 'www', 'im', 'trump', 'lol', 'people', 'expand',\n",
    "                'really', 'deal','u', 'much', 'get', 'good', 'act', 'put', 'man', 'a', 'think', 'one',\n",
    "                'say', 'like', 'go', 'do', 'head', 'yet', 'wall', 'guess', 'keep', 'oh', 'north', 'oil',\n",
    "                'prize', 'involved', 'might', 'medium''among', 'might', 'make', 'do', 'may', 'year', 'give',\n",
    "                'also', 'law', 'etc', 'wait', 'prove', 'mean', 'thing', 'rest', 'middle','rnr','u','fuck','make',\n",
    "                'would', 'know', 'lot', 'see', 'president', 'done', 'even', 'many', 'ever', 'want', 'made',\n",
    "                'got', 'going', 'need', 'view', 'something', 'lasting', 'still', 'way', 'every', 'anyone', \n",
    "                'first', 'look', 'medium', 'time', 'since', 'life', 'probably', 'anything', 'come', \n",
    "                'long', 'could', 'anything', 'donald', 'back', 'sure', 'last', 'nothing', 'rate', 'well',\n",
    "                'left', 'le', 'someone', 'example', 'seen', 'day', 'said', 'world', 'making', 'far', 'care',\n",
    "                'shit','america','issue','new', 'actually','never','whole','exposed','imapct','side','single', \n",
    "                'person', 'everyone', 'saying', 'term', 'though', 'point', 'enough', 'matter', 'fact',\n",
    "               'end', 'decade', 'pretty', 'take', 'best', 'talk', 'least', 'news', 'believe', 'positive', 'find', \n",
    "                'big', 'maybe', 'agree', 'order', 'feel', 'used', 'plan', 'result', 'try', \n",
    "               'understand', 'read', 'stop', 'getting', 'fan', 'two', 'child', 'help', 'real', 'great', \n",
    "                'without', 'brought','always', 'helped', 'better', 'bad', 'true', 'price', 'east', \n",
    "               'list', 'company', 'ask', 'college', 'major', 'fucking', 'hate', 'huge', 'opinion', 'stopped', \n",
    "                'open', 'guy', 'worse','personally', 'created', 'mind', 'started', 'benefit', 'finally', 'global',\n",
    "               'hard', 'worst', 'support', 'almost', 'simply', 'allowed', 'level', 'especially', 'thought', 'let', \n",
    "                'win', 'love', 'start', 'million', 'cause', 'let', 'rule', 'happened', 'able', 'certain', 'large', \n",
    "                'part', 'truth', 'took', 'went', 'step', 'away', 'gone', 'realize', 'around','ended', 'family', \n",
    "                'trying', 'nice', 'allowing']\n",
    "stop_words.update(my_stopwords)\n",
    "\n",
    "\n",
    "df['body_stopwords'] = df['body'].apply(remove_stopwords)\n",
    "\n",
    "\n",
    "# lemmatizing / stemming\n",
    "mod=2 #=1 means stemming, =2 means lemmatizing, =0 just lowercase\n",
    "\n",
    "text_preproc = (\n",
    "    df.body_stopwords\n",
    "    .astype(str)\n",
    "    .progress_apply(lambda row: tokenize(row, mod))\n",
    ")\n",
    "\n",
    "df_proc = df\n",
    "df_proc[\"text_preproc\"]= text_preproc\n",
    "\n",
    "\n",
    "# again stopwords\n",
    "df['text_preproc'] = df['text_preproc'].apply(remove_stopwords)\n",
    "\n",
    "\n",
    "print(\"done with text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6be5ab27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signing right legislation gravely ill patient access experimental drug\n"
     ]
    }
   ],
   "source": [
    "for article in df.text_preproc[:1]:\n",
    "    print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "672462b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Texts = df_proc.text_preproc.tolist()\n",
    "\n",
    "for text in Texts:\n",
    "    remove_stopwords(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f7748272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['signing right legislation gravely ill patient access experimental drug',\n",
       " 'posted important accomplishment keeping foreign conflict according article theelders org didnt wage war managed US foreign conflict jimmy carter thus hold second manage idiot dangerous submit keeping US war easy deserves enormous amount respect pulled',\n",
       " 'doubling standard deduction tax absolutely net country previously rarely 12k line unless mortgage interest deduction itemizing unfairly balanced towards home owner previously benefited hurt wealthy already behind earning home equity letting doubled standard deduction absolute right direction tax code partially regressive',\n",
       " 'deleted',\n",
       " 'argue favour disagree sometimes easy donated entire presidential salary variety inauguration VAs education service plenty convinced mexican government modernise labour trade treaty mexican unionise properly argued affect US happier protected workforce trade partner country reform prison system killed abu bakr al baghdadi honourable mention SPACE FORCE worth name alone gave laugh course none completely trashing ahem formerly international reputation US edit added ahem second edit add five hundred comment regarding',\n",
       " 'lowered unemployment record low high percentage american financially according official study factually job opening unemployed wage african american unemployment recently achieved lowest recorded hispanic american unemployment lowest recorded asian american unemployment recently achieved lowest recorded woman unemployment recently reached lowest youth unemployment recently hit lowest nearly half century lowest unemployment recorded american high school diploma administration veteran unemployment recently reached lowest nearly NATO ally cough money collective security ally increased defense spending billion white house report twice ally meeting commitment spend gross domestic product defense today arrived stood hong kong warned china use violence suppress pro democracy protest signed hong kong human right democracy hong kong marched american flag sang national anthem gratitude tariff threat forced mexico crack illegal immigration mexico recent history enforcing immigration sending thousand national guard force southern border caravan central american migrant plus congress poised approve mexico canada free trade agreement possible threat tariff economic growth quarter hit percent median household income hit highest recorded worked toward enforced FDA approve affordable generic drug history drug freezing reversing planned increase reformed medicare program hospital overcharging low income senior drug saving senior hundred dollar alone signed ending gag pharmacist prevented sharing money saving information secured billion funding fight opioid epidemic reduced high dose opioid prescription percent office signed bill drug import canada prescription signed executive force healthcare provider disclose cost service american comparison shop provider charge insurance signing bill american blindsided bill medical service agreed advance hospital required post standard charge service include discounted hospital willing accept signed VA choice VA accountability expanded VA telehealth service walk clinic urgent primary mental health recently signed bill native compensation spokane tribe loss land mid fund native language program third federal recognition little shell tribe chippewa indian montana signed cruelty animal federal felony animal abuser face tougher consequence violent crime fallen office rising elected signed bill CBD hemp legal EPA gave fix water infrastructure problem flint michigan leadership surpassed russia saudi arabia become largest producer crude signed allow state victim fight online sex trafficking FOSTA includes enabling sex trafficker SESTA enforcement victim tool fight sex trafficking signed bill require airport provide space breastfeeding mom signed biggest wilderness protection conservation bill designated acre protected land freaked dropped program program program misused funding implementation tell signed save sea fund per clean ton plastic garbage ocean reform addressed inequity sentencing disproportionately harmed black american reformed mandatory minimum unfair outcome expanded judicial discretion sentencing non violent crime benefiting retroactive sentencing reduction black american provides rehabilitative program inmate helping successfully rejoin society return crime increased funding historically black university HBCU signed legislation forgiving hurricane katrina debt threatened HBCU signed funding legislation september increased funding school choice tax cut signed promote school choice use saving elementary secondary education signed legislation improve national suicide hotline signed comprehensive childhood cancer legislation advance childhood cancer research improve treatment tax cut job signed doubled maximum amount tax credit available parent lifted income limit claim signed billion funding increase development fund providing total billion state fund low income dependent tax credit CDCTC signed provides tax credit equal expense per per flexible spending account FSA allow set aside pre tax use signed autism collaboration accountability research education CARES allocates billion funding next five autism spectrum disorder signed funding package providing nearly funding lupus specific research education program additional billion funding national institute health NIH lupus funding appreciate checking cool',\n",
       " 'fully funded land water conservation fund perpetuity funded nearly necessary backlog repair work national park american outdoors disproves claim edit seem misunderstood CMV arguing environmental record pointing',\n",
       " 'difference defence spending europe slowly climbing upwards whether beneficial subjective qualifies successfully achieving political goal regard',\n",
       " 'deleted',\n",
       " 'impact USA presidency given parent tell kid namely united state frankly usually joke serious perspective young happen difference certainly allows difference fit traditional political mold IMO qualified job lower bar generally case kid hey chance necessarily harvard grad war hero tough kid turn fantastic giving fight begin chance horrible show truly willing journey ultimately fail process anyway lowering bar longer lying kid',\n",
       " 'thank meaningful discussion canadian follows politics tried objective easy sometimes worship burn effigy',\n",
       " 'sole turn blind eye china motivation center CCP tyrannical regime deprives basic liberty human right systematically destroying economy country dumping cost destroy local industry addicted artificially cheap opium mention fentanyl export regime bit deadly hitler germany realization mindset chamberlain europe peace prosperity v hong kong freedom erode taiwan claim independence unrecognized south china sea piss poor job alone approach critical standing china clear motif centered idolizes putin russia subvert thuggish corrupt cleptocracy former leader refused stand CCP change presidency likely forward',\n",
       " 'legalized hemp focus southern border reducing human drug trafficking called epstein bill clinton campaign trail cool war conflict troop home given VA called military industrial complex multiple UAE peace closed travel china else taking corona seriously US death prevented favorite pleasantly surprised presidency accomplishment surprise network foreign asset despite number platform politician either terrible claim hitler hair formerly powerful angry seeing dirty politics fight rather party serve puppet master openly corrupt politics outsider corrupt clue supposed inform certainly giving straight politics',\n",
       " 'sheer amount corruption federal government',\n",
       " 'post evidence shown easily manipulated country believing purely purely together excuse leader right excuse right hope bias exists competing viewpoint presidency overwhelming society',\n",
       " 'easily hated mouth label exact reason narrative confirmation bias use crack illegals obama absolute deporter chief axios immigration ice deportation obama a72a0a44 540d 46bc a671 cd65cf72f4b1 html fine racist deporting build bullshit mainly bc US loved focused seem business owner hire work harder else work half speed felt blame illegals coming realizing racism critic poor american job black unemployment lowest history fred stlouisfed org series LNS14000031 policy poor country workforce job poor usually job near bottom illegals clear taking job thats exactly difference admins strategy harder stay welfare bc knew job opening seem cruel whatnot kick sometimes obama gutted restriction welfare poor sit collect check unemployment showed across board reading headline sturgis super spreader headline narrative total horseshit',\n",
       " 'farm_sauce OP awarded delta post comment earned delta OP user listed r DeltaLog comment iq4iv8 deltas_awarded_in_cmv_donald_trump_has_not_made_a r DeltaLog please note change necessarily reversal conversation delta system explained r changemyview deltasystem deltaboards r changemyview deltaboards',\n",
       " 'gotta mention top already mentioned statistic opened eye black grew poor prison system systematically ruined generation black 90 pre covid administration administration overall black community shockingly including obama lowest black incarceration past lowest black unemployment frame supporter highly BRC NYC father main charity provides low income home ownership opportunity black community stats stick craziest push directly influence number ive bi product bunch note SOO disappointed k harris VP announcement track record represents everything wrong justice system right thats main elizabeth warren k harris nomination',\n",
       " 'war alone warrant election american war hungry biden hold starting war obama several proxy war supplying rebel weapon libya turn wealthiest country africa war torn shithole actual slave market gave rebel syria weapon civil war causing refugee thousand death war bush require another explanation war rebel supplied weapon hell korea south korea talking pressure israel conflict highlighted show bitching peace hundred thousand dying right skin colour completely fucked',\n",
       " 'justice reform money historically black university israel UAE agreement serbia kosovo agreement US energy independence war drawing troop iraq afghanistan opportunity zone 70b moved embassy jerusalem building border mexico trade replace NAFTA US mexico canada agreement starting china place scum',\n",
       " 'administration assisted normalizing relation UAE israel historical event celebrated nobel peace US history administration lowered cost prescription drug republican democrat pocket pharmaceutical fixing effect biden clinton crime bill destroyed black community regardless COVID vaccine arrives unprecedented taken administration buy risk hundred dos instrumental different entity stage trial right federal government assistance nearly close turned tide manufacturing job pouring china recent ignored money corporation ensuring offshore manufacturing job progress opioid crisis ignored federal office',\n",
       " 'war lmao lowest unemployment black hispanic history small business owned black increased tax cut forgave debt HBCUs affected katrinahttps washingtonpost grade wp education department forgives loan historically black recover hurricane program billion minoritiy retirement whitehouse gov briefing statement remark signing executive establishing white house opportunity revitalizatiocompensation native american lost land thehill changing respect diversity inclusion signed three bill affecting nativeset legislation fight sex trafficking appointed former victim black woman newest member advisory council human trafficking theepochtimes creates position dedicated fighting human trafficking_3223105 htmllowering penalty none violent crime nytimes politics prison sentencing htmlbillions urban development led ben carson politico state york albany story administration imposes monitor nycha city pledge 22b 831349trump RESTORES funding HBCU historically black university apnews c4834e48841d97c5a93312b1bf75302ahe award jesse jackson work black community',\n",
       " 'war armed conflict conflict succeeded bringing peace korea honest bill clinton',\n",
       " 'biggest impact avoiding war iran US drone shot military planned bombing attack response realized strike killed dozen iranian rhetoric hear career politician party else office military proceed strike',\n",
       " 'disclosure idiot social policy generally stand deplorable ethical standard achievement worth noting broker fully normalized relation UAE israel BBC canada norwegian politician nominated nobel peace role normalized relation stated judged action behaves sometimes likely leftist explode handle righteous right gloating reducing dependence china matching tarrifs chinese government increasingly expansionist flouting human right stupid calling xi xinping leader maintained military pivot asia offering assurance allied asian nation action line verbal praise chinese leadership accomplishment depending political right instance religious freedom policy backward US domestic social anti abortion religion US currently synonymous christian change domestic international stage publicly called nigeria killing christian conference call nigeria criticized china persecution uighur muslim consistently strong backing religious freedom international front reuters article usa religion un un push religious freedom event slamming china uighur iduskbn1w82bj domestic social policy suck divisive biggest weakness knee jerk combative childish de escalate however foreign policy mentioned US unless stupid',\n",
       " 'plenty conservative e g court pick recent clinton earlier promise move US embassy jerusalem backed fearful palestinian backlash blinking eye walked border korea fearless unprotected utter shock watched brief hope south korea unification across asia pro fest largely shielded USA MSN running nuclear war narrative quickly shut hell changed subject destroyed ISIS giving general mattis free hand remember ISIS threat prior ordered killing soleimani entire student iran walking ground painting american flag respect changed overton window china standing theft american IP else gutless icing cake believed recent press release white house sufficiently impressed page html2 f scribdassets 99k0aq4czk81zpd2 image 0343d5eb33 jpg page html2 f scribdassets 99k0aq4czk81zpd2 image d730990f61 jpg page html2 f scribdassets 99k0aq4czk81zpd2 image 54f6e8d291 jpg page html1 f scribdassets 99k0aq4czk81zpd2 image 05310c52ad jpg page html1 f scribdassets 99k0aq4czk81zpd2 image 5f048e4bd5 jpg page html1 f scribdassets 99k0aq4czk81zpd2 image dc5031e9db jpg nicely ordered itemising amazing accomplishment frankreport hundred twenty five amazing accomplishment j hundred online comprehensive overlap unique item use google search battleground narrative wrestling balance choice clear rational vote welcome',\n",
       " 'impact USA signed reuters article usa drugprices iduskcn24p2ja lower prescription drug insulin nypost cut cost insulin medicare enrollee month month stitch peace financialexpress defence century uae israel peace accord UAE israel bought gulf story US cut iraq troop pledge endles troop home iraq afghanistan newscaf official bringing troop home germany_7091101 html troop germany US unemployment whitehouse gov article unemployment fall low fall low',\n",
       " 'mentioned pushing national security advisor foreign surveillance US security attack TikTok TikTok foreign intelligence gather information US citizen case administration pushing former GOP regard attempting lower healthcare hell extremely healthcare GOP member direction definitely worth mentioning nationalized healthcare already playing ball hong kong removing special status hong kong slighted china appreciate indirect hong kong piss china necessarily hong kong run played bargaining chip strengthen democracy hong kong administration confirmation bias clouding reality unfortunate signed executive pressuring police force US using excessive force yes EO bureaucratic hopefully effect tell anyway credit fight police reform right wing either narrative base federalregister gov document safe policing safe community federalregister gov document safe policing safe community mention obviously shrugged EPA agency political action front group power rig energy energy production conspiracy embedded function tax flow action front essentially taxing energy achieve impossibility dismantling EPA cutting revenue stream function front protecting environment pissed team place energy cheaper effect environment either stifled democrat power energy threw giant hoopla rid EPA GOP wanted DNC using regulatory ability EPA political globalist ideal funded black university terrible personality country completely listening advisor respect smartest dumb hire smartest yes conservative effectively running country office lame duck bolstering US defense class conservative attacking running country country turning turning regardless post stuff argument decision reply',\n",
       " 'war thats',\n",
       " 'hasnt impact must watch CNN pity falling lie paper excellent american history yeah easiest tough son bitch feeling recently signed three bill native compensation spokane tribe loss land mid 1900s fund native language program third federal recognition little shell tribe chippewa indian montana finalized creation space force 6th military branch signed cruelty animal federal felony animal abuser face tougher consequence violent crime fallen office rising elected signed bill CBD hemp legal EPA gave fix water infrastructure problem flint michigan leadership surpassed russia saudi arabia become largest producer crude signed ending gag pharmacist prevented sharing money saving information signed allow state victim fight online sex trafficking FOSTA includes enabling sex trafficker SESTA enforcement victim tool fight sex trafficking signed bill require airport provide space breastfeeding mom lowest paid american enjoyed income boost november outpaces gain earnings country highest paid worker low wage worker benefiting higher minimum wage corporation increasing entry pay signed biggest wilderness protection conservation bill designated acre protected land signed save sea fund per clean ton plastic garbage ocean signed bill drug import canada prescription signed executive force healthcare provider disclose cost service american comparison shop provider charge insurance signing bill american blindsided bill medical service agreed advance hospital required post standard charge service include discounted hospital willing accept eight prior inauguration prescription drug increased average per drug decline nine ten month drop recent month white house VA hotline veteran principally staffed veteran direct member veteran VA employee held accountable poor performance VA employee removed demoted suspended issued executive requiring secretary defense homeland security veteran affair submit joint provide veteran access access mental health treatment transition civilian bill signed championed federal employee pay increase average largest raise signed week paid parental leave federal worker administration provide HIV prevention drug free uninsured patient per record sale holiday signed small business group together buying insurance',\n",
       " 'scroll hear black community lowest unemployment american history reminded ordered invasion nation carter accomplishment achieved hope offer different approach eliminate thread spoken state union speech press briefing RNC acceptance speech sadly due social alienation political division trusting governmental research advise e CDC FDA certainly white house press briefing dismiss lie administration thread right divide unwilling consider right lie deceit evil understandable mostly leaning mass constantly drone right supposedly proclaims evil racist bigoted sexist paint number fascist important hear envelope bubble ego liberal month election changed sense right party type becoming comfortable gun enthusiast military veteran mechanic tradesman devoted christian general working union stiff intimidate confuse upset wanted truly believed living protect threat community maintain safe community onerous regulation hurt fruit labor grand vision else learned resent minarchist libertarian else accountability others basically bureaucrat intelligent yes leader resent intellectual dangerous path lead letting small group ultimately bias blindspots dictate policy regard others representative democracy important smaller district require focused set idea exist community continue ramble supported evil administration dimensional base conservative op hold genuine belief hold ideal firm solidify ego attrition research trust mass readily reporting lean towards outrage buck empathize sympathizing confuse hot minute clear bias truly consider conclusion thank platinum gold higher award expect',\n",
       " 'dislike dislike jumping bandwagon presidency USA china trade war devastating china impossible damaged chinese economy source crippled chinese regime desperate flip US economy booming right covid hit economy strength strength p500 small local business high killed public enemy number iranian economy fucked weak iran amazing leverage perhaps withdraw troop brings withdrawing troop syria iraq continuing obama era policing saudi arabia isreal beinging american troop home continued downwards trend violent crime right BLM protest revealed extent failure MSM providing unbiased fair journalism accidently country live covid locking spread managed escape economic covid lower death comparative european country enjoy luxury avoiding second spike second spate lockdown second spate layoff',\n",
       " 'jimmy carter war foreign conflict towards stopping CCP encroachment hong kong running actual concentration camp mention brokering historic peace isreal uae deptuizing state police washington properly convict violent rioter violating state right hopefully spell mostly peaceful riot killed march effective',\n",
       " 'prevented TVA outsourcing employee direct violation stated reason existing enrich tennessee via employment TN resident save job currently staffed facing removal favor hiring outsourced role yeah broken clock right twice',\n",
       " 'animal cruelty illegal federal cnn politics animal cruelty signed trnd index html',\n",
       " 'blow buries deed article bias',\n",
       " 'name obama',\n",
       " 'stood hong kong warned china resort violence',\n",
       " 'unfortunately change share frustration conversation friend frustrating knowing zero resource objective analysis presidency solidly naturally despise everything represents study politics sometimes nearly impossible source activity positively hideous right wing website littered clickbait ad titled feminism absolutely hint objectivity facially absurd claim black meanwhile source predominantly center apparatus histamine response mode flaring focused rejecting virus combine social pre decided narrative basic becomes nearly impossible synthesize frustrating',\n",
       " 'whitehouse gov administration accomplishment accomplishment administration september median household income hit highest recorded atr org thanks median household income highest amp american lifted food stamp election low american EBT food stamp politifact factchecks jul correct food stamp low signed biggest package tax cut reform history tax cut billion poured quarter alone african american unemployment recently achieved lowest recorded cnn economy black unemployment index html hispanic american unemployment lowest recorded asian american unemployment recently achieved lowest recorded woman unemployment recently reached lowest youth unemployment recently hit lowest nearly half century lowest unemployment recorded american high school diploma unemployment claim recently hit low manufacturing job growing fastest THREE DECADES GOES whitehouse gov administration accomplishment break deep state youtu 39m_npbbjd4 pedophile ring high number human trafficking arrest presidency ice gov release ice hsi announces record high number criminal arrest fy19 higher obama instagram p CCrTgaPAm1n kicked epstein private club suspicious activity thehill homenews administration book claim barred epstein mar lago financier hit formed stable relationship korea youtube watch v j9opal4esog signed police reform politico sign police reform executive article black community prosper delgazette helping black community thrive latino hispanic community whitehouse gov briefing statement j policy uplifting empowering hispanic american',\n",
       " 'deleted',\n",
       " 'wholeheartedly disagree quote jack nicholson batman town enema african american light majority known racism bedrock nation white citizen delusion reality constant fight front group must pro active government root kind evil vote election state local second american rot system donkey kicked face politician job active participant priveleged policy american effect become sloth oblivious ill society nc busy distracted focus right country accidentally become needed elected knowing corrupt system lame party system spout rhetoric meaning control corporation poison regard except profit justice system habitually punishes poor whether black latino white health system suck religious fascism racist right right coded rhetoric neo liberal republican light party serving corporation citizen problem surely everything front street action old system broken accidentally awoken action excuse rushed nature',\n",
       " 'signed biggest solar farm',\n",
       " 'UAE israel peace administration heavily massive forward accomplishment establishes normal relation country including business relation tourism direct flight scientific cooperation eventually include full diplomatic tie ambassadorial several relation china mentioned comment particular massive accomplishment',\n",
       " 'economy recent month v shaped recovery unemployment human trafficker caught due border security donated salary lost personal dollar pay PEACE ISRAEL AMD UAE holy besides iran country v country violence israel earned nomination nobel peace withdrawal intermediate range nuclear force INF treaty delivering china korea strategic setback tariff threat forced mexico crack illegal immigration ordered operation killed islamic state leader abu bakr al baghdadi african american unemployment achieved lowest diverse cabinet color lgb member working whitehouse ton prison reform bill ton HIV AIDS relief money bipartisan basis including criminal justice reform opioid sex trafficking legislation right giving dying american access experimental medication hope learns action policy statistical top twitter mouth logistical policy amazing',\n",
       " 'hey OP already listed shook expected hillary kind becoming standardized clinton bush obama hillary kind became run along wreck everything winning previous election undeniably stuff terrible public image choosing battle action however stuff divide comment torn apart decent actuality stuff colored presidency',\n",
       " 'lowest black unemployment substantial margin tried decriminalize gay worldwide scope',\n",
       " 'american outdoors',\n",
       " 'mostly apathetic populace pay attention politics crap system government devolved',\n",
       " 'gave greatest political drama show',\n",
       " 'war',\n",
       " 'remember republican celebs kissed',\n",
       " 'week ago relation united arab emirate israel forty war join international conflict',\n",
       " 'recall banning HIKVISION camera system federal location policy garbage',\n",
       " 'UAE israel peace treaty boarder construction replacement program',\n",
       " 'nominated three nobel peace recently week work israel uae peace agreement different work korea meet leader set peace',\n",
       " 'corporate organization pissing came called fake bit',\n",
       " 'proved democrat woman nypost wp content uploads site joe bide grand daughter ap jpg quality strip w',\n",
       " 'google prison reform black unlike democrat policy',\n",
       " 'change research expecting expect somebody disagree',\n",
       " 'careful changing acknowledge white supremacist',\n",
       " 'CMV totally brainwashed fake free represents rebrands main source',\n",
       " 'offense OP post happens due lie ommission',\n",
       " 'suggest looking FACTS user',\n",
       " 'accomplished hater fake democrat destroyed arguing leftist hater fake democrat four',\n",
       " 'record number arrest human trafficking credit preventing trafficked sex',\n",
       " 'brave call wrong dude blight general consensus place actual answer opposing',\n",
       " 'thread taught',\n",
       " 'f yr bigoted democrat impact yr viewfinder blurred',\n",
       " 'public aware fake didnt exist elected public aware china cheating economically public aware obama clinton angel',\n",
       " 'haha neither obama childish',\n",
       " 'compared liberal literally destroying country inside',\n",
       " 'flaw US version government check balance required',\n",
       " 'technically current darkness',\n",
       " 'blind pig acorn freakishly stupid surprised accidentally stumble',\n",
       " 'bring peace israel UAE domestically economy arguably cutting deficit fueled fed inflation',\n",
       " 'shone light corrupt party McConnell graham cruz et al drained swamp',\n",
       " 'healthy distrust',\n",
       " 'prison reform donated presidential paycheck helping veteran administration killed terrorist soleimani begin peace israel united arab emirate job right legislation sick option experimental healthcare approach',\n",
       " 'lay eye beholder recall seeing trending popular post save sea apnews 2d5947a8bd924adc9f0ab077177fabdd amd heard american outdoors federalnewsnetwork management american outdoors lied tried calling release methane whatever wing blasted wanting trim forest prevent wildfire whitehouse gov presidential action eo promoting active management forest rangeland federal land improve condition reduce wildfire risk conveniently forgot administration pushed wilderness protection bill newsweek wilderness protection bill sign environment largest death valley affordable clean energy bill flak restrictive obama york conveniently forgot ran article supreme court blocked obama legislation place nytimes politics supreme court block obama epa coal emission regulation html heck google windmill hundred hit repeating article claiming windmill alive sued bird death argusleader story wind energy project must nagivate protect bird nixed npr org section thetwo accidentally killing bird isnt crime administration cut bunch repopulated animal endangered specie bee added fws gov midwest endangered insect rpbb FAQsFinalListing html kind groundwork series bee protection skip red tape force bee specie area instead later TheHill impressed largest complaint NASA coming thehill technology offer promise space force nasa second court appointment average working money whitehouse gov briefing statement j delivered record breaking american three office impact elect socialist undone played',\n",
       " 'pulled TPP madeus citizen subject IP country approval voter',\n",
       " 'basically war sharpen focus office despite resistance chicken hawk party deep state killed biggest terrorist war syria ISIS appointed effective surrogate menuchin negotiate successful coronavirus relief massive ventilator PPE production spurred presidential action massive undertaking distribute coronavirus vaccine soon effective produced offer effective federal assistance combat crime urban area chicago reduction violent crime assistance accepted lowest unemployment highest wage growth lower income quintiles criminal justice reform greatly improved trade agreement canada mexico demanded ally contribute defense rather american taxpayer underwrite improved VA health system attempting prescription drug available US country hospital transparency appointed competent federal judge politician lived word',\n",
       " 'watch vernon jones RNC black democrat rogue CEO',\n",
       " 'deleted',\n",
       " 'nod tax bill positively effected income',\n",
       " 'credit credit due improve diplomatic relation korea prison reform overdue else forced republican party political faction forever changed advantage base baiting DECADES add stock buy airline industry bailout money exactly bailout given burned reserve cash greed',\n",
       " 'honestly government vast body government function fair campaign promise kept specifically sign resolution reverse obama executive gun control signed presidential memorandum officially directing united state withdraw trans pacific partnership signature symbolic campaign promise white house promising administration enforce rid excessive legislation nominates neil gorsuch fill empty supreme court seat scalia pulling paris climate agreement create percent repatriation tax repatriation tax percent close bill passed business holding asset overseas repatriate asset percent percent liquid asset guantanamo bay detention center suspend immigration terror prone place move embassy tel aviv jerusalem limit legal immigration create private white house veteran hotline happy holiday country protect pay joint defense reverse barack obama cuba policy ensure funding historic black defund planned parenthood use steel infrastructure project lifetime ban white house official lobbying foreign government effect raise tariff imported increase veteran health save carrier plant indiana cut social security proposed cut congress bite defense sequester salary politifact meter promise promise _group trumpometer ruling promise kept politifact meter promise promise_group trumpometer ruling promise kept impossible perspective stance china idea dropped TPP leaving TPP idea achievement subjective',\n",
       " 'prior tax cut job united state highest corporate tax industrialized tax largest expense despite tax generated came corporate tax US driving seek tax shelter country offshore job US based business competitive scale change tax pas allow american business compete internationally minimal impact overall tax revenue increase business stay US generate tax revenue haul',\n",
       " 'crimea annexed obama administration mostly reluctance obama specifically refused sell deadly weaponry ukrainian reluctant approve significant factor bringing stability region democrat administration wherewithal move embassy jerusalem appointing nikki haley call foreign policy loudmouth seems mostly decision',\n",
       " 'blocked TPP fucker gotta credit due',\n",
       " 'awareness divided country influenced younger generation politics',\n",
       " 'raising smoking age high schooler buy high schooler rise vapeing override',\n",
       " 'changed international postal competitive locally ordered AliExpress wondered shipping free china pay shipping country local shipping including expensive mile UPU net sender net recipient china chunk ecommerce postal organization net recipient suffer alongside factor changed USA country suffering en wikipedia org universal _postal _union shifting _balances _and _the _united _states en wikipedia org universal_postal_union shifting_balances_and_the_united_states note clock right twice',\n",
       " 'presidency expose corruption racism including BLM mainstream socio economic disparity',\n",
       " 'deleted',\n",
       " 'super emphasized explicitly reaching black american looking actively diversify republican party particularly legacy typical republican politician conservative generally uncomfortable appeal universalist touch race capitulate called racist likely buy common knowledge among political elite black destined vote bloc diversity republican party bring accountability party lessening extent charge racism wedge funding restoration historically black permanent particularly helpful feature board wedge future decision based perceived patronage party wedge mouth service abortion republican clearly incentive seeing solved swallow shittiness baby killer racist power',\n",
       " 'dunno credit tension USA korea heard threatening use nuclear weapon tax cut affected money money super educated local government politics experience fully handled county decision regarding lockdown reason pandemic poorly handled choosing close pandemic mention anti vaxxers anti masker completely disregarding safety measure unfair blame stuff tension referring upset insight idrk exactly experience political sub chill asking insight looking stuff weird accusation mental gymnastics credit stuff instead looking stuff right front biden hillary choice wish mild choice politics seem wildly shifted annoying treated showed biased source felt CNN FOX unbiased trustworthy source report',\n",
       " 'negative depend spectrum fall negative ring thinking loser managed unify folk vote organize opponent handily impressive feat turn vote taking action organizing speaking opponent else thank difference cast aside common enemy em em vote next week voted alone amazing push trigger show hand undoes facade elected official longer hide behind scene filter expose priority wear shame reelected presence ushered deserved needed change politics old fading newer era progressive capitalist leap fray igniting fire congress face mirror change challenged deep seated corrupted politician proved untouchable miserable old miser behind younger innovative thinker relying tell seeking everything hear relying doctored document article tell studying researching sorting fiction longer satisfied accepting told community call pot black mirror remember kettle barely qualifies gen z next country russia china deep seated hatred disgust denial history american rallying pushing towards forward known area history accomplished blood popping heard move difference thier magnification watching watching closer turning blind eye em em tonight name mentioned talking asking question thinking deeply fix wrong yes specifically impressive yes equal opposite vote discussion free hear doesnt wrong understanding compassion shared battlefield remember',\n",
       " 'deleted',\n",
       " 'insulin dropping imo obama btw imo dont daminor',\n",
       " 'deleted',\n",
       " 'debate statement validity',\n",
       " 'number nearly war soooo perhaps net dicey',\n",
       " 'space force separation armed force streamlines function',\n",
       " 'neutral impact national metric system adoption please',\n",
       " 'reform system future impact',\n",
       " 'youth politically fear anger',\n",
       " 'executive combat human trafficking combat HT',\n",
       " 'handwringing nobel peace nomination brokering normalization relation UAE israel significant achievement worthy nobel peace record certainly worthy clear responsible',\n",
       " 'supporter shorter anti animal cruelty',\n",
       " 'worth mentioning peace attempt UAE country forgetting formally recognizing israel sovereign legal nation peace region incredible',\n",
       " 'gotten american politics dont',\n",
       " 'signed measure congress gov bill 116th congress senate bill preserve native language article cbc ca canada measure protect indigenous language',\n",
       " 'e prison reform en wikipedia org first_step_act',\n",
       " 'supporter site luck answer',\n",
       " 'regime change war past plus',\n",
       " 'starting war lifetime change yeah',\n",
       " 'deleted',\n",
       " 'question bidens government service',\n",
       " 'gotta jump changed presidency moral changed changed forget change soon leaf office apply american consider impact USA',\n",
       " 'young politics change next',\n",
       " 'number bother tried killing abu bakr al baghdadi al baghdadi killed military personnel spite effort please credit',\n",
       " 'tell biden politician vote career politician',\n",
       " 'inside US live greece affected US intervention definitely peaceful US memory bush obama brutal destroyed country war managed negotiate peace treaty israel UAE argue clearly economic historic',\n",
       " 'presidency pointed serious flaw political power structure US likely lesson coming election debug system follow protocol break motive',\n",
       " 'several impact signed american outdoors fund maintain national park another pointless war withdrew troop iraq effective airstrikes ISIS terrorist group command killed abu bakr al baghdadi leader ISIS negotiate historic peace israel UAE signed CARES largest stimulus bill signed US gave american financial supported small business signed USMCA replaced NAFTA roughly slightly US mexico tough stance china beneficial entire raise awareness dictatorship chinese government running space force moving air force capability branch personnel autonomy room growth',\n",
       " 'agreed withdrawing trans pacific partnership TTP youtu GWHDPuDpay8 TTP extended copyright coverage raised luxury video game work cosplays doujinshis art fiction spin work subject copyright infringement criminal prosecution',\n",
       " 'tuesday signed memorandum instructing interior secretary prohibit drilling water florida coast coast georgia south carolina period',\n",
       " 'legalized hemp exhaustive miss',\n",
       " 'likely increased population vote',\n",
       " 'controversial eye half country UK net cut tax introduced job la rona unprecedented deregulation reduced illegal immigration increased legal immigration move US continued refuse enact insurrection deploy soldier state protest riot promoting state right federal government',\n",
       " 'little late party chime broker UAE israel peace historic significant reason reason nominated nobel peace political taken ease tension recall situation past impressive feat obligatory supporter disclaimer ignoring misguided ignoring negative',\n",
       " 'check magapill dot elect solely aggressive action towards ending human trafficking specifically trafficking arresting pedophile exposing network',\n",
       " 'although must contend refunding NASA creation space force powerful presidency',\n",
       " 'website listing magapill',\n",
       " 'significant peace agreement israel peace',\n",
       " 'neighbor claim increased job job area industry increased paying job overseas job state',\n",
       " 'mandatory penalty fee health insurance taken administration definitely needed shitty',\n",
       " 'meh pet',\n",
       " 'light china quest expense',\n",
       " 'signed clean ocean thats extremely',\n",
       " 'thank posting whether liberal conservative willing LISTEN political thread primarily liberal bashing conservative blanket statement forgotten courteous',\n",
       " 'impact united state impact lost member covid',\n",
       " 'rich rich',\n",
       " 'american excuse ignorance dont war german US military germany dislike US military jet nuke ramstein argue',\n",
       " 'nominate anti war ambassador afghanistan motherjones politics pick afghanistan ambassador withdraw troop immediately',\n",
       " 'jan leave impact leaf',\n",
       " 'small business owner signing tax cut job included QBI deduction allows small business owner layman pay federal income tax gross earnings experience tax cut business degree offset dramatically higher self employment tax pay w2 employee instead half pay business personal component tax towards social security medicare mention putting extra money saving expending needed equipment business pay ton tax small bit capped high earning business advantage break',\n",
       " 'music modernization music IP stolen proper payment artist en wikipedia org music_modernization_act',\n",
       " 'lament raise legal smoking age',\n",
       " 'recently played significant role brokering increased economic diplomatic relation kosovo serbia google amp thehill international webb serbia kosovo move forward united state 3famp',\n",
       " 'BLM NFL apologize kaepernick NASCAR ban confederate flag mississippi racist flag confederate flag banned US military rename base named confederate general killed coal USA paris accord agreement actual progress climate change gotten woman color elected',\n",
       " 'road government oppressive regularly committed war crime representative protect serve citizen destroyed government destroy build government',\n",
       " 'saving market crashing covid crisis',\n",
       " 'whining voted',\n",
       " 'literally nominated nobel peace',\n",
       " 'clarified court proceeding outlining exactly turn executive affect based religion established case cited future establishment similar',\n",
       " 'depends happens election following month definetly actively hopefully immense failure lead necessary change prevent abuse power diplomatic failure straight illegality committed future run happens easily',\n",
       " 'american cashed stimulus check socialist',\n",
       " 'approved largest solar farm impact googled administration approves largest solar farm US expected power home administration approved proposal monday construct operate largest solar power project history issuing final permit massive facility la vega either bit fetched',\n",
       " 'ugh thread talking stuff accomplishment tasked laying minefield cambodia feeling accomplishment thinking proud randomly maim ruin next century lay minefield future politician backing advising small government tea partyists fundamental christian state run psuedo theocracy federal government historical reaction catastrophic presidency caught attention WWII led cold war led space race generation nerd led computer revolution map genome cure disease timeline hitler monstrosity becomes crucial saving perhaps dying gasp shitty status quo room shift culture limitless capitalization industrialization fix planet cultural revolution proletariat overthrow ruling class fix wealth disparity twitter social communication McDonalds health early literally easier hitler historically right hitler evil competent evil incompetent therefore manipulated enabled GOP accomplishment seems generated executive undoing obama personal admission entire cabinet chosen desire collapse government hell FEC oversee election appointed member quorum figurehead lightning rod distraction overall agent chaos raising china american public deserved central government intolerable everything else entire terrible flawed tainted reason right refusing acknowledge interpersonal familial tainted discolored fundamental MO mortage health country trade short profitability ruling class thread giving credit signing bill didnt create endorse bill work n korea visiting kim jong un kim jong un feather cap towards peace recognized leader legitimacy job security third dictator exist china ransom nuclear attack food produce canada removing environmental regulation designed prevent drowning waste product signed save sea exxon trillion dollar spill due lax regulation underfunded agency clean dawn soap bottle floating next dead pelican yay',\n",
       " 'TPP continued hollowing american manufacturing job',\n",
       " 'wrong tiger king financially recover wealth gathered top rear accelerated unprecedented speed valuable infrastructure dissapeared deficit grew soft power dwindled pre power dynamic shifted favour totalitairian regime ally alienated pact broken female leader verbally abused basically insulting country',\n",
       " 'banned TikTok prison reform supporter argumentative',\n",
       " 'press given challenge beginning presidency atrocious personality combined lack compassion destroyed foundation country night revelation regarding lying covid potential danger nation proverbial straw broke camel',\n",
       " 'impact period inherited estate safety net failed business venture apprentice pretend businessman right',\n",
       " 'changed american foreign policy disengaging china including obama domestic disaster wish mouth shut internal',\n",
       " 'shot peace united entire',\n",
       " 'half trillion debt',\n",
       " 'tuesday expanded renewed ban drilling FL include atlantic SC',\n",
       " 'taught american happens blame voter apathy vote voter november asking vote lesser evil answer question',\n",
       " 'donated VA campaign early presidency wanted reform VA taken hasnt disrespecting word',\n",
       " 'bombarded reply thank keeping nowadays high horse comfort reasonable right leaning moderate',\n",
       " 'topic considering date sept 11th victim compensation fund permanent fund due expire victim file claim compensated',\n",
       " 'successfully flushed white supremacist hiding',\n",
       " 'running tab confirmed presidency',\n",
       " 'basically impossible US impact suck negative',\n",
       " 'shown another post somewhere today oompa loompa doom problem woken glaring facing daily basis change change painfull change needed usually lead change westernised country genuinelly hope november ball rolling',\n",
       " 'ISIS essentially disappeared administration',\n",
       " 'small banned bump stock modification weapon appreciate',\n",
       " 'border security valued party USA review behavior obama administration confirm ready instrumental bringing change border security',\n",
       " 'delta thread given freely stuff offered either bogus hold minimum scrutiny',\n",
       " 'mishandling pandemic american flawed healthcare system',\n",
       " 'leader failure representing country democracy everything blatant lie lying taking highest turnover office staff failing unite troubling hey stock market conservative comment seem signed office suddenly amount ups dude seems fond putin dictator russia routinely opposition killer',\n",
       " 'brokering largest peace history entire whitehouse gov briefing statement j secured historic israel united arab emirate advance peace prosperity region google amp nytimes politics israel united arab emirate uae amp html successfully broker peace israel UAE',\n",
       " 'permanently funded black university signed right criminal justice reform black jail worth',\n",
       " 'boat parade combined',\n",
       " 'veiled propaganda thread groomed fucked daughter epstein',\n",
       " 'historic eastern peace treaty uae israel normalized relation lead nation creating stronger anti iran coalition region historic tax cut regulation slashing led precovid economic growth foreign war modern edit renegotiated trade korea redid NAFTA trade war china damaged economy reneged iran nuclear curtailed regime ability build nuclear weapon rebuilt american military position strength addendum comment foreign war utilized restraint insuring unlike obama red line crossed syrian missile strike killing iranian revolutionary guard general projected strength needlessly endangering american',\n",
       " 'hospital shop live saving procedure',\n",
       " 'change change bashing',\n",
       " 'starting ZERO war military intervention change told jimmy carter bombastic rhetoric send war caused violence carter',\n",
       " 'strongly learn failure mistake impact call insignificant learn idiotic couch potato vote',\n",
       " 'EPA despite loosening regulation polluter revived superfund clean official deleted seven site superfund highest total politico magazine superfund activist minden wv',\n",
       " 'war death carter war',\n",
       " 'aware politically engaging right american vote whether agianst half US population voted voting',\n",
       " 'fun explain republican r conservative',\n",
       " 'sign federal animal cruelty bill animal cruelty felony similar',\n",
       " 'amazes quickly forget remember ISIS problem obama heard lately',\n",
       " 'banned critical race theory protect 2a',\n",
       " 'serbia kosovo peace israel united arab emirate',\n",
       " 'accomplishment twitter robbystarbuck status',\n",
       " 'nominated nobel peace work bring peace',\n",
       " 'opened eye incredible amount racism country',\n",
       " 'killed TPP week office promised strongly supported obama neoliberal bizarre watching vehemently suddenly killed TPP pull trade china moron trusted vote',\n",
       " 'accidental instance forced NA trade agreement modernize outdated intention working class increase money rich bigger increase',\n",
       " 'easier veteran use GI bill limit claimed sit using struggled school right due illness worth complete degredation reputation US',\n",
       " 'massive failure extreme racism beneficial bringing belligerently limelight insulting institution demographic US outside managed expose white nationalism threat rather ignored fringe group handful idiot harm beneficial crap surface',\n",
       " 'shattered notion electability border security important tried canada',\n",
       " 'tpp thats free taint corruption',\n",
       " 'orange',\n",
       " 'divisive language biased politics divisive nature course anybody answer performance full biased literally asking asking expect answer non biased',\n",
       " 'heard stated podcast today warmongering modern history',\n",
       " 'small summarisation edit reply others quick TLDR based changed',\n",
       " 'hello praise timidly entire seemed anti war described meeting expressed wanted significantly cut fighting syria ISIS lost territory entire room exploded outrage general advisor war war war war war x200b ab lutely inside told kind stuff hawkish government military industrial complex x200b ultimately walked commitment shame disliked hand wheel hoping right turn chance told war ultimately x200b suck talking obligated exception',\n",
       " 'surrendering taliban US alread spent nearly 3t yes trillion war afghanistan cost incurred veteran likely additional 3t operation handing power taliban killing US warfighters untenably unpopular democrat republican variety reason nobody surrender taliban chance money pit taking opportunity',\n",
       " 'racist bigot misogynist bringing limelight',\n",
       " 'given kudos stopping illegal war health deconstructed economy create job military commander chief afghanistan iraq libya syria excuse general disingenuous eager convertible ride dallas either',\n",
       " 'semi impact utter idiocy position smidgen smarter god forbid smart accomplished putin beyond fucked stupidity saved intellectual foe potential unlivable',\n",
       " 'postponed student loan payment till january prolonged grateful nonetheless',\n",
       " 'easy tell swamp',\n",
       " 'share contribution truly loss fit description seems smothered vitriol diversion divisiveness dishonesty lifelong independent voted party impatient hoped achieved feeling cheated afraid democracy given bloodshed hatred mistreatment name calling death mayhem whether found',\n",
       " 'reason reason war beat',\n",
       " 'nominated principled supreme court judge nominated conservative suddenly break principle siding 3rd country nominated qualified principled decide case next stand possibly hundred',\n",
       " 'dragged war unlike recent predecessor',\n",
       " 'federal judge appointed serve changed',\n",
       " 'friendly negotiation korea lower drug however compare disaster presidency',\n",
       " 'AFIK war send troop overseas warzones',\n",
       " 'worry policy speech TV affect next generation answer absolutely trace origin behavior sometimes surprising basing pop culture tv conduct unfortunately affect looking today',\n",
       " 'another case TDS orange upvotes right',\n",
       " 'trashed reputation US opened eye truly corrupt system power radicalize ruling class US obviously mask quiet loud politician obama ICE kid cage paying attention comic book villain instead speech',\n",
       " 'decent decision idea NATO country paying fair share corruption impact show country power abuse show changing government',\n",
       " 'awful absolutely right impose tariff chine sanction tech gut american benefiting illegal unethical state',\n",
       " 'war',\n",
       " 'along line stopping huawei developing 5g info structure brazil',\n",
       " 'member economy virus came messed everything',\n",
       " 'incorrect paved everyman become become JK rowling war polite politician shown digital otherwise farce funny',\n",
       " 'underrated tax cut passed overall bill limiting salt deduction state local tax deduction previously total amount paid state local income tax deductible federal income tax tax cut job changed state local tax deducted effect raise tax wealthy earner high tax state tax code progressive mitch McConnell progressive hero housing rich area fallen move area detail bill',\n",
       " 'hey half',\n",
       " 'extended foster age limit 21yo implemented change US foster system including strengthening welfare program story saw virtually nowhere link partial story abcnews health wirestory sign strengthen welfare system',\n",
       " 'shown snake brainwashed turn opened eye country HOPEFULLY spark change turn positivity ya',\n",
       " 'tax bracket lower swore tax wrong',\n",
       " 'wish seems divided admit fix unless',\n",
       " 'operation warp speed',\n",
       " 'catch disagree semantics propaganda powerful tool average',\n",
       " 'nah right',\n",
       " 'discharged student loan due disability taxed income disability',\n",
       " 'damage international reputation disrupt ability project imperial antiwar destroying republican party',\n",
       " 'amen',\n",
       " 'currently reading niece book waste gotten insight asking old cousin',\n",
       " 'biased putting increasingly tough sanction taxing incoming china aswell voicing stance china china basically nazi germany disguise putting ethnic religious group camp retaking land poking india taiwan amd shown voice sanction executive china terrible',\n",
       " 'highlighted weakness society existed focus metro focus police brutality racism sexism hillary election attention hope office problem addressed',\n",
       " 'israel EAU thanked bringing peace state literally bringing peace adding war war slowly obama noble peace elected bombed droned countless civilian hostile prejudice office bush iraq afghanistan ending nearly sending troop goal troop coming home moved active war field WWII',\n",
       " 'opened due discussion climate mistrust degree worthless showcased instant smarter tackle move distraction position necessarily talked',\n",
       " 'enjoys rubbing relevant',\n",
       " 'already covered commend minded wish follow neither right driven right past irrational unmitigated hatred cost approach politics',\n",
       " 'US send troop conflict followed draw',\n",
       " 'original article saw blog follow fortunately plenty source nytimes politics animal cruelty bill html nytimes politics animal cruelty bill html blog follow usually heavily critical particular collectivity applauded reader',\n",
       " 'populist generally medical expensive capped whatever refuse clean air clean water administration increased allowable amount toxin water selling untouched alaskan wilderness gas criminal justice reform improve decreased federal protection financial advisor lie client money overall conman business designed agreeable ultimately advantage trust',\n",
       " 'bullshit country reagan biden obama',\n",
       " 'ISIS',\n",
       " 'voting glad harder china beyond speech censorship CCP literally genociding uigur blatant displacement removal entire culture trade government nazi existed today answer trade fully aware tariff trade human right right wish harder china',\n",
       " 'executive hire intern based skill degree necessarily reaching show shift valuing degree american society work force',\n",
       " 'smooth relation korea threatening bomb neighbour',\n",
       " 'killing ISIS leader removing obamacare individual mandate penalty signing hong kong human right democracy cracking illegal immigration certainly call supporter rather questionable',\n",
       " 'animal abuse federal crime strange happen',\n",
       " 'completely opposed absolutely x200b republican gutting senate ethic committee move republican tried inauguration destroy ethic committee surprise surprise right kinda called pakistan pretending ally aid money indian use detente talking pakistan airspace access afghanistan prison reform bill democrat approached prison reform labeled weak crime criminal lover fine',\n",
       " 'disclaimer playing devil advocate favour watch blow gasket poorer black community delgazette helping black community thrive ofc covid shitstorm caused economy worth different perspective',\n",
       " 'wanna asking another site',\n",
       " 'reagan another war achieved economy history covid hit achieved lowest unemployment american history defeated isi ease',\n",
       " 'supreme court pick particularly gorsuch impeccable expanded protection covering LGBT community effect',\n",
       " 'executive signed signed hear half',\n",
       " 'positively increased negativity towards US',\n",
       " 'paid gas yesterday BOOM',\n",
       " 'ludicrously terrible amount republican wake corrupt party',\n",
       " 'thanks OP posing question valid response essentially none named hear yelling red hat defend bring trade whatever told tout actual accomplishment',\n",
       " 'peace considering fighting war past',\n",
       " 'called military industrial complex multiple high ranking official within military perpetual war spill american blood sole purpose profit source youtu x0ie3a_0muc youtu vc4vywjfjne',\n",
       " 'hope mostly surround orange',\n",
       " 'showed bottom barrel scrape sink lower rise number vote staggering everybody registered vote voted republican democrat outnumber republican registration margin wider consider poll personal belief extend likely vote neo con extreme right wing anywhere near washington alone white house ifs idea',\n",
       " 'form USA heard war check',\n",
       " 'clearly plenty effect top comment latest UAE week announcing pulling troop iraq failed mention',\n",
       " 'taken ultra conservative racist movement forefront lie worried terrible undoing hope',\n",
       " 'easier identify asshole racist',\n",
       " 'ton interested concerned politics vote',\n",
       " 'tax significantly critical race theory',\n",
       " 'certainly ignorant similarly supporter wrongdoing administration change GI bill education veteran expiration date expiration date small change old policy sense',\n",
       " 'argue legalize hemp stole native land log iirc CBD business exploded video cuyahoga fall sell',\n",
       " 'OP recommend researching seems mildly progressive beneficial passed presidency outside deregulation bill passed stuck congress republican blocking passing reply top comment',\n",
       " 'increase tax trade country china preventing leaving US',\n",
       " 'accomplishment club accomplishment online',\n",
       " 'definitely peace UAE israel severely ratcheting tension country already looking thinking joining peace hand thats definitely concrete',\n",
       " 'isi completely destroyed massive terrorist organization threat ur dirty little cave protect massive bomb overnight isi obama power vacuum destroyed democrat leader obama clinton negotiated tried barter sell firearm instead killing x200b planned attack country course zero planned attack carried whatever islamic terrorism thoroughly WHOOPED',\n",
       " 'trash prof ignorance high',\n",
       " 'supporter inspired traditional non voting group vote depend actively intended hope',\n",
       " 'recently brokered historic peace agreement remember pharmaceutical abusing significantly lower prescription medicine',\n",
       " 'glad agreed declassify CIA kennedy file',\n",
       " 'achieved incredible inherit billion manage achievement imagine incompetent',\n",
       " 'vote republicants',\n",
       " 'corona lifetime effect',\n",
       " 'pas requiring welfare pas drug test regular basis welfare somewhat welfare abuser',\n",
       " 'remotely upvote attempt hear separate viewpoint involve trashing',\n",
       " 'disagree american shined light show american political system knowing damage using position hopefully inspired next generation vote loud tampering hopefully next generation coherent steadily progressive democrat voted power',\n",
       " 'farm bill legalized hemp federal ending war agaisnt hemp acutal progress towards ending war drug',\n",
       " 'work pedophile porn',\n",
       " 'timeless rock bottom prevents ilk elected sort right protects US future',\n",
       " 'wellnhe gay trans right window',\n",
       " 'sorry impossible change based',\n",
       " 'yeah accomplishment directly overseen stuff work completed',\n",
       " 'add UAE israel peace accord',\n",
       " 'wealthy benefiting tax policy argue broke dirt sending check IRS tax save damn dime government tax paycheck owe tax forget tax return spectrum',\n",
       " 'wanted post guideline presidential power grey area clearly defined wallowing smell racist sexist bullshit light empowered number defend racist sexist bullshit wont pull remark x200b service biggest hat possible leading follower light',\n",
       " 'rolled water united state enacted obama river stream culvert ditch federal control massive overreach federal authority duplicate regulation state regs fine rolled obama horrible interpretation title XI apply sexual assault campus obama required create kangaroo court steamrolled innocent men effectively presumed guilty study obama relied change meant author study extraordinary public announcement obama misinterpreting study',\n",
       " 'buffoon reinforce meaning behind quote purpose serve warning others',\n",
       " 'space force stolen star trek logo qualify administration accomplished biggest economic crisis',\n",
       " 'saw signed animal abuse federal crime felony universal compared imagining swear saw somewhere',\n",
       " 'success subjective gas industry exec street appreciate regulation',\n",
       " 'presidency reminded importance limitation presidential government power okay obama hot seat sudden realised hand hand',\n",
       " 'war join war withdrawing troop overseas',\n",
       " 'biggest IMO presidency gotten young pay attention politics importance everyday',\n",
       " 'cant speak USA european favour TTIP CETA negotiation',\n",
       " 'tax cut job high earner high tax state didnt unemployment GDP respectively office unemployment GDP pandemic ISIS decimated troop coming home john bolton james mattis spoke war responds terrorism force saw iran known country messed brokered historic peace israel UAE forcing country bill UN NATO signed bill veteran ability seek private medical cost VA claim processing efficient minority unemployment record low pandemic thanks opportunity zone billion poor community winning trade war china war cared fight replaced NAFTA trade worked fight discrimination persecution LGBT across elected supporting gay marriage signed executive lower prescription drug forcing hospital disclose pricing effort bring healthcare cost increase transparency',\n",
       " 'war afghanistan present iraq war war west pakistan present war terror present operation ocean shield operation observant compass american led intervention iraq present operation inherent resolve present yemeni civil war present american intervention libya present war republican democrat humanitarian nightmare combine led death ten innocent military clusterfucks hurt fault another war perhaps impeached needle death inside US obama bush face gallows killed outside',\n",
       " 'hows 401k value today v obama bitch',\n",
       " 'gonna provide rather explain united state become polarized taken extreme subject depending source either terrible infallible incredibly unbiased claim accomplishment compound lazy research source likely biased',\n",
       " 'korean war facilitated peace israel UAE drag armed conflict despite democrat attempting impeach moment hand bible gay marriage entering office obama gay marriage midway election campaign laughing nominated nobel peace obama peace obama old joke obama george bush stupid dishonest justification basically george bush grasping straw case outright lying twitter echo chamber echoed amplified remembered devolving tribalists taking attitude must reading sound unironically voting non career politician become gigantic',\n",
       " 'american outdoors',\n",
       " 'recent nobel peace nomination addition israel UAE nomination letter nobel committee cited mr key role facilitating contact conflicting party kashmir border dispute india pakistan conflict south korea',\n",
       " 'wealthy oligarchy impact powerless racist poor desperately powerful loving light country fire troll liberal',\n",
       " 'key word impact ton negative impact country currupt deeper downward spiral market crash',\n",
       " 'outdoors np gov subject legal american outdoors htm',\n",
       " 'clearly divide stand managed expose kind disgusting hat bumber sticker identify po tell idiot wear mask wrong public',\n",
       " 'christmas eve federal hisay dope',\n",
       " 'federally legal hemp system obviously passed congress administration',\n",
       " 'challenge renewed veteran choice slight modification name change original version signed obama military daily sign billion bill replace va choice program html specific quite short course credit original idea mention effectively renewal extension original e lied origin managed tarnish remaining messed malicious',\n",
       " 'lower american population',\n",
       " 'animal abuse felony',\n",
       " 'zero input anyrhing possibly impact brain power toddler',\n",
       " 'change objectively correct',\n",
       " 'problem negative impact outweigh margin',\n",
       " 'korea problem completely table',\n",
       " 'obama ISIS rampant constantly korea conflict kim jong un talking possible war III threat cycle barely talked pressing currently',\n",
       " 'american korea regardless policy',\n",
       " 'effect american economy regard standing country ripping US stealing intellectual property US obvious happening short seem painful unpalatable succession previous leader perpetrator persist gradually worsen behavior',\n",
       " 'send additional troop battle decreased overseas troop troop home',\n",
       " 'clear govt dependent tradition politician rather enforceable',\n",
       " 'recent peace israel UAE monumental',\n",
       " 'entire economy covid suck anyway',\n",
       " 'success country virus instilled fear country bogged political non sense independent acknowledge either success noticeable amount recently',\n",
       " 'bit early judge impact surprise record voter turn november despite pandemic obviously source exactly early tell impact direction',\n",
       " 'absolutely loathe spirit post earlier week found interesting politico magazine superfund activist minden wv titled incredibly green EPA remediate superfund site administration bit mixed bag white neighborhood disproportionately prioritized completely ignored orphan site cleanup toxic waste dump undoubtedly clinton program EPA ruthless refusing settle penalty responsible party administration spectacular taxpayer bailing criminal corporation government agency either pay liquidated grew within five mile three noxious site zero sympathy heinous criminal whether chemical navy site responsible thousand slow painful poisoning death corporate serial killer thanking profusely throwing gulag belong instead fining worthless paper complete cleanup CEOs general clean waste bare hand handmaid tale credit american outdoors legislation nature grand accomplishment veto bill overwhelming credit promotion peace iran policy trainwreck dangerous brinksmanship luck iranian government wisdom prevented escalating WW3',\n",
       " 'destroyed ISIS acknowledge response chinese threat existential threat previous combined kind joke american politician anyway nonetheless',\n",
       " 'farm bill legalized hemp lawyered',\n",
       " 'passed bill animal cruelty felony',\n",
       " 'pick worry korea nukeing',\n",
       " 'although certainly attempt american educated supposed legally greater understanding position within administration filled security clearance granted fire hell emolument',\n",
       " 'expanded range qualified smart otherwise dissuaded presidency hey directly pointed quick past war war non intervention hallmark carry next administration',\n",
       " 'pull couple thousand troop syria debatable whether ultimately situation dunno',\n",
       " 'presidency volatile public politics net obviously downside vigilante misinformation',\n",
       " 'historic israel UAE treaty negotiated US unemployment low removed critical race theory taught government caused divisiveness including segregation dorm event however continuing taught pulling troop afghanistan bringing home add plenty dinner eat',\n",
       " 'opposed harvard affirmative action case SFFA blatant bigotry asian american boggling reason actively despite wokeness',\n",
       " 'conservation import medication country war actively bring troop home',\n",
       " 'thus american aware voting',\n",
       " 'signing bill transparency healthcare healthcare entity must itemize charge bill accurate picture stupid healthcare bill address systemic cost right legislation terminal patient fuckin send wacky experimental treatment lastly animal abuse legislation added penalty enforced animal abuse top',\n",
       " 'shown flaw within political system urgent fix competent full advantage',\n",
       " 'ate citing mostly lucky didnt',\n",
       " 'furthered peace relation korea killed abu ackbar lifted homelessness certainly useless',\n",
       " 'top administration played forcing hospital insurance disclose pricing common procedure front consumer insurancejournal national htm',\n",
       " 'country divided everything right another equally heart republican obama vacation democratic killed bush acknowledge ounce fixing NADTA funding HBCUs perpetuity funded beyond write letter congressman holding china accountable american class china consider presidential country perspective',\n",
       " 'obvious watching funded national park given money repair donated dime pay different charity education fund moved embassy jerusalem deduction average american tax froze student loan interest recently UAE israel sign peace treaty country area follow suite UAE son played hand prison reform attempted police reform gave mattis needed defeat isi rid soleimani easily anti war generation top loud mouth damage reputation wide country taking advantage kindness presence anyways biden harris choice horribly hypocritical',\n",
       " 'become politically aware engaged',\n",
       " 'nomination several federal judge literal interpretation constitution conservative protect fundamental freedom right established constitution check work federalist society',\n",
       " 'bet balance',\n",
       " 'amount money towards helping sex trafficking',\n",
       " 'managed isolate US ally edit didnt bit',\n",
       " 'aussie hot war internal',\n",
       " 'knew expected quite literally role tearing country apart constitution decency devil atheist',\n",
       " 'blatantly corrupt winning presidency despite popular choice identified urgent reform government top bottom definitely',\n",
       " 'broke clock right',\n",
       " 'trending',\n",
       " 'dominionists penny pompeo happy move jerusalem embassy jesus freak game theory',\n",
       " 'israel UAE sign peace',\n",
       " 'renegotiated trade mexico canada NAFTA didnt sign TPP scaling overseas troop presence NATO member spend defense hospital transparency removed individual mandate animal cruelty federal offense CBD hemp legalization small business buy insurance group pricing right increase energy export EU VA reform edit UAE israel promiseskept achievement overview justice',\n",
       " 'tax health insurance',\n",
       " 'okay mode appended altruistic reason sentence',\n",
       " 'tariff raw steel production US however stressful agriculture',\n",
       " 'removed semblance credible deniability GOP authoritarian racist party coward sycophant criminal defeated roundly held accountable',\n",
       " 'absolute shining glaring spotlight hypocrisy right wing christian evangelicals right',\n",
       " 'donated entire presidential salary variety inauguration VAs education service plenty obama convinced threatened forced bullied mexican government drastically tighten security southern border caused decrease illegal immigration peace israel UAE moving embassy jerusalem gonna light islamic radical fire yeah obama managed turn libya modern air slave market eh changed engagement effectively fight ISIS ISIS crushed month obama engagement killed preserve civilian corruption highest NSA FBI politician spying rival hillary spying save forgo future election opponent campaign staff email phone call text extreme bias soooo spewing crap russia collusion turned ZERO EVIDENCE COLLUSION clinton bush obama YEARS china stealing intellectual property job call',\n",
       " 'war',\n",
       " 'dat stock market doe',\n",
       " 'unintentionally opened dialogue racism equality NFL player field attention led NBA player recently taking stance playoff antagonizing movement strong',\n",
       " 'tell information information',\n",
       " 'change state',\n",
       " 'flaw american political system desperately fixing glaringly obvious hope another four place solidify',\n",
       " 'decoupling china needed concentration camp whoever next biden carry thrust political limelight democrat republican tough china serious impact everything else crap',\n",
       " 'focus bringing pedo ring outpaced past grouping consecutive',\n",
       " 'clearly unequivocally shown corruption flaw country government country use presidency change necessary avoid election another leader ilk work towards eliminating rampant corruption within government',\n",
       " 'heap bullshit',\n",
       " 'choice decision USA proven delusional decision biased little common sense',\n",
       " 'foreign policy overwhelming success metric',\n",
       " 'joe biden change united state case jojo2020',\n",
       " 'managed cool korea v west feud matching kim jong un crazy told china despite despite everything overall fighting powerful evil government',\n",
       " 'disagreed korea happening fallen apart',\n",
       " 'raising smoking age',\n",
       " 'file manila envelope titled',\n",
       " 'troop withdrawal historic israel UAE peace war past job united state minority',\n",
       " 'yes given meme obama',\n",
       " 'politics',\n",
       " 'objectively decision interest postal treaty disadvantaged USPS relative country backed war outweighed objectively reason honest right accidental',\n",
       " 'hospital must disclose starting progress toward border protection deportation limit population control climate otherwise killing political correctness withdrew troop proposed peace brokered peace treaty israel palestine negotiated trade agreement mexico canada china killed NAFTA replaced USMCA donates salary work free economy booming lowest unemployment highest stock market corruption human trafficking US energy independence jailed human trafficers',\n",
       " 'delight pay exorbitant monthly student loan bill month april extended december',\n",
       " 'forget immigrant canada spent stepping career move NYC adventure halted visa work studying ultimately affected negatively situation impact date international perspective staying NYC watching eye opening badly governed pray NYC rise stronger place',\n",
       " 'beg listen carefully remember share bill moyers white house press secretary johnson administration historian political commentator discus mike lofgren congressional staff member capitol hill accountant regarding inner working finance power wealthiest powerful nation earth discussion forth public broadcasting service relevant hiding plain sight american deep state pb org video moyers deep state hiding plain sight pb org video moyers deep state hiding plain sight rather changing video broaden showcase truly ran powerful vested interest served yield whichever party occupies executive branch four television noam chomsky asked american foreign policy',\n",
       " 'mediate armistice south korea else',\n",
       " 'historic UAE peace treaty doesnt count',\n",
       " 'weigh pet project israel embassy move total destruction barack obama devisiveness engenders difficult value cost scenario presidency',\n",
       " 'radical party headed biased ridiculous',\n",
       " 'american outdoors',\n",
       " 'aware nazi woken white moderate type mlk disparaged hundred globally admittedly split keeping myopic technically',\n",
       " 'admit threw wrench american political cycle paying attention otherwise remained undisturbed voter turnout projected biggest election unreliable voter planning voting face enemy american facing right class struggle',\n",
       " 'dumbest imaginable diplomatoc progress korea corporate tax break leading booming economy tough china break',\n",
       " 'suck biden suck suck obama ball doesnt using em wrong',\n",
       " 'unbelievable arguably withdrawing TPP mentioned',\n",
       " 'waiting claim spoiler alert proof claim',\n",
       " 'sound parliament leadership worked id assume worked member write bill form voted representative otherwise handedly US dictatorship',\n",
       " 'tariff china impact US outside hell',\n",
       " 'depends individual politics judge negative please rape presidency gotten VA reform veteran idk behind ton evangelicals causing half state old girl raped weird uncle incest ameirca approach cult follows bunch potato',\n",
       " 'leftist wanted hillary impact spotlighting progressive alternative centrist status quo unfortunately DNC rigged primary bernie twice progressive momentum shot liberal elite',\n",
       " 'post op admits stupidity',\n",
       " 'military temporarily place assigned base compensated reduced changed',\n",
       " 'massive federal deregulation transparency hospital competition generic drug canada anti war peace worldwide tax cut job removal obamacare mandate withdrawal paris accord complete revamping NAFTA USMCA stand edit website outline promiseskept recent justice reform',\n",
       " 'pardon snowden',\n",
       " 'jewish',\n",
       " 'ok pandemic fell apart overtly racist corruption aired several credible close',\n",
       " 'PRC learned official position communist government mao zedong liked using describe leader complex legacy decision implication inverse napoleon bonaparte course abe lincoln ghenghis khan figure ago asked legacy obviously credit mass murderer g khan religious tolerance award founding space force',\n",
       " 'revealed idiocy electoral shown system check balance work proven idea electing businessman office horrible idea lesson choose learn',\n",
       " 'change effect entertainment industry recent uncomfortable marriage politician shone light relationship IMO largely thanks normal healthy skepticism previously lost',\n",
       " 'OP particularly israel game iran',\n",
       " 'presidency field working hire instead bringing cheap labor india friend experience found paying job due high demand skill low payed labor home',\n",
       " 'rescinded dear colleague letter gave accuser legal right kangaroo court obama ruling UAE irasel historic carter enter conflict tax cut opportunity zone poor minority area',\n",
       " 'korea right anymore military idustrial complex boogie',\n",
       " 'kinda humbling call USA nobody mexico canada american alone hemisphere terrible needed humbling contrition',\n",
       " 'argue job deescalating tension korea mostly generally speaking militarily move everything situation atleast average american',\n",
       " 'push china change including run becoming independent china american run excuse tax cut thr majority american tax cut appreciated',\n",
       " 'opened conversation diplomacy DPRK korea meet korean leader dick count regardless popular douche',\n",
       " 'ITT excellent basic black white non nuanced explanation thorough critical analysis immediately rejected felt quote deeply reading thread little information dangerous',\n",
       " 'genuinely dont insane cant country dislike possible actual country willful ignorance',\n",
       " 'invest business set income',\n",
       " 'holy changed wow',\n",
       " 'sending troop sea killed obama bush',\n",
       " 'problem argument giving explain seem rather situation advised',\n",
       " 'twitter shown basically constitution run honor system worry smarter politician next teeth',\n",
       " 'federal employee paid leave giving federal employee mother father week paid leave birth adoption',\n",
       " 'emboldens BLM movement violent protest vote pure genius',\n",
       " 'socialist voting joe biden likely vote socialist party candidate however government employee HR federal employee gave maternal paternal leave obama gave biggest annual pay raise career signed executive calling federal hiring reform badly needed government',\n",
       " 'literally nominated nobel peace brokering sence carter literally korea stepped korea guard sitting united state nafta beneficial united state joe biden claim nafta supported baned critical race theory government organization racism low expectation claiming white construct poc expected held standard white argue divisiveness activated pushing ether vote generally government',\n",
       " 'policy change consider EO kidney disease whitehouse gov presidential action executive advancing american kidney health vox raved vox future perfect triump kidney disease transplant EO highlighting save thousand largest government action kidney vox followed additional coverage vox future perfect kidney transplant requirement white house seem x200b thinking broadly health seems fairly approach hhs gov site default file reforming healthcare system choice competition pdf helping',\n",
       " 'china korea rival china emerging dominance fighting rocket problem honest knowing democrat republican cult',\n",
       " 'energized entire generation politically',\n",
       " 'paid parental leave giving federal employee mother father week paid leave birth adoption',\n",
       " 'hot',\n",
       " 'generally loathe stand approach forced dispassionately campaign bit hated conservative obama hated liberal although fair arrogance combative nature difficult task obama official memory US politics conservative happy appointing gorsuch kavanaugh iran obama enrichment uranium nuclear weapon glad withdrew nuclear iran kind crazy withdrawal impact suspect replace sometime near future controversial politician agreed earlier border security closed border essential national security economic stability glad building process immigrant honestly immigrant',\n",
       " 'reason listed impact hmmm political lawyer career politician hopefully outsider run office career politician useless',\n",
       " 'reduction bomb excluding afghanistan along anti interventionist reducing troop bringing plenty home calling military racket ran pentagon',\n",
       " 'drawback troop impact',\n",
       " 'baiting fudds high blood pressure medication handle additional strain',\n",
       " 'supporter recall looked deep pas tougher penalty animal abuse wildly character move grateful',\n",
       " 'move US embassy jerusalem permanent job voted 90 ball',\n",
       " 'space force definitely idea kicking',\n",
       " 'escalation military conflict country showed tremendous restraint iran attacked military base injured troop',\n",
       " 'voting',\n",
       " 'rid healthy lunch initiative kid absolutely fruit vegetable lunch exception idea behind WASTE school trade type program kid food eat leave kid wanted work donate local farmer throwing hundred pound uneaten food waste reduced half fill garbage bag',\n",
       " 'wrong corruption country rug delivered red white blue platter signed name acknowledging corruption actively working wise yeah piece',\n",
       " 'successful keeping US endless war thank',\n",
       " 'pay tax insurance liked universal healthcare truly obama change access preexisting condition otherwise insurance profit taking insurance pay tax',\n",
       " 'investing newest generation nuclear power instead subsidizing coal natural gas environmental protection important imo',\n",
       " 'tax reform slashed tax class lowered corporate tax employment number',\n",
       " 'imo ridiculous system become flaw abuseable power',\n",
       " 'beef pound today',\n",
       " 'grate',\n",
       " 'human trafficking arrest trafficking arrest promised bigly',\n",
       " 'flaw system government revealed among friend complete piece',\n",
       " 'young interested politics impact country',\n",
       " 'week parental leave parent working federal government previously mother unpaid leave sick leave PS baby girl expecting november',\n",
       " 'stir undo industrial outsourcing china sabotage neoliberal gimmick beyond class asshat',\n",
       " 'change',\n",
       " 'signed forever GI bill meaning veteran GI bill longer expire',\n",
       " 'method checking confirming claim pro con',\n",
       " 'indirectly work culture dramatically shift remote potential',\n",
       " 'entertaining cared watching press conference reading twitter watching state union address fun honestly politics',\n",
       " 'payroll tax cut foot faster',\n",
       " 'post period lock',\n",
       " '000th line opinionated liberal absolutely despises everything associated considered outcome produced conclusion properly china adversary initiated strong response threat tariff strategy effective opening salvo likely increase national importance coming',\n",
       " 'heard opportunity zone en wikipedia org opportunity_zone wprov sfla1 initiative passed tax designates particular geographical area mostly urban area economically stagnant served business defer portion tax directly tied profit income generated serving zone developing kind pissed garbage partisan job reporting particular incentive edit hyperlink formatting',\n",
       " 'chew argued meaningful US nixon achieved accomplishment impact despicable',\n",
       " 'late game DJT minority national budget instead waiting fed money',\n",
       " 'public future potentially grit engenders creation pearl aside quaking western reckon implication learned behavior animal moral philosophy biology converge conclusion intolerable traditionalist unknow die attrition sometimes mature',\n",
       " 'credit due IMO due narcissistic reason another predominately muslim country recognize israel recently serbia recognize country kosovo serbian leader signing achievement considering genocide occurred ago small impression multiply reason reason biggest immigration xenophobia propaganda loving russia NK MBS turkey tyrant loving removal US troop discussion europe asia stabbing kurdish leaving syria vocally condemning prejudice action mismanaging current pandemic admitting fault edit larger add comment likely right',\n",
       " 'rebuttal donated salary donates salary avoid legal repercussion taking money taxpayer salary financials taxpayer review citizen standing sue tracking action declared intention run citizen profit decision',\n",
       " 'kinda terrible greatly national park',\n",
       " 'hope source included trajectory otherwise seem flattering statistic direction country headed office',\n",
       " 'source judge smaller court system fair trial deceit evil court system heard case judge judicial court hear main stream newspaper american avid supporter super stuff country easy opinionated biased reaction calling god fault kinda wack',\n",
       " 'eastern peace important calmed tension gave jersualem recognition country',\n",
       " 'briefly speaking vocational training program high school ton DeVos harm role hopefully future funding allocated program',\n",
       " 'spelling favour favor labour labor obviously british odd',\n",
       " 'mentioned quite bit space sector NASA commercial partner space force granted mike penny figure hear multiple space directive allocation fund yearly budget appointing administrator jim bridenstine decisive figure paying attention artemis program hoping country towards apollo mission 60 70 digress policy rhetoric massive improvement obama era undeniable god continues impact future space exploration unseen wonder quality',\n",
       " 'tax cut enormously healthcare removal tax penalty relief obama directly definitely directly hurt',\n",
       " 'subjective supreme court pick perspective disagree',\n",
       " 'politics general politician garbage exist money flow money flow involve corporation black tell whatever hear elected money flow pocket friend pocket regard coach youth football believed learn coach sometimes lesson little humorous press pause bother yes OP bother bullshit name ok suppose classroom memorize name historical significance history pandemic MLK killed rodney king trial unremarkable',\n",
       " 'china trade conversation politicized dealt apple store selling fake apple product across china apple recourse chinese government unwilling enforce patent related crime american blatant wrong fixed trade china tax reform impact economy individual sort eye beholder',\n",
       " 'animal cruelty federal crime else',\n",
       " 'gonna moving embassy jerusalem',\n",
       " 'brokered israel UAE normalizing relation saudi arabia israeli origin plane use airspace modern history lead war',\n",
       " 'legalized hemp k',\n",
       " 'recently negotiate diplomatic agreement UAE israel fully normalize relation country includes agreement prime minister netanyahu suspended annex multiple west bank forward call peace obviously agreement peace right direction',\n",
       " 'shown potential presidential candidate',\n",
       " 'seagull sweep hairstyle fashion',\n",
       " 'identifying corruption front stage',\n",
       " 'stock check market trend',\n",
       " 'tell none tell joe biden tough china',\n",
       " 'impact determined office undone bunch policy prior next undoes policy change thread pointless',\n",
       " 'peace',\n",
       " 'supposed change factual statement post scam upvotes',\n",
       " 'deleted',\n",
       " 'admit animal abuse felony obviously administration treatment human specifically south border',\n",
       " 'harris becomes fuckydoo',\n",
       " 'biden kamala held significantly longer kamala jail weed possession use basically slave labour forest fire biden word remember oohhh anyway gotta nap word mine',\n",
       " 'showed joke word country absolutely completely change warning',\n",
       " 'dumb american partially hidden underbelly inherently flawed system nazi v communist neither eye eye based upon adaptive web browsing longer opposing google search based upon cooky',\n",
       " 'giving prop job presidential stuff impressed required position praise performing task job requires expected',\n",
       " 'raising tobacco nicotine age',\n",
       " 'pen holder stick pen stupid money',\n",
       " 'gave meaningful tax break retirement pulling investment stock market spend money money circulation stronger economy gave meaningful tax break dependent limited tax write offs high valued home renegotiate NAFTA preemptively placed COVID vaccine important vaccine producer produce limited number vaccine period placing enormous gear equipment vaccine become approved churn production run short period amount money US government forth towards COVID vaccine US poised successful vaccine available imposed tarriffs foreign successfully negotiated china trade ongoing phase signed incentivized produce US',\n",
       " 'definitely voting reading',\n",
       " 'administration blame credit whatever happens right vein american outdoors greatest piece conservation legislation passed generation nobody',\n",
       " 'digging unemployment record low presidency ncsl org research labor employment national employment monthly update aspx p continuously grown signal market performing macrotrends net sp historical annual return deported obama administration cato org blog deportation historical perspective GDP continuously grown presidency whereas obama remained stagnant period regressed bit statista statistic annual gdp growth united state edit added source',\n",
       " 'either umbrella administration rhetoric toxic base face hardly tolerate change US kid double edged sword except butter knife kitchen knife lightsaber',\n",
       " 'paid grand tax suck curved vein',\n",
       " 'hope post comprehensive summary comprehensive summary honestly dislike betsy devois appointment',\n",
       " 'publish story biden china anti russia',\n",
       " 'EARN NEGATIVE impact',\n",
       " 'thank post seriously wanted starting fight thank',\n",
       " 'DJT entirely evil motif tad askew imagine vietnam POW grew everything upbringing father fred pure trash sense somewhat capable empathetic instead sociopathic different today applied adolf hitler father halfway decent human raised nurture instead nature adolf grown different sigmund freud greater importance father figure woe share derived negligence abuse others preaching sow division none exist capable feeling bleeding dying death discriminate message abuse wrath karma endure',\n",
       " 'eh korea',\n",
       " 'CPA dealing corporate tax tax cut job passed biggest piece tax legislation changed landscape international taxation change deduction tax accountant quite valuable however effort driven mostly congressional republican rather tax passed office stand republican certainly disagree tax policy certainly job security',\n",
       " 'apathetic politics',\n",
       " 'war compared',\n",
       " 'uae israel alone insane event',\n",
       " 'donated salary proved seeing tax return',\n",
       " 'action gotten vote election lose',\n",
       " 'subreddit called r changemyview r myview',\n",
       " 'deserve peace work korea general definitely flaw stride peace',\n",
       " 'politics science objective right wrong fix climate drive electric car cost gasoline job money money buy cheaper dirty stuff return CO2 emission kept gasoline car replaced electric organically organically brainer buy therefore enforcing electrification essence republican market fix problem solution become valuable provide filthy rich free country free government dictation course US perfect closest republican leadership intervention intervention',\n",
       " 'twisted exploited presidency possible past relied tradition fundamental understanding cannot despicable human leader honestly next various maker student history work fix exploit close gap country decent diplomatic tradition granted tradition however ramification perhaps enforceable place prevent happening',\n",
       " 'dunno hundred solid kinda odd shittiest shitty accident',\n",
       " 'war economy france weird french government LVMH investment US beginning',\n",
       " 'hooo wheeee comment looked proof found light outweighs wish kinder',\n",
       " 'remember month gas dropped thanks',\n",
       " 'damn subreddit discus hear eachother',\n",
       " 'starting war invading country',\n",
       " 'arguing china china policy blowing american worry country future luck wrong move china onwards cost dominance',\n",
       " 'iran nuclear ticking bomb pun intended economy track c19 became energy independent',\n",
       " 'neither changed',\n",
       " 'bash neutral anymore',\n",
       " 'CHILDREN pushing election JOE BYE',\n",
       " 'scrolled commenting mentioned ripped TPP seems forgotten vote dont earned respect short memory TPP awful agreement screwed american GODDAMN INTERNET push TPP reason cared barak throw trash thus keeping internet relatively free argued installing ajit pai position power countered internet asked trashing TPP ruined afterwards remove',\n",
       " 'taking crazy pill',\n",
       " 'plausible researching impact tell feeding',\n",
       " 'question flawed absolutely none listed administration credit god',\n",
       " 'line stance china right obama proposed policy liberal accepted necessary contain authoritarian government likely committing genocide biden recognizes policy likely similar wsj article whats bidens china policy',\n",
       " 'willing accept perspective comment section showed piece faith humanity chaotic ruined 21st century thank wonderful living',\n",
       " 'depends COVID',\n",
       " 'gotten potentially dangerous radical group country identify mockery ridicule bullying public problematic behavior avoid content aware clickbait disconnect title content delegated governor acted various shown update structure government work instead incumbent crony',\n",
       " 'fail learn succeed learn fail particular anymore',\n",
       " 'right direction abolishing puppy mill gotten supposed sign animal abuse',\n",
       " 'impact remember',\n",
       " 'change straight factual statement',\n",
       " 'giving 200k breaking emulments clause joke answer change stick parlor trick',\n",
       " 'admin IMO creation qualified opportunity zone zone established department treasury IRS tax tax cut job opportunity zone census area historically struggled lower economic indicator essentially federal government giving investor business tax incentive establish business zone approach stir growth area struggle economically experience estate development growth follows growth continue policy moving forward effect clear another',\n",
       " 'repealed infamous dear colleague letter mistake obama presidency department education sent letter suggesting become arbiter sexual misconduct conduct hearing using preponderance standard lowest standard evidence accused student entitled due process hearing charge evidence counselor available suggestion administration strongly implied cut federal funding school course university across country complied',\n",
       " 'clarify obama political impact USA attempt objective bringing right obama focused current era',\n",
       " 'signed music modernization leading establishment licensing organization administering blanket mechanical publishing license set organization responsible ensuring copyright owner paid song streamed service spotify apple music streaming service avoid infringement lawsuit due lack ownership data MMA allows artist paid pre release seems received music industry',\n",
       " '',\n",
       " 'lmao OP consider response',\n",
       " 'conservative prison reform bill speaking RNC spark note change prison system pardon handful non violent drug possession crime little early heard somewhere kim kardashian source',\n",
       " 'ASAP rocky freed',\n",
       " 'already mentioned bump stock ban gun control',\n",
       " 'majority answer seem majority american',\n",
       " 'preface statement supporter special interest excelled picked slack behind past administration ignorance underestimation respectively VA massive stride fall short mark course correction militarytimes pentagon congress va program percent funding boost fiscal budget stripe veteran budget call another increase va spending granted publication shut asked impact potentially four',\n",
       " 'monkey throw air occasionally land deed',\n",
       " 'impact swath stupid given traction preach stupid donny enabled viewed dumb viewed patriotic impact voice selfish dumb voice silenced silenced ago dumb revolution festering donny',\n",
       " 'gave TPP bullet australian trade pact basically US arse whenever totally australia interest given board political class populated completely utterly craven fuckwits former estate agent edit unlikely war russia plus physical spiritual taste cock holster edit along lady tit plus YMMV',\n",
       " 'somewhat cynical managed society clearly transparently work nation climate economy tax institutional systemic racism KKK nazi wannabe living amongst uneducated basic science critical thinking structural safeguard political appointee william barr idea problem country address taking problem exacerbating shown sorely attention yeah covid add sense showing public health threat seriously address',\n",
       " 'broke work civil name impact',\n",
       " 'retracted terrible dear colleague letter obama admin place expelled university weakest accusation',\n",
       " 'mentioned changed insurance required pay telehealth service COVID previously hospital providing telehealth money additionally HIPAA security reason telehealth sometimes drive clinic use telehealth booth use smartphone heard freakonomics podcast week nuanced motion within bill signing',\n",
       " 'gotten participate political process next informed participation',\n",
       " 'impact failure outcome sole utter incompetence attempt healthcare trans SCOTUS LGBT workplace protection ruling protected status previously ruling explicitly mentioned anti discrimination',\n",
       " 'decrease tax burden attract 8k millionaire average per debt gdp growth obama sent increased deported illegal immigrant obama substantial amount removed fine health insurance effectivelly removing poverty tax removing troop safe affordable manner executive unemployment congress playing politics passing cdc eviction deferral december woman employed men US citation google scholar plain google',\n",
       " 'china h1 b visa disaster right biden continue tel china shove',\n",
       " 'establishing UAE israel count stimulus check executive reduce prescription drug process implementing tax small business owner entrepreneur encouraged financial freedom relate economy somehow holding pandemic moving forward space exploration NASA budget higher excite space blast past coal miner lose job imagine job loss scale devastating coal losing job',\n",
       " 'thread ton accomplishment given credit current aloooott',\n",
       " 'disabled combat vet VA dramatically improved election local provider received faster referral',\n",
       " 'tripping bar low expectation plenty plenty star war movie total hot garbage tho',\n",
       " 'according ultra rich killing',\n",
       " 'wrong attrackted daughter ur wife daughter legalized separation thousand disappear w explanation total theory definetly likely run sex ring includes immigrant buy pic',\n",
       " 'FedNow system CBDC digital dollar blockchain IoT AI program launched compete',\n",
       " 'rather maestro death thousand cut',\n",
       " 'immigration policy largely failure rhetoric public profile anti immigrant rapidly reduced illegal border crossing country couple',\n",
       " 'sign executive TPP call',\n",
       " 'deleted',\n",
       " 'impact hopefully generate momentum create change within political system change focus abuse legally fix',\n",
       " 'yes shown congress corporation',\n",
       " 'thinking regularly caught hyper partisanship destroying american society difficult credit leading hyper partisanship symptom symptom firework pier port beirut ammonium nitrate tonnage lit credit following flying blind ChiMerica partnership china watched chinese action movie upon china jet li china west grievance scorn contempt humiliation history foreign power including US military diplomatic influence carve piece china economic goal enormous national shame chinese history chinese communist party opened free market reform intention becoming western style democracy human right considered pain CCP considered intellectual property theft problem chinese lifeblood business none american administration administration kept ignoring manufacturing base dried rust belt rotted asshole enormous policy change deserves credit killing qasem soleimani low cost high impact foreign policy national security move averted war fell stroke attacking troop iraq iranian regime hubris told US turn pride fall paid little killing showed iranian others region force reckoned despite drawing military presence anti war seem counterintuitive genuinely seems aversion starting military action honest american military industrial complex heard lifetime calling pentagon endless war money military contractor biggest problem bigger personality massive insecurity policy work necessary run country country country united state everything quick solution photo ops lip service praise seeking bogged detail hack problem played grand scale covid formulate national capable thinking macro sense seems focused instant gratification within cycle instead goal work running sort organization',\n",
       " 'showed hurt soul lay',\n",
       " 'gorsuch supreme court justice',\n",
       " 'strong dislike rid NAFTA originally effect clinton problem angry rid rid obama praised',\n",
       " 'unaware unprecedented peace brokered nominated nobel peace',\n",
       " 'case DJT contribution exploited loophole system allows corruption possible government response tossed aside politician disown politics corruption write prevent future hope',\n",
       " 'deny us pr stunt convince others vile',\n",
       " 'practical administration distinction previous couple else name changed specific shifted system chug along broken already status quo remains king difference invasion american authorised invasion',\n",
       " 'uh completely different approach informative answer totally funnier laugh cry scenario genuinely average humour social remember funnier dependent friend group anyway serious political gotten funnier fake reservation rally funnier protest whatever bygone era obama bush ludicrousness entire presidency existed outrageous available little easier laugh genuinely concerned seems defaulting humour rather anger obviously anecdotal aware social nowadays genuinely laughter considering tolerance shenanigan risen dramstically insinuated power delay election remain power dangerously suggested cure virus disinfecting inside meh sound tuesday obama wore tan suit bowed lost everloving press conference dressed speedo chinese flag talking monday',\n",
       " 'economy modern history stock market incredible high funded black black unemployment lowest withdrew troop abroad domestic job opioid epidemic low lowered tax cold korea historic peace UAE israel accomplishment top missed couple',\n",
       " 'outside enjoy watching beeing dictator clown amazing watch',\n",
       " 'consider right center value decided run jaded right upon taking TPP signed bill animal abuse felony clean trash pacific signed artist royalty streaming service credit greater',\n",
       " 'gave meme',\n",
       " 'putting china microscope US internationally greatest impact',\n",
       " 'obliterated country gross incompetence',\n",
       " 'honestly peaceful repaired relation korea actively pursuing peace killing solemani baghdadi amazing medicinal drug sooooo affordable paying fraction originally pay afford pay medication',\n",
       " 'couple foreign policy domestic foreign actively decriminalize homosexuality country homosexuality punishable minimum prison deescalated tension communist korea capitalist south korea set foot korean soil action interpreted war assassination spot saving crossed line influential UAE israel aware peaceful relation jew israel surrounding arab state deemed near impossible prior x200b domestic lowest recorded unemployment minority group across age gender anti socialism anti communism economic political theory nation ruin reduction tax regulation business leading increased production employment',\n",
       " 'remember OP asking designed offset donates salary kennedy set aside gritting kept tradition pointing angry decent decent',\n",
       " 'absolutely bizarre post unemployment COVID unemployment steadily race looked graph x axis unemployment race impossible tell obama began discredit supporter economy inherited absolute mess economy steadily improved COVID',\n",
       " 'surprised given current racial accusation racist mentioned historic funding HBCUs google amp blackenterprise senate bill funding historically black university hbcus amp bipartisan bill known FUTURE fostering undergraduate talent unlocking resource education provide HBCUs minimum annually',\n",
       " 'happens cycle control critical ignore alone relation korea improved albeit high low evidence change feeling kid becoming socially aware become young adult ten parrot right obama basically defcon korea nuclear threat obama horrible deserves criticism countless innocent killed drone presidency span seriously thinking west coast nuked united state shaking hand restore diplomacy nation',\n",
       " 'address broad foreign policy initially portrayed thoughtless buffoon kickstart WWIII twitter slap fight closer WWIII answer gotten missile exchange korea opposite facilitated diplomatic leader peninsula moon jae recommended nobel peace merely offhand comment quite compliment peace cannot orchestrated US alone certainly POTUS right direction diplomatic deteriorated primarily due kim stubbornness internal hostility juche hardliner else revisit seem moving war peninsula generally disengaged expending ton blood treasure costly endless war starting bring troop home avoided starting prolonged conflict e syria assume anti war type appreciate heard note peace struck israel UAE ago UAE biggest dog neighborhood move likely draw country peace table contrary notion lapdog putin seriously sanction placed russia office moved troop eastern europe ought dissuade funny business moscow war russia mindlessly NATO militarytimes military thousand troop coming home germany right tempted headed war china unfortunately aggressor china gotten skirmish indian troop conducted drill invading taiwan perhaps perplexingly grumbling china historic claim russian port city vladivostok addition history aggression south china sea crackdown pro democracy demonstrator hong kong recommend checking china uncensored info bottom line china US sideways twitter slap fight decided hit nuke button feeling hurt',\n",
       " 'lofty proposition HIV AID epidemic pumped ton funding HIV AIDS research impact defunding sort important extra money coming source parent work public health HIV AIDS research',\n",
       " 'measure legacy rather harm inflicted war demonstrated genuine willingness pull troop afghanistan antagonized national intelligence apparatus military industrial complex JFK pause launch coup regime change operation adversarial nation state escalate tension russia syria admirable stand FISA court perhaps unconstitutional government institution existence virtue incompetent damage adhered DC foreign policy consensus case killing soleimani escalation drone war deepening tie israel policy evidence dove totality administration action FP front safely safer',\n",
       " 'economic gift country liberal stepped coronavirus political game pandemic thanks liberal bitching til flame',\n",
       " 'website unfortunately covid outbreak dont number factual however accomplishment ive physically witnessed past tax tax relief blue collar class average four parent tax return instead owing mother longer walked refund average work',\n",
       " 'showed truly deplorable racist conservative republican count idea truly terrible selfish un american relative emboldened admit awfulness treat accordingly worth collapse impact garbage',\n",
       " 'together dozen US hostage freed including obama freed signed music modernization biggest change copyright administration promoting second chance hiring former inmate opportunity live crime free meaningful employment DOJ board prison launched ready work initiative connect employer directly former prisoner historic tax cut legislation included opportunity zone incentive promote investment low income community across country opportunity zone expected spur billion private capital investment economically distressed community across country directed education secretary common core signed victim compensation fund signed measure funding prevention program veteran suicide TRILLION dollar overseas TCJA bill signed manufacturing job growing fastest pledge worker resulted employer committing train american republican tax bill small business lowest top marginal tax record number regulation eliminated hurt small business signed welfare reform requiring bodied adult work work welfare FDA approved affordable generic drug history reformed medicare program hospital overcharging low income senior drug saving senior alone signed right legislation terminally ill patient experimental treatment secured billion funding fight opioid epidemic signed VA choice VA accountability expanded VA telehealth service walk clinic urgent primary mental health production recently reached high dependent net natural gas exporter NATO ally increased defense spending pressure campaign circuit court judge nominee confirmed faster administration moved embassy israel jerusalem imposed tariff china response china forced technology transfer intellectual property theft chronically abusive trade practice agreed trade china signed legislation improve national suicide hotline signed comprehensive childhood cancer legislation advance childhood cancer research improve treatment tax cut job signed doubled maximum amount tax credit available parent lifted income limit claim signed billion funding increase development fund providing total billion state fund low income dependent tax credit CDCTC signed provides tax credit equal expense per per flexible spending account FSAs allow set aside pre tax use signed autism collaboration accountability research education CARES allocates billion funding next five autism spectrum disorder signed funding package providing nearly funding lupus specific research education program additional billion funding national institute health NIH lupus funding another upcoming accomplishment add next week signing anti robocall called TRACED telephone robocall abuse criminal enforcement deterrence TRACED extend period FCC catch punish intentionally break telemarketing restriction bill requires voice service provider develop framework verify call legitimate reach phone',\n",
       " 'yeah cult absolutely rabid defend die',\n",
       " 'hidden hatred subtle nazi uniform',\n",
       " 'written history book',\n",
       " 'ppl whining five another',\n",
       " 'shitty move',\n",
       " 'raised awareness racism white supremacy police brutality flaw american democracy using power executive branch steamroll branch expose GOP cult retaining power expose significant population authoritarianism right USA US showing change country',\n",
       " 'broken clock right twice incompetence lead positivity check balance system depends deem conservative negative tenure spent putting conservative judge court system conservative',\n",
       " 'together recently signed bill native compensation spokane tribe loss land mid 1900s fund native language program third federal recognition little shell tribe chippewa indian montana signed cruelty animal federal felony animal abuser face tougher consequence signed bill CBD hemp legal EPA gave fix water infrastructure problem flint michigan leadership surpassed russia saudi arabia become largest producer crude signed ending gag pharmacist prevented sharing money saving information signed allow state victim fight online sex trafficking FOSTA includes enabling sex trafficker SESTA enforcement victim tool fight sex trafficking signed bill require airport provide space breastfeeding mom lowest paid american enjoyed income boost november outpaces gain earnings country highest paid worker signed biggest wilderness protection conservation bill designated acre protected land signed save sea fund per clean ton plastic garbage ocean signed bill drug import canada prescription signed executive force healthcare provider disclose cost service american comparison shop provider charge insurance hospital required post standard charge service include discounted hospital willing accept eight prior inauguration prescription drug increased average per drug decline nine ten month drop recent month white house VA hotline veteran principally staffed veteran direct member veteran VA employee held accountable poor performance VA employee removed demoted suspended issued executive requiring secretary defense homeland security veteran affair submit joint provide veteran access access mental health treatment transition civilian bill signed championed federal employee pay increase average largest raise signed week paid parental leave federal worker administration provide HIV prevention drug free uninsured patient per signed small business group together buying insurance signed groundbreaking criminal justice bill enacted reform justice system fairer former inmate successfully return society reform addressed inequity sentencing disproportionately harmed black american reformed mandatory minimum unfair outcome expanded judicial discretion sentencing non violent crime benefiting retroactive sentencing reduction black american provides rehabilitative program inmate helping successfully rejoin society return crime increased funding historically black university HBCUs signed legislation forgiving hurricane katrina debt threatened HBCUs home sale october compared ago HBCUs priority creating position executive director white house initiative HBCUs received bipartisan justice award historically black criminal justice reform accomplishment poverty african american hispanic american reached lowest began collecting data signed bill creates five national monument expands several national park add acre wilderness permanently reauthorizes land water conservation fund USDA committed rebuild rural water infrastructure appointed openly gay ambassador ordered ric grenell openly gay ambassador germany lead initiative decriminalize homosexuality across globe anti trafficking coordination team ACTeam initiative federal enforcement doubled conviction human trafficker increased number defendant charged ACTeam district department justice DOJ dismantled organization internet leading source prostitution related advertisement resulting sex trafficking immigration custom enforcement homeland security investigation arrested criminal associated human trafficking department health human service provided funding national human trafficking hotline identify perpetrator victim DOJ provided grant organization human trafficking victim serving nearly case july june called congress pas school choice legislation trapped failing school zip code signed funding legislation september increased funding school choice tax cut signed promote school choice use saving elementary secondary education ISIS leader abu bakr al baghdadi killed signed perkins CTE reauthorization authorizing billion state fund vocational career education program executive expanding apprenticeship opportunity student worker issued executive prohibiting government discriminating christian punishing expression faith signed executive allows government withhold money campus deemed anti semitic fail combat anti semitism ordered halt tax money international organization fund perform abortion imposed sanction socialist venezuela killed citizen finalized trade agreement south korea secured billion trade investment china billion vietnam okay billion aid farmer affected unfair trade retaliation',\n",
       " 'merry band mobster showing crack democracy fully descended oligarchy republican showing actual traitor USA fixed majority broken place country together fight blatant fascism right',\n",
       " 'loses november admittedly idea elect failed business governing experience watch fox commiserates tucker carlson policy advice',\n",
       " 'republican scumbag racist liar rape christian accused christian anti jesus exaggerating easy show government employee willing ignore duty republican government essentially criminal welfare cop criminal easy proved republican enemy country republican lose power impact hold encourages republican voter die covid right winger pestilence become stupid filthy hateful trash decent human republican republican show top bottom concept integrity shameless liar consider corruption ideal hire republican idiot trust republican idiot expects republican oath idiot',\n",
       " 'valuable lesson',\n",
       " 'agreed agent chaos destruction putin china saudi arabia pledged elected enemy US ally knowingly pandemic spread everywhere US',\n",
       " 'attempt change current figure puppet add handedly impact change millionaire lobbying congress listen wrong dude chance tell face hesitate bet chance meet federal lawmaker tell EXACT snake grass government',\n",
       " 'reduced insulin',\n",
       " 'confirmed plain inadequate protection tyranny US populous believed exist',\n",
       " 'yes teamed tech giant hollywood radical wealthy easy odds stacked equipped little intelligence republican national convention regular citizen regular citizen explained impact truer',\n",
       " 'argue highlighted limit presidential power conversation electoral',\n",
       " 'economy coronavirus surprisingly strong account lockdown simultaneously blame coronavirus current economic crisis economic crisis lockdown solution coronavirus serious effort twarted congress bring troop iraq afganistan defeat ISIS quickly effectively constant terrorism US western europe france stoped ISIS defeated became biggest producer gaining energy independence stuff partisan cultural stuff delivered base classify objectively massively slowed cultural demographic change country reinvigorated american cultural nationalism across class line white working class ex obama voter american identitarianism reduced immigration massively legal illegal travel ban majority muslim country banned chinese infiltrator place replaced old fencing improved original fencing joke compared taller panel',\n",
       " 'EDIT barf undecided somehow information adult american strong informed fishy swayed unless easily swayed',\n",
       " 'stain history laughing stock',\n",
       " 'appointed excellent SCOTUS nominee filled record number court vacancy passed massive tax cut passed executive limiting regulation overthrow net neutrality general right police tried pas forced shitty payer health gun control legislation reparation payment required ect',\n",
       " 'chicken nugget golden tasty full cheap shitty ingridients puke sh video chicken nugget',\n",
       " 'tremendous impact successful tearing foundation upon democracy founded yay',\n",
       " 'kept dems awful change exactly opposition',\n",
       " 'china trade recognition jerusalem capitol israeli peace UAE stopping NAFTA',\n",
       " 'accomplished goal built improving pull troop afghanistan source google amp amp cnn cnn politics troop afghanistan index html negotiating peace source google amp washingtonpost politics announces historic peace agreement israel united arab emirate 363f3c54 dd76 11ea d5f887d73381_story html 3foutputtype amp sanction china source google amp nytimes asia china hong kong sanction amp html standing human trafficking strongest economy american history source google amp amp cnn cnn economy economy recession index html historic low african american unemployment source google amp cnbc amp black hispanic unemployment record low html condem rioting looting beginning achievement black hispanic approval beginning sky rocket source google amp thehill hilltv thinking poll approval rise among black hispanic voter 3famp recent saw downplayed wanted downplay create panic scavenged toilet paper roll sight CALM version imagine yup gonna die nut closed travel china country early backlash claim racist chinese democrat job state local government source1 google amp nytimes business china travel coronavirus amp html source2 google amp nbcbayarea local nancy pelosi visit san franciscos chinatown 3famp incredibly aggressive human trafficking google amp abcnews amp politics anti human trafficking group boycotting ivanka white story 3fid main reason build keeping mexican fleeing border human trafficking drug trade kid kidnapped border taken across raped attempting alone greatest achievement begin capturing ghilaine maxwell informant inside epstein human trafficking source coincidence maxwell arrested protected month later found truck kidnapped asshole hothead racist nowhere near',\n",
       " 'arent gonna legit response mostly full ignorant leftist kid ability pro right pro statement immediately removed braindead fascist mod',\n",
       " 'absolutist stand absolutism attractive arguing internet lead',\n",
       " 'rascist nasty body shaming git trash cheated vote',\n",
       " 'asktrumpsupporters biased info',\n",
       " 'banned bump stock although pardon opposition question btw learning',\n",
       " 'manage ban tik tok',\n",
       " 'covfefe voting supposedly',\n",
       " 'disagree handedly spotlighted voter apathy hurt vote motivated younger generation politics',\n",
       " 'kept fearful answer pacified becoming shitty stick sand ignore small kept adding occasionally pop folk pissy change voicing sand functioning born occasionally given cookie appease pacified cookie served distraction happening moreso nearly constant threat cookie taken distraction today moderate centralist progressive vote nonsense honestly scary fear called normal everyday normal working place rich richer poor poorer keeping scary maddening today monster cannot ignored establishment empowers working apparent ordinary citizen fellow american engaged unable sleep participating became active current affair putting sand progressive movement die elected number ideal mainstream light fascist conservative fear god road boiling frog sand mindset caused harm nonsense problem madman fellow american wake sleep work',\n",
       " 'warmonger obama bush',\n",
       " 'challenge define change impact post using standard applied',\n",
       " 'prez regs deemed negative depending stand reduction regs tends stimulate economy needed economically depressed area regardless stand negative regs multiple admins enact prez signature depending stand decreased trade deficit supply chain reliance china bringing manufacturing job process viewed prez brokered israel UAE nominated noble peace',\n",
       " 'record short administration achievement coming economy economic growth averaged administration accounting unemployment recently hit lowest mark administration policy foremost signature tax cut economy add seven job half manufacturing alone notably job economically inept predecessor barack obama insisted barometer consumer business confidence stock market record provide greater net worth broad swath american equity holder either directly indirectly close p NASDAQ composite heavily loaded dow jones industrial average prospect forward wage american worker rising fastest rise low income worker addition poverty african american hispanic american reached low street journal note wage rank file worker rising quickest pace faster boss average hourly earnings production nonsupervisory worker private sector november earlier seeing hourly earnings stumble along barack obama looking income earner consumer spending surged reflection consumer confidence reflection administration policy cancerous recession seeded democrat policy clearly recession trade nation worker implementing trade protect job house approved united state mexico canada agreement trade pact estimated provide significant worker farmer manufacturer hardline trade stance china politically risky among key constituency balancing trade relationship overdue needed protect american job economy national security brash brawn china according hill dawn nation manufacturing sector experiencing aggressive growth steel industry particular offer ample evidence steel investing billion steelmaking mill across nation economist predicted however frustrated flow economics profession looking wage war tariff regulatory state rolled impressive government regulation implemented saving american taxpayer billion spurring additional economic growth according washington federal register dec published final page lowest number record began thus broke previous record low administration cut oppressive environmental regulation air breathable water drinkable court senate confirmed nominated judge meaning federal bench occupied nominee includes outstanding supreme court justice neil gorsuch brett kavanaugh reelected conservative revitalization federal court continue including possibly SCOTUS appointee gratitude former democrat senate majority leader harry reid nuked senate filibuster judge thereby paved republican appointee healthcare administration continues implement policy advance patient right slow healthcare cost increase roll obama called affordable disastrous impact medical cost success include largest decline drug recorded energy administration energy policy united state net exporter product including refined petroleum crude resulted reduced dependence eastern weakening russia export revenue domestic security nation safer reached agreement mexico central american country stem flood illegal immigrant across southern border border apprehension fallen administration replaced catch release catch return sending phony asylum seeker mexico national security enormous national security challenge unlike predecessor appeasing tyrant administration authorized military squash islamic state unlike prior administration policy jihadi terror flourish additionally NATO ally increased defense spending billion withdrew UN arm trade treaty terrible nuke iran clearly imposed terrorist including significant state sponsor terrorism iran perhaps democrat despite impeachment charade popularity growing choice among voter democrat contender hope january affirm oath defend according article II section clause constitution united state solemnly swear faithfully execute office united state ability preserve protect defend constitution united state unlike congressional democrat abided oath liberty american serf semper vigilans fortis paratus et fidelis pro deo et libertate',\n",
       " 'deleted',\n",
       " 'answer simple question legacy treated unfairly history mention completely',\n",
       " 'talking obviously history corrupt',\n",
       " 'easy opened others eye racist corrupt country built brutality right',\n",
       " 'uhhh space force idea space force kinda cool',\n",
       " 'promiseskept politifact meter promise trumpometer ruling promise kept',\n",
       " 'increased voter turnout',\n",
       " 'easier tell low key racist',\n",
       " 'country elect lose faith democracy',\n",
       " 'waste trash removed qe vote blue house blue elected impeached second imagine impeachment cliud twice donnie asshole arent serving constitution termw wont thwt arent dictator usa fucked',\n",
       " 'transient impact',\n",
       " 'set record game golf',\n",
       " 'calling liberal',\n",
       " '',\n",
       " 'fully moral bankruptcy republican party worth',\n",
       " 'showed face heart changed',\n",
       " 'accelerated decline depending achievement',\n",
       " 'leaf impact US murderer fertilizes forest burying body',\n",
       " 'handful prison reform unsuccessfully medicare drug tied germany couple influence cliche hitler administration non jewish german stalin russia employment suffer economic crash administration achievement weighs decent prison reform bill administration COVID response labor right mexico tried ban muslim literally interview muslim registry racist sexiest fascistic rhetoric economic improvement road economic progress obama administration administration suing remove preexisting condition coverage affordable banned trans military asked supreme court firing gay legal paris climate accord conscious effort fight science climate change police break peaceful protest lafayette square ordered unmarked DHS officer kidnap random protestors fascist fringe mexican labor right mexican lying fascist stupid',\n",
       " 'easier tell goddamn idiot red hat',\n",
       " 'impact meteor destructuve',\n",
       " 'fart sometimes impact doubt shite sort impact',\n",
       " 'gotten banned leaving sinking ship pressured succeed die damn wanna leave',\n",
       " 'american dumb racist piece',\n",
       " 'prattle accomplishment question substance sign thousand bill pointing inconsequential bill context leading powerful nation earth interesting question test instrumental achieving either space force happen employment basically trajectory obama pandemic material context overall federal budget structure organization government society writ tax reform multi trillion dollar decision increasing school choice literally rounding error department education budget depends potentially pas three loathsome engaging frank direct via twitter throwing old dogma aside motif suspect rightly shifted strategic focus countering china incredibly repressive authoritarian state changed public business question business method right stolen IP posing challenge western tax reform corporate earnings boosted share bit rich democrat rich deduction smaller prevalent killing suleimani obama foreign policy particularly assertive scared shadow foreign policy expert obama foreign policy success course iran responded highly measured US escalation dominance v iran suicidal aggressive US posture likely',\n",
       " 'hitler autobhan super employment number due highway idea pioneer animal right piece piece',\n",
       " 'ignorant selfish conservative credit',\n",
       " 'item ie falling crime falling job creation obama black employment thread propaganda sheet',\n",
       " 'stack SCOTUS debt continues exponential growth fiscal responsibility republican supposed',\n",
       " 'woke asleep reagan goal outcome presidency potential pendulum swing reverse rightward drift',\n",
       " 'space force netflix',\n",
       " 'squashed ISIS within month extra week paycheck via tax reform spurred US populace political action personal amount political interest activism necessary welcome functional democracy',\n",
       " 'hospital pricing transparent cut tax tax revenue despite tax cut',\n",
       " 'clearly karma farming brain hatred towards human race hatred towards implimented sytestems control built fell laughable amount wind area impact anywhere korea weak dumb vulnerable stupid investment yanke flocking EU hit eu russia china umbrella waiting thankfully american happen stupidity',\n",
       " 'comment old yeah russian',\n",
       " 'defeated isi half alone blow jesus christ thats book',\n",
       " 'contribution lowered tax clobbered ISIS deregulated rebuilt military CLINTON OBAMA destroyed MUSLIMS immigrating central american peon crashing country armed ukraine JAVELIN fight putin killing russian syria job available unemployment BLACKS employed obama betrayed rebuilding military NATO partner pay mueller COLLUSION economy rockin rollin libs sex melania shoe undoing damage obama wrought army beat navy',\n",
       " 'accomplishment thme category economic economic opportunity zone trade greatly increased energy production provided billion aid farmer affected unfair trade retaliation trillion dollar overseas TCJA bill trade mexico canada imposed tariff china response china intellectual property theft abusive trade practice criminal social justice reform criminal justice system former inmate return successfully society signed three important bill native signed fight online sex trafficking signed tax cut job doubling maximum amount tax credit available parent e nvironmental signed biggest wilderness protection bill designated acre protected land signed save sea funding per clean ton garbage plastic ocean signed bill expands several national park adding acre wilderness medical gag pharmacist prevented sharing money saving info customer signed bill prescription drug import canada reduce prescription provided HIV orevention drug free uninsured patient administration FDA approved affordable generic drug accomplishment spite continuous unrelenting opposition attack political opponent',\n",
       " 'impact presidency molded federal judiciary image generation exploded debt deficit repercussion irritate cynical died covid watch reversed climate change regulation period stave drastic climate crisis',\n",
       " 'firstly secondly legalized hemp nation wide',\n",
       " 'white supremacy perspective',\n",
       " 'proved check balance limitation',\n",
       " 'wrong ensured voting republican',\n",
       " 'fraud league wizard oz supporter basically dorothy toto foolish naivete arrives forced confront today',\n",
       " 'thank god stuck obama obama phone',\n",
       " 'yes opened white eye racial divide country racism em systemic racism',\n",
       " 'staff handed easily executable pandemic outgoing obama administration quite literally dumped useless cut funding department handle epidemic pandemic three later feckless loser currently squatting white house useless failure draft dodging loser traitor country defends complicit action therefore traitor TRAITORS',\n",
       " 'cost golfing trip outweighs donating salary',\n",
       " 'ridiculous kinda wonder WEEK nominated norwegian nobel peace moon south korea noble peace publicly attacked military complex live v pressure china largerst contribution kill disagree china stubborn main stream dictate action lack empathy latin decent reverse role looked pandemic caused china changed horrible x police brutality foreigner ive told exist hand account either privileged live granted',\n",
       " 'lover odds everything favor considered right wing propaganda deny presidency sparked change whether optimist belief presidency necessary evil fight country freedom eroded unprecedented pace sex trafficking load criminal activity washington fully congress politician maintaining power blatantly corrupt forcing seriously government grilled corruption McConnell republican disregard duty maintain power nancy dems factual sentiment change hope happens office foot gas continue zero tolerance corruption washington',\n",
       " 'judge presidency based effect generally approve US screwing honestly presidency decent adult tax saw advancement field graduating early industry pandemic hit fault story short pay tax personal complaint service obama screwed entire health insurance situation switched ACA cost significantly obamacare idea terrible practice rushed screwed already health insurance bush blame housing market crash recession dent fund use pay school job market awful graduated fault bush jumping war war fought wasted tax dollar american',\n",
       " 'counting dead american',\n",
       " 'easier ignorant racist',\n",
       " 'comedy admittedly short politics meme power top brexit handled entrance boris johnson prime minter gotta alive SURELY disagree']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "024cc1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of DT matrix are (736, 19506)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/irenevillalonga/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Without tf-idf \n",
    "cv = CountVectorizer(ngram_range = (1,2))\n",
    "cv.fit(Texts)\n",
    "vectorized_text=cv.transform(Texts)\n",
    "names=np.array(cv.get_feature_names())\n",
    "print(\"dimensions of DT matrix are\", vectorized_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "af1683d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19496</th>\n",
       "      <th>19497</th>\n",
       "      <th>19498</th>\n",
       "      <th>19499</th>\n",
       "      <th>19500</th>\n",
       "      <th>19501</th>\n",
       "      <th>19502</th>\n",
       "      <th>19503</th>\n",
       "      <th>19504</th>\n",
       "      <th>19505</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 19506 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3      4      5      6      7      8      9      ...  \\\n",
       "0      0      0      0      0      0      0      0      0      0      0  ...   \n",
       "1      0      0      0      0      0      0      0      0      0      0  ...   \n",
       "2      0      0      0      0      0      0      0      0      0      0  ...   \n",
       "3      0      0      0      0      0      0      0      0      0      0  ...   \n",
       "4      0      0      0      0      0      0      0      0      0      0  ...   \n",
       "\n",
       "   19496  19497  19498  19499  19500  19501  19502  19503  19504  19505  \n",
       "0      0      0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 19506 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm = pd.DataFrame.sparse.from_spmatrix(vectorized_text)\n",
    "dtm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a693269a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Heat matrix from Document Term Matrix\n",
    "sns.heatmap(dtm.iloc[:100, :100], cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df219b97",
   "metadata": {},
   "source": [
    "### 3) POST PROCESSING\n",
    "In this section we are going to use both the term frequency and the inverse document frequency to reweight the terms in the document term matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ab4bb1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try if tf-idf \n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# cv = TfidfVectorizer(ngram_range = (1,2), norm=None)\n",
    "# cv.fit(Texts)\n",
    "# vectorized_text=cv.transform(Texts)\n",
    "# vectorized_text=vectorized_text.todense()\n",
    "# print(\"document term matrix has size\", vectorized_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513f5eb0",
   "metadata": {},
   "source": [
    "### 4A) LDA IMPLEMENTATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "38446022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "736"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2e195a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/irenevillalonga/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "corp = [nltk.word_tokenize(text) for text in Texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8276528b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5133"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "1f965653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce bigrams\n",
    "bigram = gensim.models.Phrases(corp, min_count=1, threshold=1)\n",
    "trigram = gensim.models.Phrases(bigram[corp], threshold=1)  \n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "corp = [bigram_mod[doc] for doc in corp] # TEXT_BIGRAMS ES NEW CORP CON BIGRAMS\n",
    "texts_trigrams = [trigram_mod[doc] for doc in corp] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f7f883ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "3d258d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = [dictionary.doc2bow(line) for line in corp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3d0cca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "\n",
    "def viz_model(model, modeldict):\n",
    "    ntopics = model.num_topics\n",
    "    # top words associated with the resulting topics\n",
    "    topics = ['Topic {}: {}'.format(t,modeldict[w]) \n",
    "              for t in range(ntopics) \n",
    "              for w,p in model.get_topic_terms(t, topn=1)]\n",
    "              \n",
    "    terms = [modeldict[w] for w in modeldict.keys()]\n",
    "    fig,ax=plt.subplots()\n",
    "    ax.imshow(model.get_topics())  # plot the numpy matrix\n",
    "    ax.set_xticks(modeldict.keys())  # set up the x-axis\n",
    "    ax.set_xticklabels(terms, rotation=90)\n",
    "    ax.set_yticks(np.arange(ntopics))  # set up the y-axis\n",
    "    ax.set_yticklabels(topics)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c691480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eta(eta, dictionary, ntopics, print_topics=True, print_dist=True):\n",
    "    np.random.seed(42) # set the random seed for repeatability\n",
    "    bow = [dictionary.doc2bow(line) for line in corp] # get the bow-format lines with the set dictionary\n",
    "    with (np.errstate(divide='ignore')):  # ignore divide-by-zero warnings\n",
    "        model = gensim.models.ldamodel.LdaModel(\n",
    "            corpus=bow, id2word=dictionary, num_topics=ntopics,\n",
    "            random_state=42, chunksize=100, eta=eta,\n",
    "            eval_every=-1, update_every=1,\n",
    "            passes=150, alpha=0.8, per_word_topics=True)\n",
    "    # visuzlize the model term topics\n",
    "    #viz_model(model, dictionary)\n",
    "    print('Perplexity: {:.2f}'.format(model.log_perplexity(bow)))\n",
    "    if print_topics:\n",
    "        # display the top terms for each topic\n",
    "        for topic in range(ntopics):\n",
    "            print('Topic {}: {}'.format(topic, [dictionary[w] for w,p in model.get_topic_terms(topic, topn=8)]))\n",
    "    if print_dist:\n",
    "        # display the topic probabilities for each document\n",
    "        doc_topics_list = []\n",
    "        for idx, (line,bag) in enumerate(zip(Texts,bow)):\n",
    "            doc_topics = model.get_document_topics(bag)\n",
    "            doc_topics_list.append({'document': idx, 'probabilities': dict(doc_topics)})\n",
    "            print('{}: {}'.format(idx, doc_topics))\n",
    "    return model, doc_topics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a50a4cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -5.77\n",
      "Topic 0: ['administration', 'bill', 'policy', 'federal', 'action', 'state', 'killing', 'democrat']\n",
      "Topic 1: ['right', 'accomplishment', 'peace', 'trade', 'anti', 'likely', 'due', 'cut']\n",
      "Topic 2: ['impact', 'war', 'politics', 'pandemic', 'gotten', 'sometimes', 'USA', 'elected']\n",
      "Topic 3: ['country', 'china', 'US', 'change', 'economic', 'fight', 'basically', 'leader']\n",
      "Topic 4: ['obama', 'american', 'economy', 'military', 'korea', 'changed', 'iran', 'ISIS']\n",
      "Topic 5: ['republican', 'vote', 'criminal', 'cost', 'tell', 'wrong', 'area', 'disagree']\n",
      "Topic 6: ['government', 'power', 'work', 'signed', 'corruption', 'idea', 'politician', 'funding']\n",
      "Topic 7: ['presidency', 'office', 'credit', 'history', 'source', 'congress', 'low', 'everything']\n",
      "Topic 8: ['job', 'tax', 'business', 'pay', 'drug', 'employment', 'unemployment', 'amount']\n",
      "Topic 9: ['racist', 'political', 'system', 'conservative', 'nation', 'problem', 'party', 'covid']\n",
      "0: [(0, 0.08000048), (1, 0.08000049), (2, 0.080000564), (3, 0.080000475), (4, 0.08000048), (5, 0.08000052), (6, 0.08000049), (7, 0.0800005), (8, 0.2799955), (9, 0.080000505)]\n",
      "1: [(0, 0.057143595), (1, 0.12857138), (2, 0.12857297), (3, 0.19999672), (4, 0.057143595), (5, 0.05714365), (6, 0.0571436), (7, 0.057143614), (8, 0.19999726), (9, 0.05714363)]\n",
      "2: [(0, 0.053333893), (1, 0.0533339), (2, 0.053333987), (3, 0.12000056), (4, 0.053333897), (5, 0.053333938), (6, 0.0533339), (7, 0.25333062), (8, 0.25333136), (9, 0.053333923)]\n",
      "3: [(0, 0.08888917), (1, 0.08888917), (2, 0.19999745), (3, 0.08888916), (4, 0.08888917), (5, 0.08888919), (6, 0.08888917), (7, 0.088889174), (8, 0.088889174), (9, 0.08888918)]\n",
      "4: [(0, 0.07499685), (1, 0.20000008), (2, 0.0749997), (3, 0.15833752), (4, 0.03333436), (5, 0.1583345), (6, 0.033334367), (7, 0.1583316), (8, 0.07499665), (9, 0.033334404)]\n",
      "5: [(0, 0.114284866), (1, 0.019048514), (2, 0.019048648), (3, 0.06666253), (4, 0.066665046), (5, 0.04285642), (6, 0.18571873), (7, 0.3761922), (8, 0.09047446), (9, 0.019048546)]\n",
      "6: [(0, 0.12857011), (1, 0.12856984), (2, 0.057144865), (3, 0.12856841), (4, 0.12857138), (5, 0.057144724), (6, 0.1285738), (7, 0.12856755), (8, 0.05714464), (9, 0.057144668)]\n",
      "7: [(0, 0.08888903), (1, 0.08888904), (2, 0.088889055), (3, 0.08888903), (4, 0.08888903), (5, 0.08888904), (6, 0.08888904), (7, 0.08888904), (8, 0.08888904), (9, 0.1999987)]\n",
      "8: [(0, 0.08888917), (1, 0.08888917), (2, 0.19999745), (3, 0.08888916), (4, 0.08888917), (5, 0.08888919), (6, 0.08888917), (7, 0.088889174), (8, 0.088889174), (9, 0.08888918)]\n",
      "9: [(0, 0.040000964), (1, 0.08999582), (2, 0.090002194), (3, 0.18999758), (4, 0.040000964), (5, 0.13999864), (6, 0.040000968), (7, 0.09000125), (8, 0.09000118), (9, 0.19000039)]\n",
      "10: [(0, 0.08000018), (1, 0.08000018), (2, 0.27999842), (3, 0.08000017), (4, 0.08000018), (5, 0.080000184), (6, 0.08000018), (7, 0.08000018), (8, 0.08000018), (9, 0.08000018)]\n",
      "11: [(0, 0.029630495), (1, 0.0666648), (2, 0.029630635), (3, 0.3599722), (4, 0.10370344), (5, 0.06666609), (6, 0.029630503), (7, 0.1407405), (8, 0.06966484), (9, 0.103696495)]\n",
      "12: [(0, 0.15262662), (1, 0.07368315), (2, 0.12632348), (3, 0.100002974), (4, 0.12631446), (5, 0.07368311), (6, 0.047367483), (7, 0.04736975), (8, 0.07368074), (9, 0.17894827)]\n",
      "13: [(0, 0.08000046), (1, 0.08000047), (2, 0.08000054), (3, 0.08000045), (4, 0.08000046), (5, 0.0800005), (6, 0.17999855), (7, 0.080000475), (8, 0.17999762), (9, 0.08000049)]\n",
      "14: [(0, 0.105879754), (1, 0.16470788), (2, 0.04705976), (3, 0.16470593), (4, 0.047059633), (5, 0.047059692), (6, 0.1647041), (7, 0.105883256), (8, 0.04705965), (9, 0.1058803)]\n",
      "15: [(0, 0.14146034), (1, 0.04389886), (2, 0.043901335), (3, 0.06829458), (4, 0.14146625), (5, 0.04390033), (6, 0.0926837), (7, 0.068290465), (8, 0.19024867), (9, 0.16585544)]\n",
      "16: [(0, 0.14999734), (1, 0.23333101), (2, 0.06666749), (3, 0.066667356), (4, 0.06666738), (5, 0.06666743), (6, 0.066667385), (7, 0.06666739), (8, 0.0666674), (9, 0.1499998)]\n",
      "17: [(0, 0.03200091), (1, 0.0720011), (2, 0.032001056), (3, 0.071994245), (4, 0.03200091), (5, 0.1519978), (6, 0.032000918), (7, 0.23200096), (8, 0.07199848), (9, 0.2720036)]\n",
      "18: [(0, 0.054540467), (1, 0.11515254), (2, 0.20607007), (3, 0.11514865), (4, 0.14545643), (5, 0.11515087), (6, 0.08484564), (7, 0.11514877), (8, 0.024243264), (9, 0.024243278)]\n",
      "19: [(0, 0.13999644), (1, 0.29000092), (2, 0.14000249), (3, 0.09000171), (4, 0.08999622), (5, 0.040001206), (6, 0.08999756), (7, 0.040001143), (8, 0.04000115), (9, 0.04000117)]\n",
      "20: [(0, 0.22308469), (1, 0.0692139), (2, 0.030770635), (3, 0.14615709), (4, 0.06922603), (5, 0.14615668), (6, 0.06923247), (7, 0.18461755), (8, 0.030770473), (9, 0.030770496)]\n",
      "21: [(0, 0.21250036), (1, 0.02500122), (2, 0.056252908), (3, 0.025001178), (4, 0.087499976), (5, 0.118747346), (6, 0.11875251), (7, 0.11875091), (8, 0.11874232), (9, 0.11875129)]\n",
      "22: [(0, 0.07272767), (1, 0.07272767), (2, 0.1636369), (3, 0.07272766), (4, 0.2545417), (5, 0.0727277), (6, 0.07272767), (7, 0.07272768), (8, 0.07272768), (9, 0.072727695)]\n",
      "23: [(0, 0.03809587), (1, 0.18094927), (2, 0.13333574), (3, 0.085714675), (4, 0.2761912), (5, 0.038095918), (6, 0.038095877), (7, 0.085713215), (8, 0.038095888), (9, 0.08571231)]\n",
      "24: [(0, 0.14146423), (1, 0.33658952), (2, 0.04390048), (3, 0.23902832), (4, 0.043901395), (5, 0.04389849), (6, 0.04390029), (7, 0.019512698), (8, 0.019512702), (9, 0.06829186)]\n",
      "25: [(0, 0.049995143), (1, 0.07776624), (2, 0.077780336), (3, 0.050001457), (4, 0.3833483), (5, 0.05000067), (6, 0.10555541), (7, 0.077773914), (8, 0.022223284), (9, 0.10555527)]\n",
      "26: [(0, 0.03478367), (1, 0.16521896), (2, 0.034783836), (3, 0.0782623), (4, 0.07825363), (5, 0.0782608), (6, 0.07826138), (7, 0.078254685), (8, 0.339137), (9, 0.034783717)]\n",
      "27: [(0, 0.1368429), (1, 0.031573378), (2, 0.01403581), (3, 0.3649243), (4, 0.03157532), (5, 0.1368439), (6, 0.08421046), (7, 0.0666647), (8, 0.066661574), (9, 0.06666762)]\n",
      "28: [(0, 0.08888895), (1, 0.08888895), (2, 0.1999995), (3, 0.08888895), (4, 0.08888895), (5, 0.08888895), (6, 0.08888895), (7, 0.08888895), (8, 0.08888895), (9, 0.08888895)]\n",
      "29: [(0, 0.11999689), (1, 0.11999568), (2, 0.120004), (3, 0.11999613), (4, 0.12000153), (5, 0.053334914), (6, 0.053334814), (7, 0.12000012), (8, 0.12000107), (9, 0.053334866)]\n",
      "30: [(0, 0.09600125), (1, 0.21600415), (2, 0.01600069), (3, 0.095992535), (4, 0.095998436), (5, 0.01600064), (6, 0.1159989), (7, 0.016000608), (8, 0.055995267), (9, 0.2760075)]\n",
      "31: [(0, 0.075671546), (1, 0.07567719), (2, 0.075676635), (3, 0.21081421), (4, 0.21081457), (5, 0.048643634), (6, 0.02162234), (7, 0.102703005), (8, 0.048648212), (9, 0.12972863)]\n",
      "32: [(0, 0.1285716), (1, 0.12856857), (2, 0.12857509), (3, 0.057144243), (4, 0.12857084), (5, 0.05714439), (6, 0.057144288), (7, 0.12856771), (8, 0.12856893), (9, 0.05714434)]\n",
      "33: [(0, 0.06666802), (1, 0.14999849), (2, 0.06666824), (3, 0.06666799), (4, 0.06666802), (5, 0.06666812), (6, 0.14998853), (7, 0.06666805), (8, 0.23333648), (9, 0.06666808)]\n",
      "34: [(0, 0.16363636), (1, 0.072728194), (2, 0.072728336), (3, 0.07272816), (4, 0.07272819), (5, 0.072728254), (6, 0.25453788), (7, 0.0727282), (8, 0.07272821), (9, 0.072728224)]\n",
      "35: [(0, 0.08888955), (1, 0.088889554), (2, 0.08888965), (3, 0.088889524), (4, 0.08888955), (5, 0.0888896), (6, 0.088889554), (7, 0.19999391), (8, 0.08888957), (9, 0.08888958)]\n",
      "36: [(0, 0.08888895), (1, 0.08888895), (2, 0.088888966), (3, 0.08888895), (4, 0.1999994), (5, 0.08888896), (6, 0.08888895), (7, 0.08888895), (8, 0.08888895), (9, 0.08888895)]\n",
      "37: [(0, 0.08888896), (1, 0.08888896), (2, 0.08888897), (3, 0.1999993), (4, 0.08888896), (5, 0.088888966), (6, 0.08888896), (7, 0.088888966), (8, 0.088888966), (9, 0.088888966)]\n",
      "38: [(0, 0.04210609), (1, 0.09473316), (2, 0.14736997), (3, 0.094737254), (4, 0.09473417), (5, 0.04210615), (6, 0.0421061), (7, 0.30526602), (8, 0.04210611), (9, 0.09473497)]\n",
      "39: [(0, 0.1263181), (1, 0.047368318), (2, 0.02105353), (3, 0.07367495), (4, 0.17895205), (5, 0.04736763), (6, 0.099999726), (7, 0.25790063), (8, 0.12631159), (9, 0.021053443)]\n",
      "40: [(0, 0.08888917), (1, 0.08888917), (2, 0.19999745), (3, 0.08888916), (4, 0.08888917), (5, 0.08888919), (6, 0.08888917), (7, 0.088889174), (8, 0.088889174), (9, 0.08888918)]\n",
      "41: [(0, 0.0844438), (1, 0.039999977), (2, 0.039997526), (3, 0.084442645), (4, 0.06222293), (5, 0.106664926), (6, 0.06222134), (7, 0.03999694), (8, 0.039999865), (9, 0.44001007)]\n",
      "42: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "43: [(0, 0.18095265), (1, 0.32381365), (2, 0.038096275), (3, 0.08571546), (4, 0.038096134), (5, 0.038096197), (6, 0.038096137), (7, 0.03809615), (8, 0.1809412), (9, 0.03809617)]\n",
      "44: [(0, 0.22564457), (1, 0.1487166), (2, 0.02051363), (3, 0.071796596), (4, 0.12307848), (5, 0.07179252), (6, 0.04615151), (7, 0.020513535), (8, 0.20000274), (9, 0.071789764)]\n",
      "45: [(0, 0.13333279), (1, 0.1333309), (2, 0.03809639), (3, 0.1333307), (4, 0.22857374), (5, 0.03809631), (6, 0.08571208), (7, 0.13333453), (8, 0.038096257), (9, 0.03809628)]\n",
      "46: [(0, 0.08888917), (1, 0.08888917), (2, 0.08888921), (3, 0.08888916), (4, 0.08888917), (5, 0.08888919), (6, 0.08888917), (7, 0.088889174), (8, 0.19999741), (9, 0.08888918)]\n",
      "47: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "48: [(0, 0.08000052), (1, 0.17999515), (2, 0.18000057), (3, 0.080000505), (4, 0.08000053), (5, 0.080000564), (6, 0.08000053), (7, 0.080000535), (8, 0.080000535), (9, 0.08000054)]\n",
      "49: [(0, 0.080000445), (1, 0.080000445), (2, 0.08000052), (3, 0.08000044), (4, 0.080000445), (5, 0.17999685), (6, 0.080000445), (7, 0.08000045), (8, 0.08000046), (9, 0.17999947)]\n",
      "50: [(0, 0.08888895), (1, 0.08888895), (2, 0.1999995), (3, 0.08888895), (4, 0.08888895), (5, 0.08888895), (6, 0.08888895), (7, 0.08888895), (8, 0.08888895), (9, 0.08888895)]\n",
      "51: [(0, 0.080000244), (1, 0.080000244), (2, 0.08000028), (3, 0.08000024), (4, 0.080000244), (5, 0.27999774), (6, 0.080000244), (7, 0.080000244), (8, 0.08000025), (9, 0.08000025)]\n",
      "52: [(0, 0.06666819), (1, 0.14999954), (2, 0.06666843), (3, 0.066668145), (4, 0.14999668), (5, 0.0666683), (6, 0.0666682), (7, 0.06666822), (8, 0.23332603), (9, 0.06666826)]\n",
      "53: [(0, 0.23333287), (1, 0.06666716), (2, 0.14999782), (3, 0.06666715), (4, 0.06666716), (5, 0.06666719), (6, 0.06666716), (7, 0.06666717), (8, 0.06666717), (9, 0.14999917)]\n",
      "54: [(0, 0.08888925), (1, 0.08888925), (2, 0.08888931), (3, 0.08888924), (4, 0.08888925), (5, 0.08888928), (6, 0.08888925), (7, 0.19999671), (8, 0.088889256), (9, 0.08888926)]\n",
      "55: [(0, 0.053334553), (1, 0.18666771), (2, 0.053334747), (3, 0.18666416), (4, 0.053334553), (5, 0.119996734), (6, 0.12000103), (7, 0.053334575), (8, 0.1199973), (9, 0.053334605)]\n",
      "56: [(0, 0.08000091), (1, 0.08000092), (2, 0.080001056), (3, 0.1558035), (4, 0.08000091), (5, 0.17999786), (6, 0.080000915), (7, 0.08000093), (8, 0.104192026), (9, 0.08000095)]\n",
      "57: [(0, 0.19999747), (1, 0.08888917), (2, 0.088889204), (3, 0.08888916), (4, 0.08888917), (5, 0.08888919), (6, 0.08888917), (7, 0.088889174), (8, 0.088889174), (9, 0.088889174)]\n",
      "58: [(0, 0.08888919), (1, 0.08888919), (2, 0.08888924), (3, 0.08888918), (4, 0.08888919), (5, 0.08888921), (6, 0.08888919), (7, 0.0888892), (8, 0.0888892), (9, 0.19999723)]\n",
      "59: [(0, 0.08000033), (1, 0.08000033), (2, 0.080000386), (3, 0.1799994), (4, 0.08000033), (5, 0.17999789), (6, 0.08000033), (7, 0.08000034), (8, 0.08000034), (9, 0.08000034)]\n",
      "60: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "61: [(0, 0.08000063), (1, 0.08000063), (2, 0.080000736), (3, 0.08000061), (4, 0.08000063), (5, 0.080000676), (6, 0.17999637), (7, 0.1799984), (8, 0.08000065), (9, 0.08000066)]\n",
      "62: [(0, 0.17999722), (1, 0.17999779), (2, 0.0800007), (3, 0.08000059), (4, 0.0800006), (5, 0.08000065), (6, 0.0800006), (7, 0.08000061), (8, 0.08000062), (9, 0.08000063)]\n",
      "63: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "64: [(0, 0.27999735), (1, 0.08000028), (2, 0.080000326), (3, 0.080000274), (4, 0.08000028), (5, 0.0800003), (6, 0.08000028), (7, 0.08000029), (8, 0.08000029), (9, 0.080000296)]\n",
      "65: [(0, 0.080000825), (1, 0.080000825), (2, 0.08000095), (3, 0.080000795), (4, 0.080000825), (5, 0.1799938), (6, 0.080000825), (7, 0.17999947), (8, 0.08000085), (9, 0.080000855)]\n",
      "66: [(0, 0.06666762), (1, 0.23333082), (2, 0.06666777), (3, 0.066667594), (4, 0.06666762), (5, 0.06666769), (6, 0.14999756), (7, 0.14999801), (8, 0.066667646), (9, 0.06666766)]\n",
      "67: [(0, 0.088889204), (1, 0.088889204), (2, 0.088889256), (3, 0.0888892), (4, 0.088889204), (5, 0.08888923), (6, 0.088889204), (7, 0.08888921), (8, 0.08888921), (9, 0.19999708)]\n",
      "68: [(0, 0.17999734), (1, 0.08000028), (2, 0.18000035), (3, 0.080000274), (4, 0.08000028), (5, 0.0800003), (6, 0.08000028), (7, 0.08000029), (8, 0.08000029), (9, 0.080000296)]\n",
      "69: [(0, 0.08000031), (1, 0.08000032), (2, 0.17999758), (3, 0.17999983), (4, 0.08000031), (5, 0.08000033), (6, 0.08000032), (7, 0.08000032), (8, 0.08000032), (9, 0.080000326)]\n",
      "70: [(0, 0.08888895), (1, 0.08888895), (2, 0.088888966), (3, 0.08888895), (4, 0.1999994), (5, 0.08888896), (6, 0.08888895), (7, 0.08888895), (8, 0.08888895), (9, 0.08888895)]\n",
      "71: [(0, 0.0727279), (1, 0.0727279), (2, 0.07272801), (3, 0.16363713), (4, 0.0727279), (5, 0.07272795), (6, 0.0727279), (7, 0.16363345), (8, 0.07272792), (9, 0.16363394)]\n",
      "72: [(0, 0.08000017), (1, 0.08000017), (2, 0.08000019), (3, 0.17999938), (4, 0.08000017), (5, 0.08000018), (6, 0.17999923), (7, 0.08000017), (8, 0.08000017), (9, 0.08000017)]\n",
      "73: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "74: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "75: [(0, 0.08888903), (1, 0.08888904), (2, 0.088889055), (3, 0.08888903), (4, 0.1999987), (5, 0.08888904), (6, 0.08888904), (7, 0.08888904), (8, 0.08888904), (9, 0.08888904)]\n",
      "76: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "77: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "78: [(0, 0.12857293), (1, 0.05714392), (2, 0.05714408), (3, 0.057143882), (4, 0.12856996), (5, 0.12856966), (6, 0.19999473), (7, 0.057143934), (8, 0.12857294), (9, 0.05714396)]\n",
      "79: [(0, 0.4638446), (1, 0.017022016), (2, 0.03829955), (3, 0.0382916), (4, 0.10213172), (5, 0.080850326), (6, 0.03829424), (7, 0.14468016), (8, 0.038292855), (9, 0.038292967)]\n",
      "80: [(0, 0.072727926), (1, 0.07272793), (2, 0.07272804), (3, 0.25454262), (4, 0.072727926), (5, 0.07272798), (6, 0.16363372), (7, 0.07272794), (8, 0.07272795), (9, 0.072727956)]\n",
      "81: [(0, 0.28387725), (1, 0.09032037), (2, 0.09032733), (3, 0.12258391), (4, 0.05806288), (5, 0.09032094), (6, 0.058063958), (7, 0.05806466), (8, 0.090314955), (9, 0.05806378)]\n",
      "82: [(0, 0.179998), (1, 0.08000052), (2, 0.080000594), (3, 0.0800005), (4, 0.08000051), (5, 0.08000056), (6, 0.08000052), (7, 0.08000053), (8, 0.08000053), (9, 0.17999773)]\n",
      "83: [(0, 0.08888917), (1, 0.08888917), (2, 0.19999745), (3, 0.08888916), (4, 0.08888917), (5, 0.08888919), (6, 0.08888917), (7, 0.088889174), (8, 0.088889174), (9, 0.08888918)]\n",
      "84: [(0, 0.088889614), (1, 0.08888962), (2, 0.08888973), (3, 0.0888896), (4, 0.088889614), (5, 0.08888967), (6, 0.08888962), (7, 0.088889636), (8, 0.19999318), (9, 0.08888965)]\n",
      "85: [(0, 0.10588084), (1, 0.047059804), (2, 0.047059953), (3, 0.04705977), (4, 0.28235215), (5, 0.047059868), (6, 0.0470598), (7, 0.10588204), (8, 0.105879135), (9, 0.16470668)]\n",
      "86: [(0, 0.06222284), (1, 0.15110897), (2, 0.017778497), (3, 0.040000644), (4, 0.04000069), (5, 0.039996818), (6, 0.41778815), (7, 0.08444162), (8, 0.10666437), (9, 0.03999744)]\n",
      "87: [(0, 0.107685745), (1, 0.030769808), (2, 0.06923197), (3, 0.2230802), (4, 0.06923118), (5, 0.030769845), (6, 0.030769806), (7, 0.06922594), (8, 0.33846566), (9, 0.030769827)]\n",
      "88: [(0, 0.1555565), (1, 0.26666525), (2, 0.0444455), (3, 0.044445332), (4, 0.10000146), (5, 0.04444542), (6, 0.1555532), (7, 0.04444537), (8, 0.099996574), (9, 0.044445395)]\n",
      "89: [(0, 0.08888925), (1, 0.08888925), (2, 0.08888931), (3, 0.08888924), (4, 0.08888925), (5, 0.08888927), (6, 0.19999671), (7, 0.08888925), (8, 0.088889256), (9, 0.08888926)]\n",
      "90: [(0, 0.08000013), (1, 0.08000013), (2, 0.17999937), (3, 0.17999955), (4, 0.08000013), (5, 0.08000014), (6, 0.08000013), (7, 0.08000013), (8, 0.08000013), (9, 0.08000014)]\n",
      "91: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "92: [(0, 0.038096093), (1, 0.085710295), (2, 0.085713334), (3, 0.22857614), (4, 0.1333339), (5, 0.03809616), (6, 0.085710675), (7, 0.03809611), (8, 0.22857112), (9, 0.038096134)]\n",
      "93: [(0, 0.06153938), (1, 0.13845809), (2, 0.061539527), (3, 0.13846111), (4, 0.06153938), (5, 0.06153945), (6, 0.13846113), (7, 0.13846278), (8, 0.061539404), (9, 0.13845974)]\n",
      "94: [(0, 0.08888917), (1, 0.08888917), (2, 0.19999745), (3, 0.08888916), (4, 0.08888917), (5, 0.08888919), (6, 0.08888917), (7, 0.088889174), (8, 0.088889174), (9, 0.08888918)]\n",
      "95: [(0, 0.02963004), (1, 0.06666346), (2, 0.029630106), (3, 0.02963003), (4, 0.02963004), (5, 0.1777779), (6, 0.25185055), (7, 0.02963005), (8, 0.029630052), (9, 0.32592776)]\n",
      "96: [(0, 0.07777395), (1, 0.13333401), (2, 0.16111663), (3, 0.022222927), (4, 0.16111), (5, 0.0777758), (6, 0.077774696), (7, 0.21666914), (8, 0.022222964), (9, 0.049999915)]\n",
      "97: [(0, 0.03461401), (1, 0.111535914), (2, 0.053846676), (3, 0.130768), (4, 0.073075235), (5, 0.30385998), (6, 0.053843353), (7, 0.16923368), (8, 0.034610253), (9, 0.03461288)]\n",
      "98: [(0, 0.08888917), (1, 0.08888917), (2, 0.19999745), (3, 0.08888916), (4, 0.08888917), (5, 0.08888919), (6, 0.08888917), (7, 0.088889174), (8, 0.088889174), (9, 0.08888918)]\n",
      "99: [(0, 0.08888895), (1, 0.08888895), (2, 0.088888966), (3, 0.08888895), (4, 0.1999994), (5, 0.08888896), (6, 0.08888895), (7, 0.08888895), (8, 0.08888895), (9, 0.08888895)]\n",
      "100: [(0, 0.08888917), (1, 0.08888917), (2, 0.19999745), (3, 0.08888916), (4, 0.08888917), (5, 0.08888919), (6, 0.08888917), (7, 0.088889174), (8, 0.088889174), (9, 0.08888918)]\n",
      "101: [(0, 0.08889004), (1, 0.088890046), (2, 0.088890225), (3, 0.08889001), (4, 0.08889004), (5, 0.08889013), (6, 0.088890046), (7, 0.08889006), (8, 0.08889007), (9, 0.19998933)]\n",
      "102: [(0, 0.06666778), (1, 0.06666779), (2, 0.1500027), (3, 0.06666775), (4, 0.06666778), (5, 0.06666786), (6, 0.06666779), (7, 0.14999439), (8, 0.1499975), (9, 0.1499986)]\n",
      "103: [(0, 0.19999516), (1, 0.088889405), (2, 0.08888949), (3, 0.0888894), (4, 0.088889405), (5, 0.08888945), (6, 0.088889405), (7, 0.08888941), (8, 0.08888942), (9, 0.08888943)]\n",
      "104: [(0, 0.072727785), (1, 0.07272779), (2, 0.16363744), (3, 0.07272777), (4, 0.072727785), (5, 0.07272782), (6, 0.072727785), (7, 0.16363245), (8, 0.0727278), (9, 0.1636356)]\n",
      "105: [(0, 0.14999661), (1, 0.06666744), (2, 0.15000185), (3, 0.06666741), (4, 0.06666743), (5, 0.06666749), (6, 0.14999692), (7, 0.06666745), (8, 0.06666745), (9, 0.1499999)]\n",
      "106: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "107: [(0, 0.088889204), (1, 0.088889204), (2, 0.088889256), (3, 0.0888892), (4, 0.088889204), (5, 0.08888923), (6, 0.19999708), (7, 0.08888921), (8, 0.08888921), (9, 0.08888921)]\n",
      "108: [(0, 0.06666753), (1, 0.31666452), (2, 0.06666766), (3, 0.149995), (4, 0.06666753), (5, 0.06666759), (6, 0.06666753), (7, 0.06666754), (8, 0.06666755), (9, 0.066667564)]\n",
      "109: [(0, 0.08000052), (1, 0.17999808), (2, 0.08000061), (3, 0.080000505), (4, 0.08000053), (5, 0.080000564), (6, 0.08000053), (7, 0.17999758), (8, 0.080000535), (9, 0.08000054)]\n",
      "110: [(0, 0.053333957), (1, 0.2533331), (2, 0.053334057), (3, 0.12000072), (4, 0.053333957), (5, 0.053334), (6, 0.05333396), (7, 0.053333968), (8, 0.18666358), (9, 0.11999866)]\n",
      "111: [(0, 0.07272747), (1, 0.07272747), (2, 0.25454432), (3, 0.072727464), (4, 0.1636359), (5, 0.07272748), (6, 0.07272747), (7, 0.07272747), (8, 0.07272747), (9, 0.07272747)]\n",
      "112: [(0, 0.13846195), (1, 0.13845706), (2, 0.06153937), (3, 0.06153922), (4, 0.061539248), (5, 0.061539307), (6, 0.06153925), (7, 0.29230604), (8, 0.061539263), (9, 0.061539277)]\n",
      "113: [(0, 0.08888938), (1, 0.08888938), (2, 0.088889465), (3, 0.08888937), (4, 0.08888938), (5, 0.08888941), (6, 0.08888938), (7, 0.08888939), (8, 0.19999546), (9, 0.088889405)]\n",
      "114: [(0, 0.08000035), (1, 0.080000356), (2, 0.08000041), (3, 0.08000034), (4, 0.08000035), (5, 0.08000038), (6, 0.080000356), (7, 0.27999678), (8, 0.080000356), (9, 0.08000036)]\n",
      "115: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "116: [(0, 0.088889), (1, 0.088889), (2, 0.08888902), (3, 0.19999892), (4, 0.088889), (5, 0.08888901), (6, 0.088889), (7, 0.088889), (8, 0.088889), (9, 0.088889)]\n",
      "117: [(0, 0.08888917), (1, 0.08888917), (2, 0.19999745), (3, 0.08888916), (4, 0.08888917), (5, 0.08888919), (6, 0.08888917), (7, 0.088889174), (8, 0.088889174), (9, 0.08888918)]\n",
      "118: [(0, 0.08000026), (1, 0.08000027), (2, 0.08000031), (3, 0.08000025), (4, 0.08000026), (5, 0.08000028), (6, 0.27999753), (7, 0.08000027), (8, 0.080000274), (9, 0.080000274)]\n",
      "119: [(0, 0.053333625), (1, 0.05333363), (2, 0.053333674), (3, 0.11999923), (4, 0.31999955), (5, 0.053333648), (6, 0.05333363), (7, 0.18666576), (8, 0.053333633), (9, 0.05333364)]\n",
      "120: [(0, 0.07272781), (1, 0.072727814), (2, 0.16363697), (3, 0.16363621), (4, 0.07272781), (5, 0.07272785), (6, 0.072727814), (7, 0.16363208), (8, 0.07272782), (9, 0.07272783)]\n",
      "121: [(0, 0.1384601), (1, 0.06153924), (2, 0.061539356), (3, 0.061539214), (4, 0.2153842), (5, 0.061539292), (6, 0.061539236), (7, 0.1384608), (8, 0.061539255), (9, 0.1384593)]\n",
      "122: [(0, 0.06666706), (1, 0.066667065), (2, 0.066667125), (3, 0.06666705), (4, 0.066667065), (5, 0.3166656), (6, 0.14999785), (7, 0.06666707), (8, 0.06666707), (9, 0.06666708)]\n",
      "123: [(0, 0.061539095), (1, 0.13845623), (2, 0.0615392), (3, 0.3692308), (4, 0.061539095), (5, 0.061539143), (6, 0.061539102), (7, 0.06153911), (8, 0.061539114), (9, 0.061539125)]\n",
      "124: [(0, 0.053333905), (1, 0.119997546), (2, 0.053333994), (3, 0.120000295), (4, 0.053333905), (5, 0.05333395), (6, 0.18666537), (7, 0.120000295), (8, 0.05333392), (9, 0.18666685)]\n",
      "125: [(0, 0.10768507), (1, 0.030770173), (2, 0.10769623), (3, 0.10769249), (4, 0.22307895), (5, 0.06922845), (6, 0.22308037), (7, 0.030770184), (8, 0.06922792), (9, 0.030770207)]\n",
      "126: [(0, 0.072727606), (1, 0.072727606), (2, 0.07272765), (3, 0.07272759), (4, 0.072727606), (5, 0.16363464), (6, 0.2545445), (7, 0.072727606), (8, 0.072727606), (9, 0.07272761)]\n",
      "127: [(0, 0.08000305), (1, 0.08000308), (2, 0.08000355), (3, 0.08000297), (4, 0.17996874), (5, 0.08000329), (6, 0.18000582), (7, 0.08000312), (8, 0.08000314), (9, 0.080003195)]\n",
      "128: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "129: [(0, 0.072727896), (1, 0.1636342), (2, 0.072728), (3, 0.07272788), (4, 0.072727896), (5, 0.16363615), (6, 0.0727279), (7, 0.07272791), (8, 0.16363424), (9, 0.072727926)]\n",
      "130: [(0, 0.17499624), (1, 0.050000537), (2, 0.05000062), (3, 0.112500496), (4, 0.050000533), (5, 0.050000574), (6, 0.050000537), (7, 0.050000545), (8, 0.36249936), (9, 0.050000556)]\n",
      "131: [(0, 0.050000895), (1, 0.2375001), (2, 0.050001036), (3, 0.05000087), (4, 0.050000895), (5, 0.050000962), (6, 0.11249544), (7, 0.112498194), (8, 0.050000917), (9, 0.23750067)]\n",
      "132: [(0, 0.34544593), (1, 0.07272821), (2, 0.07272835), (3, 0.07272817), (4, 0.0727282), (5, 0.07272827), (6, 0.07272821), (7, 0.07272822), (8, 0.072728224), (9, 0.07272824)]\n",
      "133: [(0, 0.17999509), (1, 0.08000056), (2, 0.08000065), (3, 0.080000535), (4, 0.08000056), (5, 0.080000594), (6, 0.08000056), (7, 0.18000033), (8, 0.080000564), (9, 0.08000058)]\n",
      "134: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "135: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "136: [(0, 0.057143405), (1, 0.05714341), (2, 0.05714349), (3, 0.057143386), (4, 0.12856828), (5, 0.12856975), (6, 0.05714341), (7, 0.057143413), (8, 0.34285802), (9, 0.057143427)]\n",
      "137: [(0, 0.08888925), (1, 0.08888925), (2, 0.08888931), (3, 0.08888924), (4, 0.08888925), (5, 0.19999664), (6, 0.08888925), (7, 0.088889256), (8, 0.088889256), (9, 0.08888926)]\n",
      "138: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "139: [(0, 0.080000475), (1, 0.08000048), (2, 0.08000056), (3, 0.18000025), (4, 0.080000475), (5, 0.08000051), (6, 0.080000475), (7, 0.08000049), (8, 0.08000049), (9, 0.17999578)]\n",
      "140: [(0, 0.1799931), (1, 0.08000082), (2, 0.080000944), (3, 0.08000079), (4, 0.08000082), (5, 0.08000088), (6, 0.18000014), (7, 0.08000083), (8, 0.08000083), (9, 0.08000085)]\n",
      "141: [(0, 0.05333362), (1, 0.053333625), (2, 0.05333367), (3, 0.053333614), (4, 0.05333362), (5, 0.053333644), (6, 0.053333625), (7, 0.05333363), (8, 0.05333363), (9, 0.5199973)]\n",
      "142: [(0, 0.14999303), (1, 0.06666743), (2, 0.23333654), (3, 0.06666741), (4, 0.06666742), (5, 0.06666748), (6, 0.06666743), (7, 0.06666744), (8, 0.066667445), (9, 0.14999834)]\n",
      "143: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "144: [(0, 0.08000012), (1, 0.08000012), (2, 0.17999965), (3, 0.08000012), (4, 0.17999937), (5, 0.080000125), (6, 0.08000012), (7, 0.080000125), (8, 0.080000125), (9, 0.080000125)]\n",
      "145: [(0, 0.07272753), (1, 0.07272753), (2, 0.34545222), (3, 0.072727524), (4, 0.07272753), (5, 0.072727546), (6, 0.07272753), (7, 0.07272754), (8, 0.07272754), (9, 0.07272754)]\n",
      "146: [(0, 0.08888893), (1, 0.08888893), (2, 0.19999963), (3, 0.08888893), (4, 0.08888893), (5, 0.08888893), (6, 0.08888893), (7, 0.08888893), (8, 0.08888893), (9, 0.08888893)]\n",
      "147: [(0, 0.058060605), (1, 0.025806952), (2, 0.025807029), (3, 0.02697258), (4, 0.025806949), (5, 0.058060743), (6, 0.025806952), (7, 0.0903175), (8, 0.63755375), (9, 0.02580697)]\n",
      "148: [(0, 0.08888938), (1, 0.08888938), (2, 0.088889465), (3, 0.08888937), (4, 0.08888938), (5, 0.08888941), (6, 0.08888938), (7, 0.08888939), (8, 0.19999543), (9, 0.088889405)]\n",
      "149: [(0, 0.08888932), (1, 0.08888932), (2, 0.0888894), (3, 0.08888931), (4, 0.08888932), (5, 0.08888935), (6, 0.08888932), (7, 0.08888933), (8, 0.19999595), (9, 0.088889346)]\n",
      "150: [(0, 0.06666739), (1, 0.0666674), (2, 0.066667505), (3, 0.1499991), (4, 0.06666739), (5, 0.2333309), (6, 0.0666674), (7, 0.06666741), (8, 0.14999807), (9, 0.06666742)]\n",
      "151: [(0, 0.053334165), (1, 0.18666178), (2, 0.25333506), (3, 0.053334147), (4, 0.11999773), (5, 0.05333423), (6, 0.053334173), (7, 0.053334188), (8, 0.05333419), (9, 0.120000325)]\n",
      "152: [(0, 0.061538815), (1, 0.06153882), (2, 0.13846193), (3, 0.061538808), (4, 0.06153882), (5, 0.13845812), (6, 0.29230818), (7, 0.061538827), (8, 0.06153883), (9, 0.061538834)]\n",
      "153: [(0, 0.072728135), (1, 0.07272814), (2, 0.072728276), (3, 0.07272811), (4, 0.07272814), (5, 0.0727282), (6, 0.07272814), (7, 0.07272816), (8, 0.25453952), (9, 0.16363516)]\n",
      "154: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "155: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "156: [(0, 0.13845596), (1, 0.06153969), (2, 0.061539873), (3, 0.21538134), (4, 0.06153968), (5, 0.061539773), (6, 0.21538453), (7, 0.061539702), (8, 0.061539713), (9, 0.061539732)]\n",
      "157: [(0, 0.0533341), (1, 0.05333411), (2, 0.053334225), (3, 0.18666549), (4, 0.0533341), (5, 0.053334158), (6, 0.31999886), (7, 0.053334117), (8, 0.11999673), (9, 0.05333413)]\n",
      "158: [(0, 0.08888896), (1, 0.088888966), (2, 0.08888897), (3, 0.08888896), (4, 0.19999929), (5, 0.088888966), (6, 0.08888896), (7, 0.088888966), (8, 0.088888966), (9, 0.088888966)]\n",
      "159: [(0, 0.20000024), (1, 0.042105857), (2, 0.09473812), (3, 0.19612087), (4, 0.042105854), (5, 0.0421059), (6, 0.14736882), (7, 0.14736623), (8, 0.045982257), (9, 0.04210588)]\n",
      "160: [(0, 0.11147374), (1, 0.22623536), (2, 0.029509187), (3, 0.09508202), (4, 0.09508234), (5, 0.013115382), (6, 0.19344798), (7, 0.111475326), (8, 0.045893423), (9, 0.07868523)]\n",
      "161: [(0, 0.08000039), (1, 0.08000039), (2, 0.08000045), (3, 0.08000038), (4, 0.18000007), (5, 0.08000042), (6, 0.17999673), (7, 0.08000039), (8, 0.08000039), (9, 0.08000041)]\n",
      "162: [(0, 0.053333815), (1, 0.05333382), (2, 0.053333897), (3, 0.25333196), (4, 0.053333815), (5, 0.18666455), (6, 0.18666665), (7, 0.05333383), (8, 0.05333383), (9, 0.05333384)]\n",
      "163: [(0, 0.088889204), (1, 0.088889204), (2, 0.088889256), (3, 0.0888892), (4, 0.088889204), (5, 0.08888923), (6, 0.088889204), (7, 0.1999971), (8, 0.08888921), (9, 0.08888921)]\n",
      "164: [(0, 0.061538797), (1, 0.0615388), (2, 0.061538853), (3, 0.13846157), (4, 0.061538797), (5, 0.061538827), (6, 0.0615388), (7, 0.13846125), (8, 0.061538808), (9, 0.2923055)]\n",
      "165: [(0, 0.07272772), (1, 0.16363642), (2, 0.16363724), (3, 0.07272771), (4, 0.07272772), (5, 0.072727755), (6, 0.072727725), (7, 0.07272773), (8, 0.16363223), (9, 0.07272774)]\n",
      "166: [(0, 0.16363542), (1, 0.07272761), (2, 0.072727665), (3, 0.16363628), (4, 0.16363491), (5, 0.072727636), (6, 0.07272761), (7, 0.07272761), (8, 0.07272762), (9, 0.07272763)]\n",
      "167: [(0, 0.072732754), (1, 0.16364801), (2, 0.07273364), (3, 0.072732605), (4, 0.25448826), (5, 0.07273317), (6, 0.0727328), (7, 0.072732866), (8, 0.0727329), (9, 0.072733)]\n",
      "168: [(0, 0.08888925), (1, 0.08888925), (2, 0.08888931), (3, 0.08888924), (4, 0.08888925), (5, 0.08888928), (6, 0.08888925), (7, 0.088889256), (8, 0.1999967), (9, 0.08888926)]\n",
      "169: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "170: [(0, 0.061539385), (1, 0.061539397), (2, 0.06153954), (3, 0.13845472), (4, 0.1384629), (5, 0.2153865), (6, 0.061539397), (7, 0.13845935), (8, 0.061539408), (9, 0.061539426)]\n",
      "171: [(0, 0.31666017), (1, 0.06666737), (2, 0.066667475), (3, 0.06666735), (4, 0.06666737), (5, 0.066667415), (6, 0.06666737), (7, 0.15000068), (8, 0.066667385), (9, 0.0666674)]\n",
      "172: [(0, 0.08000036), (1, 0.17999987), (2, 0.08000042), (3, 0.08000035), (4, 0.08000036), (5, 0.08000039), (6, 0.08000036), (7, 0.17999712), (8, 0.08000036), (9, 0.08000038)]\n",
      "173: [(0, 0.0666675), (1, 0.14999823), (2, 0.06666763), (3, 0.06666748), (4, 0.14999774), (5, 0.066667564), (6, 0.066667505), (7, 0.06666752), (8, 0.23333134), (9, 0.066667534)]\n",
      "174: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "175: [(0, 0.08888897), (1, 0.08888897), (2, 0.08888899), (3, 0.08888897), (4, 0.08888897), (5, 0.08888898), (6, 0.08888897), (7, 0.19999915), (8, 0.08888897), (9, 0.08888897)]\n",
      "176: [(0, 0.07272751), (1, 0.07272751), (2, 0.16363658), (3, 0.2545433), (4, 0.07272751), (5, 0.072727524), (6, 0.07272751), (7, 0.07272751), (8, 0.07272751), (9, 0.07272752)]\n",
      "177: [(0, 0.099997714), (1, 0.04444541), (2, 0.044445552), (3, 0.21111463), (4, 0.044445403), (5, 0.09999785), (6, 0.21111088), (7, 0.099997945), (8, 0.044445425), (9, 0.09999926)]\n",
      "178: [(0, 0.17999978), (1, 0.080000356), (2, 0.08000041), (3, 0.08000034), (4, 0.1799973), (5, 0.08000038), (6, 0.080000356), (7, 0.080000356), (8, 0.080000356), (9, 0.08000036)]\n",
      "179: [(0, 0.08000149), (1, 0.0800015), (2, 0.080001734), (3, 0.08000145), (4, 0.17999102), (5, 0.08000161), (6, 0.0800015), (7, 0.08000152), (8, 0.08000153), (9, 0.17999658)]\n",
      "180: [(0, 0.0666675), (1, 0.066667505), (2, 0.14999913), (3, 0.15000053), (4, 0.0666675), (5, 0.066667564), (6, 0.066667505), (7, 0.06666751), (8, 0.14999652), (9, 0.1499988)]\n",
      "181: [(0, 0.0666675), (1, 0.06666751), (2, 0.06666763), (3, 0.14999734), (4, 0.14999746), (5, 0.066667564), (6, 0.066667505), (7, 0.06666752), (8, 0.06666753), (9, 0.23333243)]\n",
      "182: [(0, 0.08000016), (1, 0.08000016), (2, 0.17999925), (3, 0.080000155), (4, 0.17999946), (5, 0.08000017), (6, 0.08000016), (7, 0.08000016), (8, 0.08000016), (9, 0.08000016)]\n",
      "183: [(0, 0.07825786), (1, 0.07825905), (2, 0.034784008), (3, 0.16521893), (4, 0.034783814), (5, 0.0782601), (6, 0.12173961), (7, 0.16522053), (8, 0.121735625), (9, 0.12174048)]\n",
      "184: [(0, 0.13845402), (1, 0.13846274), (2, 0.061540075), (3, 0.06153981), (4, 0.13846055), (5, 0.061539955), (6, 0.061539862), (7, 0.13846213), (8, 0.13846093), (9, 0.06153991)]\n",
      "185: [(0, 0.08888954), (1, 0.19999395), (2, 0.08888964), (3, 0.08888952), (4, 0.08888954), (5, 0.088889584), (6, 0.08888955), (7, 0.088889554), (8, 0.088889554), (9, 0.08888956)]\n",
      "186: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "187: [(0, 0.088889204), (1, 0.088889204), (2, 0.088889256), (3, 0.0888892), (4, 0.088889204), (5, 0.08888923), (6, 0.088889204), (7, 0.08888921), (8, 0.08888921), (9, 0.19999708)]\n",
      "188: [(0, 0.116664246), (1, 0.32499865), (2, 0.03333402), (3, 0.03333391), (4, 0.24166937), (5, 0.03333397), (6, 0.03333393), (7, 0.03333394), (8, 0.03333394), (9, 0.11666402)]\n",
      "189: [(0, 0.08000134), (1, 0.17999552), (2, 0.08000156), (3, 0.08000131), (4, 0.08000135), (5, 0.08000145), (6, 0.080001354), (7, 0.08000137), (8, 0.17999338), (9, 0.0800014)]\n",
      "190: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "191: [(0, 0.06666693), (1, 0.06666693), (2, 0.23333378), (3, 0.14999913), (4, 0.14999853), (5, 0.066666946), (6, 0.06666693), (7, 0.06666693), (8, 0.06666693), (9, 0.06666694)]\n",
      "192: [(0, 0.07272776), (1, 0.16363253), (2, 0.16363737), (3, 0.07272775), (4, 0.07272776), (5, 0.16363573), (6, 0.07272777), (7, 0.07272777), (8, 0.07272777), (9, 0.072727785)]\n",
      "193: [(0, 0.29230624), (1, 0.06153968), (2, 0.13846166), (3, 0.06153964), (4, 0.061539672), (5, 0.06153976), (6, 0.06153968), (7, 0.0615397), (8, 0.13845421), (9, 0.06153973)]\n",
      "194: [(0, 0.1799952), (1, 0.0800005), (2, 0.18000083), (3, 0.08000048), (4, 0.08000049), (5, 0.080000535), (6, 0.0800005), (7, 0.080000505), (8, 0.080000505), (9, 0.08000051)]\n",
      "195: [(0, 0.05333399), (1, 0.12000054), (2, 0.0533341), (3, 0.12000052), (4, 0.25333184), (5, 0.11999982), (6, 0.053333998), (7, 0.05333401), (8, 0.11999713), (9, 0.053334024)]\n",
      "196: [(0, 0.08000023), (1, 0.08000024), (2, 0.080000274), (3, 0.08000023), (4, 0.08000023), (5, 0.17999955), (6, 0.08000024), (7, 0.08000024), (8, 0.08000024), (9, 0.17999849)]\n",
      "197: [(0, 0.17999905), (1, 0.08000053), (2, 0.08000061), (3, 0.080000505), (4, 0.08000053), (5, 0.080000564), (6, 0.08000053), (7, 0.17999661), (8, 0.080000535), (9, 0.08000054)]\n",
      "198: [(0, 0.07272786), (1, 0.07272787), (2, 0.07272795), (3, 0.072727844), (4, 0.16363707), (5, 0.0727279), (6, 0.07272787), (7, 0.16363291), (8, 0.072727874), (9, 0.16363482)]\n",
      "199: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "200: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "201: [(0, 0.08888904), (1, 0.1999986), (2, 0.088889055), (3, 0.08888903), (4, 0.08888904), (5, 0.08888904), (6, 0.08888904), (7, 0.08888904), (8, 0.08888904), (9, 0.08888904)]\n",
      "202: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "203: [(0, 0.072727844), (1, 0.072727844), (2, 0.07272793), (3, 0.16363698), (4, 0.072727844), (5, 0.07272788), (6, 0.072727844), (7, 0.07272785), (8, 0.16363421), (9, 0.16363372)]\n",
      "204: [(0, 0.06666733), (1, 0.06666733), (2, 0.06666744), (3, 0.06666731), (4, 0.1500009), (5, 0.14999987), (6, 0.06666733), (7, 0.14999919), (8, 0.14999597), (9, 0.06666736)]\n",
      "205: [(0, 0.16363439), (1, 0.07272797), (2, 0.07272807), (3, 0.07272794), (4, 0.07272796), (5, 0.07272801), (6, 0.07272796), (7, 0.25454178), (8, 0.07272798), (9, 0.07272799)]\n",
      "206: [(0, 0.06153905), (1, 0.29230565), (2, 0.061539143), (3, 0.06153903), (4, 0.06153905), (5, 0.06153909), (6, 0.21538174), (7, 0.06153906), (8, 0.061539065), (9, 0.061539073)]\n",
      "207: [(0, 0.18666507), (1, 0.053334463), (2, 0.05333463), (3, 0.053334422), (4, 0.053334452), (5, 0.053334538), (6, 0.11999632), (7, 0.05333448), (8, 0.11999717), (9, 0.25333446)]\n",
      "208: [(0, 0.08888939), (1, 0.19999526), (2, 0.08888948), (3, 0.088889375), (4, 0.08888939), (5, 0.088889435), (6, 0.0888894), (7, 0.088889405), (8, 0.088889405), (9, 0.08888942)]\n",
      "209: [(0, 0.08000034), (1, 0.08000034), (2, 0.08000039), (3, 0.080000326), (4, 0.08000034), (5, 0.08000036), (6, 0.27999687), (7, 0.08000035), (8, 0.08000035), (9, 0.08000035)]\n",
      "210: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "211: [(0, 0.06153886), (1, 0.061538864), (2, 0.1384617), (3, 0.061538853), (4, 0.06153886), (5, 0.061538894), (6, 0.061538864), (7, 0.36922735), (8, 0.061538875), (9, 0.06153888)]\n",
      "212: [(0, 0.08000039), (1, 0.08000039), (2, 0.08000045), (3, 0.08000038), (4, 0.08000039), (5, 0.08000042), (6, 0.08000039), (7, 0.27999642), (8, 0.08000039), (9, 0.08000041)]\n",
      "213: [(0, 0.07272832), (1, 0.16363318), (2, 0.0727285), (3, 0.0727283), (4, 0.16363667), (5, 0.0727284), (6, 0.072728336), (7, 0.07272835), (8, 0.07272836), (9, 0.16363163)]\n",
      "214: [(0, 0.042106036), (1, 0.19999951), (2, 0.14737143), (3, 0.09473033), (4, 0.25263095), (5, 0.042106096), (6, 0.094737455), (7, 0.04210605), (8, 0.042106055), (9, 0.04210607)]\n",
      "215: [(0, 0.13999996), (1, 0.13999902), (2, 0.04000124), (3, 0.14000286), (4, 0.04000107), (5, 0.1899994), (6, 0.14000036), (7, 0.08999392), (8, 0.040001094), (9, 0.040001117)]\n",
      "216: [(0, 0.08000052), (1, 0.08000053), (2, 0.08000061), (3, 0.080000505), (4, 0.08000053), (5, 0.080000564), (6, 0.08000053), (7, 0.080000535), (8, 0.17999598), (9, 0.17999971)]\n",
      "217: [(0, 0.040001467), (1, 0.08999702), (2, 0.14000422), (3, 0.13999842), (4, 0.19000255), (5, 0.040001575), (6, 0.13999109), (7, 0.040001497), (8, 0.09000251), (9, 0.08999967)]\n",
      "218: [(0, 0.08000043), (1, 0.08000044), (2, 0.18000081), (3, 0.08000042), (4, 0.08000043), (5, 0.08000047), (6, 0.08000044), (7, 0.080000445), (8, 0.080000445), (9, 0.17999567)]\n",
      "219: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "220: [(0, 0.088889115), (1, 0.08888912), (2, 0.08888915), (3, 0.08888911), (4, 0.088889115), (5, 0.19999792), (6, 0.088889115), (7, 0.08888912), (8, 0.08888912), (9, 0.08888913)]\n",
      "221: [(0, 0.13845754), (1, 0.061539356), (2, 0.061539486), (3, 0.061539322), (4, 0.061539344), (5, 0.061539415), (6, 0.1384585), (7, 0.061539363), (8, 0.061539367), (9, 0.29230833)]\n",
      "222: [(0, 0.08888895), (1, 0.08888895), (2, 0.1999995), (3, 0.08888895), (4, 0.08888895), (5, 0.08888895), (6, 0.08888895), (7, 0.08888895), (8, 0.08888895), (9, 0.08888895)]\n",
      "223: [(0, 0.044445135), (1, 0.044445142), (2, 0.044445246), (3, 0.4333329), (4, 0.044445135), (5, 0.044445187), (6, 0.044445142), (7, 0.15555134), (8, 0.044445153), (9, 0.099999584)]\n",
      "224: [(0, 0.080000564), (1, 0.08000057), (2, 0.18000104), (3, 0.08000054), (4, 0.17999434), (5, 0.0800006), (6, 0.08000057), (7, 0.08000057), (8, 0.08000058), (9, 0.08000059)]\n",
      "225: [(0, 0.08888908), (1, 0.08888908), (2, 0.088889115), (3, 0.08888908), (4, 0.19999824), (5, 0.08888909), (6, 0.08888908), (7, 0.088889085), (8, 0.088889085), (9, 0.088889085)]\n",
      "226: [(0, 0.0615395), (1, 0.13845785), (2, 0.06153967), (3, 0.06153947), (4, 0.13846193), (5, 0.13845804), (6, 0.06153951), (7, 0.13846307), (8, 0.1384614), (9, 0.061539546)]\n",
      "227: [(0, 0.08888895), (1, 0.08888895), (2, 0.1999995), (3, 0.08888895), (4, 0.08888895), (5, 0.08888895), (6, 0.08888895), (7, 0.08888895), (8, 0.08888895), (9, 0.08888895)]\n",
      "228: [(0, 0.13846113), (1, 0.061539058), (2, 0.1384601), (3, 0.06153903), (4, 0.06153905), (5, 0.06153909), (6, 0.061539058), (7, 0.29230538), (8, 0.061539065), (9, 0.061539073)]\n",
      "229: [(0, 0.072728075), (1, 0.16363734), (2, 0.0727282), (3, 0.16363221), (4, 0.072728075), (5, 0.072728135), (6, 0.16363366), (7, 0.07272809), (8, 0.0727281), (9, 0.07272811)]\n",
      "230: [(0, 0.12856849), (1, 0.05714355), (2, 0.05714366), (3, 0.12857203), (4, 0.12857237), (5, 0.12856911), (6, 0.20000005), (7, 0.05714356), (8, 0.057143565), (9, 0.057143576)]\n",
      "231: [(0, 0.047059096), (1, 0.0470591), (2, 0.10588267), (3, 0.16470602), (4, 0.047059096), (5, 0.04705912), (6, 0.39999762), (7, 0.047059104), (8, 0.047059104), (9, 0.047059108)]\n",
      "232: [(0, 0.12857185), (1, 0.12857412), (2, 0.05714459), (3, 0.057144314), (4, 0.12857422), (5, 0.057144463), (6, 0.19998738), (7, 0.12857027), (8, 0.057144392), (9, 0.057144415)]\n",
      "233: [(0, 0.08888895), (1, 0.08888895), (2, 0.1999995), (3, 0.08888895), (4, 0.08888895), (5, 0.08888895), (6, 0.08888895), (7, 0.08888895), (8, 0.08888895), (9, 0.08888895)]\n",
      "234: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "235: [(0, 0.16363037), (1, 0.0727282), (2, 0.07272834), (3, 0.072728164), (4, 0.16363697), (5, 0.07272826), (6, 0.0727282), (7, 0.16363508), (8, 0.07272822), (9, 0.07272823)]\n",
      "236: [(0, 0.061539017), (1, 0.06153902), (2, 0.13846251), (3, 0.061539), (4, 0.061539017), (5, 0.061539058), (6, 0.21538216), (7, 0.061539028), (8, 0.061539028), (9, 0.21538214)]\n",
      "237: [(0, 0.15555535), (1, 0.04444487), (2, 0.044444934), (3, 0.044444855), (4, 0.09999853), (5, 0.21110965), (6, 0.04444487), (7, 0.044444874), (8, 0.2666672), (9, 0.044444885)]\n",
      "238: [(0, 0.08888925), (1, 0.08888925), (2, 0.08888931), (3, 0.08888924), (4, 0.08888925), (5, 0.08888928), (6, 0.08888925), (7, 0.088889256), (8, 0.19999671), (9, 0.08888926)]\n",
      "239: [(0, 0.057143863), (1, 0.12856805), (2, 0.05714403), (3, 0.057143837), (4, 0.057143867), (5, 0.05714394), (6, 0.12856796), (7, 0.19999865), (8, 0.057143893), (9, 0.20000187)]\n",
      "240: [(0, 0.06153904), (1, 0.061539043), (2, 0.061539132), (3, 0.36922926), (4, 0.06153904), (5, 0.061539084), (6, 0.1384582), (7, 0.06153905), (8, 0.061539058), (9, 0.061539065)]\n",
      "241: [(0, 0.06666709), (1, 0.06666709), (2, 0.06666715), (3, 0.06666707), (4, 0.06666709), (5, 0.23332974), (6, 0.06666709), (7, 0.06666709), (8, 0.23333348), (9, 0.0666671)]\n",
      "242: [(0, 0.08000088), (1, 0.17999539), (2, 0.08000103), (3, 0.080000855), (4, 0.08000088), (5, 0.08000095), (6, 0.17999727), (7, 0.0800009), (8, 0.08000091), (9, 0.08000092)]\n",
      "243: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "244: [(0, 0.17999384), (1, 0.08000084), (2, 0.08000097), (3, 0.08000081), (4, 0.08000083), (5, 0.17999925), (6, 0.08000084), (7, 0.080000855), (8, 0.080000855), (9, 0.08000087)]\n",
      "245: [(0, 0.08888897), (1, 0.19999918), (2, 0.08888899), (3, 0.08888897), (4, 0.08888897), (5, 0.08888898), (6, 0.08888897), (7, 0.08888897), (8, 0.08888897), (9, 0.08888897)]\n",
      "246: [(0, 0.08000094), (1, 0.17999864), (2, 0.08000109), (3, 0.080000915), (4, 0.08000094), (5, 0.08000101), (6, 0.08000095), (7, 0.08000096), (8, 0.17999355), (9, 0.08000098)]\n",
      "247: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "248: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "249: [(0, 0.08888908), (1, 0.088889085), (2, 0.19999816), (3, 0.08888908), (4, 0.08888908), (5, 0.0888891), (6, 0.088889085), (7, 0.088889085), (8, 0.088889085), (9, 0.08888909)]\n",
      "250: [(0, 0.10586892), (1, 0.0470599), (2, 0.04706006), (3, 0.3411822), (4, 0.04705989), (5, 0.04705997), (6, 0.22352923), (7, 0.047059916), (8, 0.04705992), (9, 0.04705994)]\n",
      "251: [(0, 0.05714347), (1, 0.057143472), (2, 0.057143565), (3, 0.05714345), (4, 0.05714347), (5, 0.057143517), (6, 0.19999765), (7, 0.12857044), (8, 0.057143487), (9, 0.27142745)]\n",
      "252: [(0, 0.09999821), (1, 0.09999709), (2, 0.15555649), (3, 0.044445127), (4, 0.10000092), (5, 0.044445198), (6, 0.044445153), (7, 0.32222143), (8, 0.04444517), (9, 0.044445176)]\n",
      "253: [(0, 0.0666675), (1, 0.14999817), (2, 0.06666763), (3, 0.06666748), (4, 0.0666675), (5, 0.14999893), (6, 0.066667505), (7, 0.06666752), (8, 0.06666753), (9, 0.23333025)]\n",
      "254: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "255: [(0, 0.06153908), (1, 0.21538585), (2, 0.13846233), (3, 0.06153906), (4, 0.06153908), (5, 0.13846037), (6, 0.061539084), (7, 0.06153909), (8, 0.061539095), (9, 0.13845699)]\n",
      "256: [(0, 0.080000594), (1, 0.080000594), (2, 0.08000068), (3, 0.18000042), (4, 0.17999464), (5, 0.08000063), (6, 0.080000594), (7, 0.0800006), (8, 0.08000061), (9, 0.08000062)]\n",
      "257: [(0, 0.080000475), (1, 0.08000048), (2, 0.08000056), (3, 0.08000047), (4, 0.080000475), (5, 0.08000051), (6, 0.08000048), (7, 0.27999556), (8, 0.08000049), (9, 0.0800005)]\n",
      "258: [(0, 0.4333309), (1, 0.044444904), (2, 0.04444497), (3, 0.044444885), (4, 0.0444449), (5, 0.044444934), (6, 0.044444904), (7, 0.044444907), (8, 0.21110974), (9, 0.04444492)]\n",
      "259: [(0, 0.07272761), (1, 0.07272762), (2, 0.07272767), (3, 0.16363646), (4, 0.16363648), (5, 0.16363364), (6, 0.07272762), (7, 0.07272763), (8, 0.07272763), (9, 0.07272763)]\n",
      "260: [(0, 0.08888918), (1, 0.08888919), (2, 0.08888924), (3, 0.08888918), (4, 0.19999725), (5, 0.08888921), (6, 0.08888919), (7, 0.08888919), (8, 0.0888892), (9, 0.088889204)]\n",
      "261: [(0, 0.040000867), (1, 0.24000305), (2, 0.040001005), (3, 0.04000084), (4, 0.189998), (5, 0.04000093), (6, 0.13999423), (7, 0.18999925), (8, 0.04000089), (9, 0.040000904)]\n",
      "262: [(0, 0.16363162), (1, 0.07272793), (2, 0.07272804), (3, 0.07272791), (4, 0.072727926), (5, 0.07272798), (6, 0.25454476), (7, 0.07272794), (8, 0.07272795), (9, 0.072727956)]\n",
      "263: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "264: [(0, 0.18666707), (1, 0.18666574), (2, 0.05333493), (3, 0.053334672), (4, 0.05333471), (5, 0.053334814), (6, 0.05333472), (7, 0.11999951), (8, 0.11999434), (9, 0.11999952)]\n",
      "265: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "266: [(0, 0.17272744), (1, 0.08181883), (2, 0.08181561), (3, 0.03636435), (4, 0.03636437), (5, 0.35455072), (6, 0.08181533), (7, 0.08181455), (8, 0.036364388), (9, 0.036364403)]\n",
      "267: [(0, 0.07272815), (1, 0.16363057), (2, 0.0727283), (3, 0.07272813), (4, 0.16363686), (5, 0.072728224), (6, 0.07272816), (7, 0.07272817), (8, 0.07272817), (9, 0.16363525)]\n",
      "268: [(0, 0.08888927), (1, 0.08888928), (2, 0.08888934), (3, 0.08888926), (4, 0.08888927), (5, 0.0888893), (6, 0.19999638), (7, 0.088889286), (8, 0.088889286), (9, 0.08888929)]\n",
      "269: [(0, 0.07272761), (1, 0.07272762), (2, 0.07272767), (3, 0.072727606), (4, 0.16363548), (5, 0.07272764), (6, 0.07272762), (7, 0.2545435), (8, 0.07272763), (9, 0.07272763)]\n",
      "270: [(0, 0.16363034), (1, 0.072728574), (2, 0.07272877), (3, 0.25454074), (4, 0.07272856), (5, 0.072728656), (6, 0.072728574), (7, 0.07272859), (8, 0.07272859), (9, 0.07272861)]\n",
      "271: [(0, 0.06153921), (1, 0.13845772), (2, 0.06153933), (3, 0.06153919), (4, 0.06153921), (5, 0.061539266), (6, 0.29230866), (7, 0.061539225), (8, 0.13845895), (9, 0.06153924)]\n",
      "272: [(0, 0.07272771), (1, 0.07272771), (2, 0.07272778), (3, 0.16363639), (4, 0.07272771), (5, 0.07272774), (6, 0.07272771), (7, 0.07272772), (8, 0.25454178), (9, 0.072727725)]\n",
      "273: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "274: [(0, 0.072727926), (1, 0.07272793), (2, 0.07272804), (3, 0.1636329), (4, 0.072727926), (5, 0.16363707), (6, 0.07272793), (7, 0.07272794), (8, 0.16363442), (9, 0.072727956)]\n",
      "275: [(0, 0.06666696), (1, 0.3999973), (2, 0.06666701), (3, 0.06666695), (4, 0.06666696), (5, 0.06666698), (6, 0.06666697), (7, 0.06666697), (8, 0.06666697), (9, 0.066666976)]\n",
      "276: [(0, 0.08888907), (1, 0.19999826), (2, 0.08888911), (3, 0.08888906), (4, 0.08888907), (5, 0.088889085), (6, 0.08888907), (7, 0.08888908), (8, 0.08888908), (9, 0.08888908)]\n",
      "277: [(0, 0.061538834), (1, 0.061538838), (2, 0.13846195), (3, 0.061538823), (4, 0.21538433), (5, 0.06153886), (6, 0.061538834), (7, 0.21538183), (8, 0.06153884), (9, 0.06153885)]\n",
      "278: [(0, 0.08000071), (1, 0.17999582), (2, 0.08000083), (3, 0.0800007), (4, 0.08000071), (5, 0.080000766), (6, 0.17999826), (7, 0.080000736), (8, 0.080000736), (9, 0.08000075)]\n",
      "279: [(0, 0.05000073), (1, 0.050000735), (2, 0.050000846), (3, 0.05000071), (4, 0.05000073), (5, 0.30000204), (6, 0.112498775), (7, 0.17499729), (8, 0.05000075), (9, 0.11249737)]\n",
      "280: [(0, 0.14999013), (1, 0.06666786), (2, 0.23333687), (3, 0.066667825), (4, 0.066667855), (5, 0.066667944), (6, 0.06666786), (7, 0.14999783), (8, 0.066667885), (9, 0.06666791)]\n",
      "281: [(0, 0.06153999), (1, 0.061540004), (2, 0.061540235), (3, 0.13845983), (4, 0.13845617), (5, 0.06154011), (6, 0.061540004), (7, 0.1384583), (8, 0.21538532), (9, 0.061540056)]\n",
      "282: [(0, 0.06666733), (1, 0.06666733), (2, 0.06666744), (3, 0.1499965), (4, 0.06666733), (5, 0.06666738), (6, 0.149998), (7, 0.06666735), (8, 0.06666735), (9, 0.23333396)]\n",
      "283: [(0, 0.08888903), (1, 0.08888904), (2, 0.088889055), (3, 0.08888903), (4, 0.08888903), (5, 0.08888904), (6, 0.08888904), (7, 0.08888904), (8, 0.08888904), (9, 0.1999987)]\n",
      "284: [(0, 0.07272774), (1, 0.07272775), (2, 0.16363674), (3, 0.07272773), (4, 0.07272774), (5, 0.16363569), (6, 0.07272775), (7, 0.072727755), (8, 0.16363336), (9, 0.07272776)]\n",
      "285: [(0, 0.088888995), (1, 0.088889), (2, 0.08888902), (3, 0.088888995), (4, 0.088889), (5, 0.08888901), (6, 0.088889), (7, 0.088889), (8, 0.19999897), (9, 0.088889)]\n",
      "286: [(0, 0.16470997), (1, 0.105879694), (2, 0.04706051), (3, 0.16470976), (4, 0.047060277), (5, 0.10587825), (6, 0.10588097), (7, 0.105882026), (8, 0.047060315), (9, 0.105878256)]\n",
      "287: [(0, 0.16362582), (1, 0.072729275), (2, 0.07272958), (3, 0.07272921), (4, 0.07272926), (5, 0.072729416), (6, 0.072729275), (7, 0.072729304), (8, 0.25453952), (9, 0.07272935)]\n",
      "288: [(0, 0.066667214), (1, 0.066667214), (2, 0.066667296), (3, 0.0666672), (4, 0.066667214), (5, 0.15000036), (6, 0.14999615), (7, 0.2333329), (8, 0.06666723), (9, 0.06666724)]\n",
      "289: [(0, 0.057143435), (1, 0.12856999), (2, 0.05714353), (3, 0.27143055), (4, 0.057143435), (5, 0.057143483), (6, 0.057143442), (7, 0.12856711), (8, 0.12857156), (9, 0.05714346)]\n",
      "290: [(0, 0.080000155), (1, 0.2799985), (2, 0.080000184), (3, 0.080000155), (4, 0.080000155), (5, 0.08000016), (6, 0.08000016), (7, 0.08000016), (8, 0.08000016), (9, 0.08000016)]\n",
      "291: [(0, 0.066667244), (1, 0.23333192), (2, 0.06666733), (3, 0.15000065), (4, 0.14999656), (5, 0.06666729), (6, 0.066667244), (7, 0.06666726), (8, 0.06666726), (9, 0.066667266)]\n",
      "292: [(0, 0.29000184), (1, 0.040000867), (2, 0.040001), (3, 0.14000018), (4, 0.09000133), (5, 0.089997984), (6, 0.13999856), (7, 0.040000882), (8, 0.040000882), (9, 0.08999645)]\n",
      "293: [(0, 0.088889204), (1, 0.088889204), (2, 0.088889256), (3, 0.0888892), (4, 0.088889204), (5, 0.08888923), (6, 0.088889204), (7, 0.19999708), (8, 0.08888921), (9, 0.08888921)]\n",
      "294: [(0, 0.057143804), (1, 0.057143815), (2, 0.05714396), (3, 0.05714378), (4, 0.128569), (5, 0.12857205), (6, 0.19999799), (7, 0.1999979), (8, 0.05714383), (9, 0.05714385)]\n",
      "295: [(0, 0.08000046), (1, 0.08000047), (2, 0.08000054), (3, 0.08000045), (4, 0.08000046), (5, 0.2799957), (6, 0.08000047), (7, 0.080000475), (8, 0.080000475), (9, 0.08000049)]\n",
      "296: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "297: [(0, 0.08000094), (1, 0.17999555), (2, 0.08000109), (3, 0.080000915), (4, 0.08000094), (5, 0.08000101), (6, 0.080000944), (7, 0.1799967), (8, 0.08000096), (9, 0.08000098)]\n",
      "298: [(0, 0.08888905), (1, 0.08888905), (2, 0.08888907), (3, 0.08888904), (4, 0.08888905), (5, 0.19999856), (6, 0.08888905), (7, 0.08888905), (8, 0.088889055), (9, 0.088889055)]\n",
      "299: [(0, 0.08888932), (1, 0.08888932), (2, 0.08888939), (3, 0.19999601), (4, 0.08888932), (5, 0.08888935), (6, 0.08888932), (7, 0.08888933), (8, 0.08888933), (9, 0.088889346)]\n",
      "300: [(0, 0.08888911), (1, 0.08888911), (2, 0.088889144), (3, 0.0888891), (4, 0.08888911), (5, 0.08888913), (6, 0.08888911), (7, 0.088889115), (8, 0.19999793), (9, 0.08888912)]\n",
      "301: [(0, 0.08888934), (1, 0.1999958), (2, 0.08888942), (3, 0.08888933), (4, 0.08888934), (5, 0.08888938), (6, 0.088889346), (7, 0.08888935), (8, 0.08888935), (9, 0.08888936)]\n",
      "302: [(0, 0.11249804), (1, 0.050000656), (2, 0.050000753), (3, 0.050000634), (4, 0.11250063), (5, 0.17499985), (6, 0.11249988), (7, 0.050000664), (8, 0.050000668), (9, 0.23749821)]\n",
      "303: [(0, 0.15000032), (1, 0.066667765), (2, 0.15000263), (3, 0.06666773), (4, 0.06666776), (5, 0.06666783), (6, 0.06666776), (7, 0.06666777), (8, 0.23332265), (9, 0.0666678)]\n",
      "304: [(0, 0.08888905), (1, 0.08888905), (2, 0.08888907), (3, 0.08888904), (4, 0.08888905), (5, 0.08888906), (6, 0.19999853), (7, 0.088889055), (8, 0.088889055), (9, 0.088889055)]\n",
      "305: [(0, 0.0666673), (1, 0.1500005), (2, 0.14999847), (3, 0.15000048), (4, 0.0666673), (5, 0.06666735), (6, 0.14999662), (7, 0.06666731), (8, 0.06666732), (9, 0.066667326)]\n",
      "306: [(0, 0.08888897), (1, 0.19999918), (2, 0.08888899), (3, 0.08888897), (4, 0.08888897), (5, 0.08888898), (6, 0.08888897), (7, 0.08888897), (8, 0.08888897), (9, 0.08888897)]\n",
      "307: [(0, 0.088889), (1, 0.088889), (2, 0.08888902), (3, 0.19999892), (4, 0.088889), (5, 0.08888901), (6, 0.088889), (7, 0.088889), (8, 0.088889), (9, 0.088889)]\n",
      "308: [(0, 0.07272784), (1, 0.16363595), (2, 0.07272792), (3, 0.07272782), (4, 0.16363323), (5, 0.07272788), (6, 0.16363584), (7, 0.072727844), (8, 0.072727844), (9, 0.07272786)]\n",
      "309: [(0, 0.08888932), (1, 0.08888933), (2, 0.0888894), (3, 0.08888931), (4, 0.08888932), (5, 0.08888935), (6, 0.08888932), (7, 0.08888933), (8, 0.19999593), (9, 0.088889346)]\n",
      "310: [(0, 0.13845232), (1, 0.061539765), (2, 0.06153997), (3, 0.06153972), (4, 0.061539758), (5, 0.061539855), (6, 0.21538053), (7, 0.061539784), (8, 0.21538846), (9, 0.061539814)]\n",
      "311: [(0, 0.119998164), (1, 0.11999837), (2, 0.053334646), (3, 0.053334437), (4, 0.053334463), (5, 0.11999993), (6, 0.119997494), (7, 0.05333449), (8, 0.053334497), (9, 0.2533335)]\n",
      "312: [(0, 0.35789263), (1, 0.04210554), (2, 0.042105578), (3, 0.09473598), (4, 0.25263253), (5, 0.04210556), (6, 0.04210554), (7, 0.04210554), (8, 0.04210554), (9, 0.042105548)]\n",
      "313: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "314: [(0, 0.17999989), (1, 0.17999685), (2, 0.08000045), (3, 0.08000038), (4, 0.08000039), (5, 0.08000042), (6, 0.08000039), (7, 0.08000039), (8, 0.0800004), (9, 0.08000041)]\n",
      "315: [(0, 0.08888905), (1, 0.08888905), (2, 0.08888907), (3, 0.08888904), (4, 0.08888905), (5, 0.08888906), (6, 0.19999851), (7, 0.088889055), (8, 0.088889055), (9, 0.088889055)]\n",
      "316: [(0, 0.1999967), (1, 0.08888924), (2, 0.08888929), (3, 0.08888923), (4, 0.088889234), (5, 0.08888926), (6, 0.08888924), (7, 0.08888924), (8, 0.08888925), (9, 0.088889256)]\n",
      "317: [(0, 0.053333875), (1, 0.053333882), (2, 0.053333964), (3, 0.053333864), (4, 0.253332), (5, 0.05333392), (6, 0.2533305), (7, 0.12000022), (8, 0.053333893), (9, 0.0533339)]\n",
      "318: [(0, 0.08888895), (1, 0.08888895), (2, 0.1999995), (3, 0.08888895), (4, 0.08888895), (5, 0.08888895), (6, 0.08888895), (7, 0.08888895), (8, 0.08888895), (9, 0.08888895)]\n",
      "319: [(0, 0.08000039), (1, 0.17999673), (2, 0.18000016), (3, 0.08000038), (4, 0.08000039), (5, 0.08000042), (6, 0.08000039), (7, 0.08000039), (8, 0.08000039), (9, 0.08000041)]\n",
      "320: [(0, 0.08888916), (1, 0.08888916), (2, 0.19999754), (3, 0.088889144), (4, 0.08888916), (5, 0.088889174), (6, 0.08888916), (7, 0.08888916), (8, 0.08888917), (9, 0.08888917)]\n",
      "321: [(0, 0.10269897), (1, 0.048647482), (2, 0.12973496), (3, 0.1837868), (4, 0.12972963), (5, 0.021622514), (6, 0.048645247), (7, 0.1837862), (8, 0.102703795), (9, 0.048644397)]\n",
      "322: [(0, 0.16470194), (1, 0.047059663), (2, 0.04705979), (3, 0.047059637), (4, 0.40000272), (5, 0.10587751), (6, 0.047059663), (7, 0.047059674), (8, 0.04705968), (9, 0.047059696)]\n",
      "323: [(0, 0.08000037), (1, 0.08000038), (2, 0.08000043), (3, 0.080000356), (4, 0.18000017), (5, 0.0800004), (6, 0.08000037), (7, 0.17999679), (8, 0.08000038), (9, 0.080000386)]\n",
      "324: [(0, 0.11999704), (1, 0.18666747), (2, 0.05333451), (3, 0.05333432), (4, 0.11999805), (5, 0.053334422), (6, 0.053334355), (7, 0.18666695), (8, 0.053334378), (9, 0.1199985)]\n",
      "325: [(0, 0.12173805), (1, 0.03478381), (2, 0.07826377), (3, 0.12173718), (4, 0.2087004), (5, 0.078260034), (6, 0.078259036), (7, 0.12173861), (8, 0.07825944), (9, 0.07825963)]\n",
      "326: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "327: [(0, 0.061540823), (1, 0.061540846), (2, 0.061541207), (3, 0.061540764), (4, 0.36920834), (5, 0.061541006), (6, 0.061540846), (7, 0.06154087), (8, 0.061540887), (9, 0.13846444)]\n",
      "328: [(0, 0.057143204), (1, 0.057143204), (2, 0.12857197), (3, 0.12857147), (4, 0.057143204), (5, 0.05714323), (6, 0.057143204), (7, 0.057143208), (8, 0.05714321), (9, 0.3428541)]\n",
      "329: [(0, 0.07272763), (1, 0.07272763), (2, 0.16363697), (3, 0.16363646), (4, 0.07272763), (5, 0.07272766), (6, 0.07272763), (7, 0.072727636), (8, 0.16363312), (9, 0.07272764)]\n",
      "330: [(0, 0.08888932), (1, 0.08888932), (2, 0.0888894), (3, 0.08888931), (4, 0.08888932), (5, 0.08888935), (6, 0.08888932), (7, 0.08888933), (8, 0.19999596), (9, 0.088889346)]\n",
      "331: [(0, 0.14999749), (1, 0.06666761), (2, 0.06666775), (3, 0.14999492), (4, 0.0666676), (5, 0.23333414), (6, 0.06666761), (7, 0.06666762), (8, 0.066667624), (9, 0.066667646)]\n",
      "332: [(0, 0.19999827), (1, 0.08888908), (2, 0.088889115), (3, 0.08888908), (4, 0.08888908), (5, 0.08888909), (6, 0.08888908), (7, 0.088889085), (8, 0.088889085), (9, 0.088889085)]\n",
      "333: [(0, 0.12857383), (1, 0.057144288), (2, 0.057144508), (3, 0.057144243), (4, 0.05714428), (5, 0.12857106), (6, 0.057144288), (7, 0.12857148), (8, 0.19998895), (9, 0.12857306)]\n",
      "334: [(0, 0.078261085), (1, 0.034783356), (2, 0.034783468), (3, 0.12173542), (4, 0.20869538), (5, 0.034783404), (6, 0.16521738), (7, 0.25217378), (8, 0.034783367), (9, 0.034783382)]\n",
      "335: [(0, 0.08000052), (1, 0.08000053), (2, 0.0800006), (3, 0.080000505), (4, 0.18000038), (5, 0.17999534), (6, 0.08000053), (7, 0.080000535), (8, 0.080000535), (9, 0.08000054)]\n",
      "336: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "337: [(0, 0.080000155), (1, 0.08000016), (2, 0.17999998), (3, 0.080000155), (4, 0.08000016), (5, 0.08000017), (6, 0.17999874), (7, 0.08000016), (8, 0.08000016), (9, 0.08000016)]\n",
      "338: [(0, 0.088889), (1, 0.088889), (2, 0.08888902), (3, 0.19999892), (4, 0.088889), (5, 0.08888901), (6, 0.088889), (7, 0.088889), (8, 0.088889), (9, 0.088889)]\n",
      "339: [(0, 0.088889144), (1, 0.088889144), (2, 0.08888918), (3, 0.08888914), (4, 0.088889144), (5, 0.08888916), (6, 0.088889144), (7, 0.088889144), (8, 0.088889144), (9, 0.19999768)]\n",
      "340: [(0, 0.072727896), (1, 0.0727279), (2, 0.072728), (3, 0.07272788), (4, 0.16363576), (5, 0.16363396), (6, 0.0727279), (7, 0.07272791), (8, 0.07272791), (9, 0.16363491)]\n",
      "341: [(0, 0.0571434), (1, 0.057143405), (2, 0.12857237), (3, 0.057143383), (4, 0.3428566), (5, 0.05714344), (6, 0.057143405), (7, 0.05714341), (8, 0.057143413), (9, 0.12856714)]\n",
      "342: [(0, 0.16363516), (1, 0.07272751), (2, 0.072727546), (3, 0.0727275), (4, 0.25454468), (5, 0.072727524), (6, 0.07272751), (7, 0.07272751), (8, 0.07272751), (9, 0.07272752)]\n",
      "343: [(0, 0.1124955), (1, 0.050000474), (2, 0.05000055), (3, 0.36250073), (4, 0.17500035), (5, 0.050000507), (6, 0.050000474), (7, 0.05000048), (8, 0.050000485), (9, 0.050000492)]\n",
      "344: [(0, 0.08888929), (1, 0.0888893), (2, 0.08888936), (3, 0.088889286), (4, 0.08888929), (5, 0.08888933), (6, 0.08888929), (7, 0.0888893), (8, 0.19999622), (9, 0.088889316)]\n",
      "345: [(0, 0.17999639), (1, 0.08000062), (2, 0.080000706), (3, 0.080000594), (4, 0.08000062), (5, 0.080000654), (6, 0.17999858), (7, 0.080000624), (8, 0.080000624), (9, 0.08000064)]\n",
      "346: [(0, 0.08000082), (1, 0.08000082), (2, 0.080000944), (3, 0.08000079), (4, 0.17999515), (5, 0.08000088), (6, 0.08000082), (7, 0.08000083), (8, 0.17999807), (9, 0.08000085)]\n",
      "347: [(0, 0.07272774), (1, 0.07272774), (2, 0.072727814), (3, 0.072727725), (4, 0.2545438), (5, 0.07272778), (6, 0.07272774), (7, 0.07272775), (8, 0.07272775), (9, 0.16363415)]\n",
      "348: [(0, 0.05333434), (1, 0.053334348), (2, 0.0533345), (3, 0.25333613), (4, 0.05333434), (5, 0.119996145), (6, 0.053334348), (7, 0.119996116), (8, 0.11999882), (9, 0.120000884)]\n",
      "349: [(0, 0.089995526), (1, 0.040000886), (2, 0.19000511), (3, 0.28778094), (4, 0.040000882), (5, 0.13999954), (6, 0.040000886), (7, 0.0899986), (8, 0.04221674), (9, 0.04000092)]\n",
      "350: [(0, 0.23158471), (1, 0.12631194), (2, 0.02105361), (3, 0.061537027), (4, 0.0736824), (5, 0.12631772), (6, 0.07368604), (7, 0.12631683), (8, 0.08582638), (9, 0.07368336)]\n",
      "351: [(0, 0.06153934), (1, 0.13845839), (2, 0.061539482), (3, 0.06153932), (4, 0.13846278), (5, 0.061539404), (6, 0.13846064), (7, 0.06153936), (8, 0.061539363), (9, 0.21538195)]\n",
      "352: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "353: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "354: [(0, 0.08888906), (1, 0.08888906), (2, 0.088889085), (3, 0.088889055), (4, 0.19999838), (5, 0.08888908), (6, 0.08888906), (7, 0.08888906), (8, 0.08888906), (9, 0.08888907)]\n",
      "355: [(0, 0.13846384), (1, 0.13845891), (2, 0.061540056), (3, 0.13845398), (4, 0.13846403), (5, 0.061539937), (6, 0.061539844), (7, 0.061539862), (8, 0.061539866), (9, 0.13845967)]\n",
      "356: [(0, 0.13846213), (1, 0.06153916), (2, 0.061539266), (3, 0.061539132), (4, 0.06153915), (5, 0.061539207), (6, 0.06153916), (7, 0.29230633), (8, 0.06153917), (9, 0.13845733)]\n",
      "357: [(0, 0.08888945), (1, 0.08888946), (2, 0.08888955), (3, 0.19999476), (4, 0.08888945), (5, 0.088889495), (6, 0.08888946), (7, 0.088889465), (8, 0.088889465), (9, 0.08888948)]\n",
      "358: [(0, 0.13845882), (1, 0.06153939), (2, 0.13846327), (3, 0.061539356), (4, 0.06153938), (5, 0.13845976), (6, 0.061539385), (7, 0.13846274), (8, 0.13845846), (9, 0.061539423)]\n",
      "359: [(0, 0.044445496), (1, 0.21110693), (2, 0.09999863), (3, 0.044445466), (4, 0.044445496), (5, 0.044445578), (6, 0.10000133), (7, 0.15555488), (8, 0.21111067), (9, 0.044445544)]\n",
      "360: [(0, 0.23333232), (1, 0.14999759), (2, 0.0666678), (3, 0.1499962), (4, 0.066667646), (5, 0.06666772), (6, 0.06666765), (7, 0.06666766), (8, 0.066667676), (9, 0.06666769)]\n",
      "361: [(0, 0.08888926), (1, 0.08888926), (2, 0.08888932), (3, 0.08888925), (4, 0.08888926), (5, 0.088889286), (6, 0.08888926), (7, 0.19999652), (8, 0.08888927), (9, 0.08888928)]\n",
      "362: [(0, 0.072727606), (1, 0.072727606), (2, 0.07272766), (3, 0.07272759), (4, 0.34545144), (5, 0.07272763), (6, 0.072727606), (7, 0.072727606), (8, 0.072727606), (9, 0.07272761)]\n",
      "363: [(0, 0.17499939), (1, 0.050000325), (2, 0.050000373), (3, 0.050000314), (4, 0.05000032), (5, 0.42499796), (6, 0.050000325), (7, 0.05000033), (8, 0.05000033), (9, 0.050000336)]\n",
      "364: [(0, 0.0727282), (1, 0.16363169), (2, 0.07272834), (3, 0.07272817), (4, 0.0727282), (5, 0.07272827), (6, 0.16363396), (7, 0.07272822), (8, 0.072728224), (9, 0.16363676)]\n",
      "365: [(0, 0.08888938), (1, 0.19999538), (2, 0.088889465), (3, 0.08888937), (4, 0.08888938), (5, 0.08888942), (6, 0.08888938), (7, 0.08888939), (8, 0.0888894), (9, 0.088889405)]\n",
      "366: [(0, 0.08000035), (1, 0.27999675), (2, 0.08000041), (3, 0.08000034), (4, 0.08000035), (5, 0.08000038), (6, 0.080000356), (7, 0.080000356), (8, 0.080000356), (9, 0.08000036)]\n",
      "367: [(0, 0.15000163), (1, 0.14999504), (2, 0.066667944), (3, 0.066667736), (4, 0.066667765), (5, 0.14999886), (6, 0.06666777), (7, 0.14999767), (8, 0.066667795), (9, 0.06666782)]\n",
      "368: [(0, 0.036364388), (1, 0.08181894), (2, 0.036364507), (3, 0.2636393), (4, 0.17272902), (5, 0.081819065), (6, 0.1272659), (7, 0.12727001), (8, 0.036364406), (9, 0.03636442)]\n",
      "369: [(0, 0.062067453), (1, 0.09654399), (2, 0.027587466), (3, 0.09655495), (4, 0.096548416), (5, 0.30345756), (6, 0.027587302), (7, 0.062064707), (8, 0.16552074), (9, 0.062067453)]\n",
      "370: [(0, 0.08000079), (1, 0.080000795), (2, 0.080000915), (3, 0.080000766), (4, 0.17999534), (5, 0.08000085), (6, 0.080000795), (7, 0.0800008), (8, 0.0800008), (9, 0.17999814)]\n",
      "371: [(0, 0.13845171), (1, 0.13846312), (2, 0.061539695), (3, 0.061539494), (4, 0.061539523), (5, 0.061539605), (6, 0.1384622), (7, 0.061539546), (8, 0.06153955), (9, 0.21538559)]\n",
      "372: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "373: [(0, 0.08888937), (1, 0.088889375), (2, 0.08888944), (3, 0.08888935), (4, 0.08888937), (5, 0.088889405), (6, 0.088889375), (7, 0.08888938), (8, 0.19999558), (9, 0.08888938)]\n",
      "374: [(0, 0.07272833), (1, 0.16363329), (2, 0.0727285), (3, 0.24435104), (4, 0.07272833), (5, 0.07272841), (6, 0.072728336), (7, 0.07272835), (8, 0.08291703), (9, 0.07272838)]\n",
      "375: [(0, 0.08888895), (1, 0.08888895), (2, 0.1999995), (3, 0.08888895), (4, 0.08888895), (5, 0.08888895), (6, 0.08888895), (7, 0.08888895), (8, 0.08888895), (9, 0.08888895)]\n",
      "376: [(0, 0.08888895), (1, 0.08888895), (2, 0.08888896), (3, 0.19999942), (4, 0.08888895), (5, 0.08888896), (6, 0.08888895), (7, 0.08888895), (8, 0.08888895), (9, 0.08888895)]\n",
      "377: [(0, 0.21538171), (1, 0.061539356), (2, 0.061539486), (3, 0.061539322), (4, 0.13845675), (5, 0.061539415), (6, 0.13846253), (7, 0.13846269), (8, 0.061539367), (9, 0.061539385)]\n",
      "378: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "379: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "380: [(0, 0.072728954), (1, 0.16363339), (2, 0.07272923), (3, 0.07272891), (4, 0.072728954), (5, 0.25453457), (6, 0.07272897), (7, 0.07272899), (8, 0.072729), (9, 0.07272903)]\n",
      "381: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "382: [(0, 0.13999745), (1, 0.34000105), (2, 0.04000116), (3, 0.04000097), (4, 0.040001), (5, 0.040001076), (6, 0.13999757), (7, 0.13999763), (8, 0.040001027), (9, 0.040001046)]\n",
      "383: [(0, 0.088888995), (1, 0.088889), (2, 0.08888902), (3, 0.088888995), (4, 0.088889), (5, 0.08888901), (6, 0.088889), (7, 0.088889), (8, 0.19999897), (9, 0.088889)]\n",
      "384: [(0, 0.088889286), (1, 0.19999632), (2, 0.088889346), (3, 0.08888927), (4, 0.088889286), (5, 0.088889316), (6, 0.088889286), (7, 0.08888929), (8, 0.08888929), (9, 0.0888893)]\n",
      "385: [(0, 0.07272867), (1, 0.16363387), (2, 0.072728895), (3, 0.16363873), (4, 0.07272868), (5, 0.072728775), (6, 0.16362622), (7, 0.0727287), (8, 0.07272871), (9, 0.07272874)]\n",
      "386: [(0, 0.07272764), (1, 0.07272764), (2, 0.0727277), (3, 0.07272763), (4, 0.07272764), (5, 0.16363473), (6, 0.07272764), (7, 0.07272765), (8, 0.07272765), (9, 0.2545441)]\n",
      "387: [(0, 0.08888897), (1, 0.19999918), (2, 0.08888899), (3, 0.08888897), (4, 0.08888897), (5, 0.08888898), (6, 0.08888897), (7, 0.08888897), (8, 0.08888897), (9, 0.08888897)]\n",
      "388: [(0, 0.022857932), (1, 0.10856832), (2, 0.022858057), (3, 0.19428457), (4, 0.25143608), (5, 0.022857992), (6, 0.16571295), (7, 0.02285795), (8, 0.1371417), (9, 0.051424455)]\n",
      "389: [(0, 0.08888895), (1, 0.08888895), (2, 0.1999995), (3, 0.08888895), (4, 0.08888895), (5, 0.08888895), (6, 0.08888895), (7, 0.08888895), (8, 0.08888895), (9, 0.08888895)]\n",
      "390: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "391: [(0, 0.06666738), (1, 0.066667385), (2, 0.06666749), (3, 0.066667356), (4, 0.06666738), (5, 0.23333044), (6, 0.066667385), (7, 0.06666739), (8, 0.0666674), (9, 0.23333041)]\n",
      "392: [(0, 0.088889115), (1, 0.08888912), (2, 0.08888915), (3, 0.08888911), (4, 0.088889115), (5, 0.19999792), (6, 0.088889115), (7, 0.08888912), (8, 0.08888912), (9, 0.08888913)]\n",
      "393: [(0, 0.17999771), (1, 0.08000035), (2, 0.08000039), (3, 0.17999944), (4, 0.08000034), (5, 0.08000036), (6, 0.08000034), (7, 0.08000035), (8, 0.08000035), (9, 0.080000356)]\n",
      "394: [(0, 0.07272763), (1, 0.07272763), (2, 0.07272768), (3, 0.07272761), (4, 0.07272763), (5, 0.07272766), (6, 0.3454512), (7, 0.072727636), (8, 0.072727636), (9, 0.07272764)]\n",
      "395: [(0, 0.05714357), (1, 0.057143573), (2, 0.12857307), (3, 0.12857215), (4, 0.05714357), (5, 0.19999816), (6, 0.057143573), (7, 0.12856728), (8, 0.057143584), (9, 0.12857148)]\n",
      "396: [(0, 0.080000825), (1, 0.080000825), (2, 0.08000096), (3, 0.080000795), (4, 0.080000825), (5, 0.080000885), (6, 0.080000825), (7, 0.08000084), (8, 0.17999676), (9, 0.17999642)]\n",
      "397: [(0, 0.03478304), (1, 0.034783043), (2, 0.034783106), (3, 0.20869614), (4, 0.03478304), (5, 0.034783073), (6, 0.4260891), (7, 0.078260764), (8, 0.07825562), (9, 0.034783054)]\n",
      "398: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "399: [(0, 0.061539404), (1, 0.061539415), (2, 0.138461), (3, 0.06153938), (4, 0.13845673), (5, 0.06153948), (6, 0.29230627), (7, 0.061539423), (8, 0.061539426), (9, 0.061539445)]\n",
      "400: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "401: [(0, 0.08000032), (1, 0.080000326), (2, 0.08000037), (3, 0.27999702), (4, 0.08000032), (5, 0.08000034), (6, 0.080000326), (7, 0.080000326), (8, 0.08000033), (9, 0.08000033)]\n",
      "402: [(0, 0.18666351), (1, 0.053334314), (2, 0.05333446), (3, 0.18666448), (4, 0.120000236), (5, 0.053334378), (6, 0.1200012), (7, 0.11999872), (8, 0.05333433), (9, 0.05333435)]\n",
      "403: [(0, 0.08888906), (1, 0.08888906), (2, 0.088889085), (3, 0.088889055), (4, 0.19999838), (5, 0.08888908), (6, 0.08888906), (7, 0.08888906), (8, 0.08888906), (9, 0.08888907)]\n",
      "404: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "405: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "406: [(0, 0.17999834), (1, 0.0800006), (2, 0.0800007), (3, 0.08000059), (4, 0.0800006), (5, 0.08000065), (6, 0.0800006), (7, 0.08000061), (8, 0.17999662), (9, 0.08000063)]\n",
      "407: [(0, 0.08000034), (1, 0.08000034), (2, 0.08000039), (3, 0.080000326), (4, 0.1800001), (5, 0.08000036), (6, 0.08000034), (7, 0.08000035), (8, 0.08000035), (9, 0.17999712)]\n",
      "408: [(0, 0.08888897), (1, 0.08888897), (2, 0.19999923), (3, 0.08888897), (4, 0.08888897), (5, 0.08888898), (6, 0.08888897), (7, 0.08888897), (8, 0.08888897), (9, 0.08888897)]\n",
      "409: [(0, 0.057143558), (1, 0.2714252), (2, 0.12857287), (3, 0.12857234), (4, 0.057143558), (5, 0.057143614), (6, 0.12856813), (7, 0.057143576), (8, 0.05714358), (9, 0.057143595)]\n",
      "410: [(0, 0.12727019), (1, 0.1727257), (2, 0.036365166), (3, 0.08182037), (4, 0.17272748), (5, 0.036365055), (6, 0.17273039), (7, 0.036364984), (8, 0.0818108), (9, 0.08181983)]\n",
      "411: [(0, 0.16363585), (1, 0.07272767), (2, 0.07272773), (3, 0.07272766), (4, 0.07272767), (5, 0.0727277), (6, 0.07272767), (7, 0.07272768), (8, 0.2545427), (9, 0.072727695)]\n",
      "412: [(0, 0.12856722), (1, 0.12856753), (2, 0.12857422), (3, 0.05714391), (4, 0.05714394), (5, 0.12857096), (6, 0.2000003), (7, 0.05714396), (8, 0.05714397), (9, 0.057143986)]\n",
      "413: [(0, 0.32381102), (1, 0.03809591), (2, 0.03809601), (3, 0.038095888), (4, 0.08571494), (5, 0.085711025), (6, 0.13333185), (7, 0.038095918), (8, 0.03809592), (9, 0.1809515)]\n",
      "414: [(0, 0.088889375), (1, 0.08888938), (2, 0.08888946), (3, 0.08888937), (4, 0.19999549), (5, 0.08888941), (6, 0.08888938), (7, 0.08888939), (8, 0.08888939), (9, 0.088889405)]\n",
      "415: [(0, 0.08000077), (1, 0.27999282), (2, 0.0800009), (3, 0.08000076), (4, 0.08000077), (5, 0.08000083), (6, 0.08000078), (7, 0.08000079), (8, 0.080000795), (9, 0.08000081)]\n",
      "416: [(0, 0.061539546), (1, 0.1384598), (2, 0.061539724), (3, 0.061539516), (4, 0.06153955), (5, 0.29229996), (6, 0.061539553), (7, 0.13846317), (8, 0.061539575), (9, 0.061539594)]\n",
      "417: [(0, 0.08888916), (1, 0.08888916), (2, 0.0888892), (3, 0.088889144), (4, 0.08888916), (5, 0.088889174), (6, 0.08888916), (7, 0.08888916), (8, 0.08888917), (9, 0.19999751)]\n",
      "418: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "419: [(0, 0.07272838), (1, 0.07272839), (2, 0.07272856), (3, 0.07272834), (4, 0.1636324), (5, 0.072728455), (6, 0.07272838), (7, 0.16363293), (8, 0.0727284), (9, 0.16363576)]\n",
      "420: [(0, 0.05000127), (1, 0.17500202), (2, 0.050001476), (3, 0.17499442), (4, 0.17500181), (5, 0.11249646), (6, 0.1124986), (7, 0.050001297), (8, 0.050001305), (9, 0.050001327)]\n",
      "421: [(0, 0.08000017), (1, 0.08000017), (2, 0.08000019), (3, 0.08000016), (4, 0.27999848), (5, 0.08000018), (6, 0.08000017), (7, 0.08000017), (8, 0.08000017), (9, 0.08000017)]\n",
      "422: [(0, 0.07272758), (1, 0.07272758), (2, 0.072727636), (3, 0.072727576), (4, 0.16363637), (5, 0.25454292), (6, 0.07272758), (7, 0.07272759), (8, 0.07272759), (9, 0.0727276)]\n",
      "423: [(0, 0.08000077), (1, 0.17999582), (2, 0.08000091), (3, 0.08000076), (4, 0.08000078), (5, 0.08000084), (6, 0.1799977), (7, 0.08000079), (8, 0.080000795), (9, 0.08000081)]\n",
      "424: [(0, 0.08000036), (1, 0.08000036), (2, 0.08000042), (3, 0.08000035), (4, 0.27999663), (5, 0.080000386), (6, 0.08000036), (7, 0.08000036), (8, 0.08000036), (9, 0.08000037)]\n",
      "425: [(0, 0.2545417), (1, 0.072727725), (2, 0.07272779), (3, 0.16363642), (4, 0.07272772), (5, 0.072727755), (6, 0.072727725), (7, 0.07272773), (8, 0.07272773), (9, 0.07272774)]\n",
      "426: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "427: [(0, 0.12857065), (1, 0.057143908), (2, 0.12857354), (3, 0.12856787), (4, 0.0571439), (5, 0.05714398), (6, 0.12856904), (7, 0.057143923), (8, 0.19999924), (9, 0.05714395)]\n",
      "428: [(0, 0.08000034), (1, 0.08000034), (2, 0.18000053), (3, 0.080000326), (4, 0.08000034), (5, 0.08000036), (6, 0.08000034), (7, 0.08000035), (8, 0.08000035), (9, 0.17999676)]\n",
      "429: [(0, 0.19999668), (1, 0.08888925), (2, 0.08888931), (3, 0.08888924), (4, 0.08888925), (5, 0.08888927), (6, 0.08888925), (7, 0.08888925), (8, 0.088889256), (9, 0.08888926)]\n",
      "430: [(0, 0.07272776), (1, 0.07272777), (2, 0.07272784), (3, 0.07272775), (4, 0.25454494), (5, 0.0727278), (6, 0.16363283), (7, 0.07272777), (8, 0.07272777), (9, 0.072727785)]\n",
      "431: [(0, 0.15555577), (1, 0.3222239), (2, 0.044445753), (3, 0.099995136), (4, 0.09999536), (5, 0.044445656), (6, 0.04444558), (7, 0.044445597), (8, 0.10000166), (9, 0.044445623)]\n",
      "432: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "433: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "434: [(0, 0.07272764), (1, 0.07272765), (2, 0.16363625), (3, 0.07272764), (4, 0.07272765), (5, 0.07272768), (6, 0.16363458), (7, 0.07272766), (8, 0.07272766), (9, 0.1636356)]\n",
      "435: [(0, 0.057143915), (1, 0.057143927), (2, 0.057144087), (3, 0.12856942), (4, 0.057143915), (5, 0.12857008), (6, 0.20000067), (7, 0.1285714), (8, 0.05714394), (9, 0.1285686)]\n",
      "436: [(0, 0.057143293), (1, 0.057143297), (2, 0.057143368), (3, 0.057143286), (4, 0.057143297), (5, 0.057143327), (6, 0.34285635), (7, 0.12857), (8, 0.05714331), (9, 0.12857045)]\n",
      "437: [(0, 0.06666752), (1, 0.06666753), (2, 0.06666766), (3, 0.23333278), (4, 0.14999512), (5, 0.06666759), (6, 0.14999914), (7, 0.06666754), (8, 0.06666754), (9, 0.06666756)]\n",
      "438: [(0, 0.08000054), (1, 0.17999722), (2, 0.08000063), (3, 0.08000053), (4, 0.17999828), (5, 0.08000059), (6, 0.08000055), (7, 0.08000056), (8, 0.08000056), (9, 0.080000564)]\n",
      "439: [(0, 0.11249798), (1, 0.11249875), (2, 0.050001282), (3, 0.050001077), (4, 0.050001107), (5, 0.05000119), (6, 0.050001115), (7, 0.30000383), (8, 0.11249711), (9, 0.112496585)]\n",
      "440: [(0, 0.04705989), (1, 0.16470532), (2, 0.04706006), (3, 0.04705986), (4, 0.16470537), (5, 0.16470607), (6, 0.047059897), (7, 0.047059912), (8, 0.16470449), (9, 0.105879135)]\n",
      "441: [(0, 0.07272751), (1, 0.16363588), (2, 0.072727546), (3, 0.0727275), (4, 0.25454402), (5, 0.072727524), (6, 0.07272751), (7, 0.07272751), (8, 0.07272751), (9, 0.07272752)]\n",
      "442: [(0, 0.057144042), (1, 0.12856923), (2, 0.12857158), (3, 0.12856899), (4, 0.12857342), (5, 0.12856995), (6, 0.057144053), (7, 0.12857057), (8, 0.057144072), (9, 0.057144094)]\n",
      "443: [(0, 0.06153939), (1, 0.13845757), (2, 0.061539542), (3, 0.061539367), (4, 0.13846166), (5, 0.13846073), (6, 0.0615394), (7, 0.13846014), (8, 0.13846274), (9, 0.061539434)]\n",
      "444: [(0, 0.06153902), (1, 0.13845725), (2, 0.06153911), (3, 0.36923042), (4, 0.06153902), (5, 0.061539065), (6, 0.061539024), (7, 0.06153903), (8, 0.06153904), (9, 0.061539046)]\n",
      "445: [(0, 0.08000074), (1, 0.08000076), (2, 0.08000087), (3, 0.17999753), (4, 0.08000075), (5, 0.0800008), (6, 0.08000075), (7, 0.08000076), (8, 0.080000766), (9, 0.17999627)]\n",
      "446: [(0, 0.06153943), (1, 0.06153944), (2, 0.061539587), (3, 0.061539408), (4, 0.06153943), (5, 0.061539505), (6, 0.13845734), (7, 0.13845758), (8, 0.06153946), (9, 0.2923088)]\n",
      "447: [(0, 0.08000006), (1, 0.08000006), (2, 0.08000007), (3, 0.2799994), (4, 0.08000006), (5, 0.080000065), (6, 0.08000006), (7, 0.08000006), (8, 0.080000065), (9, 0.080000065)]\n",
      "448: [(0, 0.08000046), (1, 0.08000047), (2, 0.08000054), (3, 0.08000045), (4, 0.08000046), (5, 0.0800005), (6, 0.08000047), (7, 0.080000475), (8, 0.2799957), (9, 0.08000049)]\n",
      "449: [(0, 0.08888908), (1, 0.08888908), (2, 0.088889115), (3, 0.08888908), (4, 0.19999826), (5, 0.08888909), (6, 0.08888908), (7, 0.088889085), (8, 0.088889085), (9, 0.088889085)]\n",
      "450: [(0, 0.088889204), (1, 0.08888921), (2, 0.088889256), (3, 0.0888892), (4, 0.19999702), (5, 0.08888923), (6, 0.08888921), (7, 0.08888921), (8, 0.08888921), (9, 0.08888921)]\n",
      "451: [(0, 0.2333299), (1, 0.066667385), (2, 0.066667505), (3, 0.06666736), (4, 0.066667385), (5, 0.06666744), (6, 0.066667385), (7, 0.0666674), (8, 0.0666674), (9, 0.2333308)]\n",
      "452: [(0, 0.057143793), (1, 0.057143804), (2, 0.05714394), (3, 0.19999783), (4, 0.057143793), (5, 0.057143863), (6, 0.19999923), (7, 0.1285681), (8, 0.05714382), (9, 0.1285718)]\n",
      "453: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "454: [(0, 0.08888905), (1, 0.08888905), (2, 0.08888907), (3, 0.08888904), (4, 0.08888905), (5, 0.19999856), (6, 0.08888905), (7, 0.08888905), (8, 0.088889055), (9, 0.088889055)]\n",
      "455: [(0, 0.13999921), (1, 0.13999744), (2, 0.040001117), (3, 0.040000938), (4, 0.1400005), (5, 0.19000015), (6, 0.09000109), (7, 0.040000983), (8, 0.08999964), (9, 0.08999893)]\n",
      "456: [(0, 0.044445097), (1, 0.044445105), (2, 0.044445205), (3, 0.044445083), (4, 0.21111102), (5, 0.09999973), (6, 0.1555567), (7, 0.09999677), (8, 0.044445116), (9, 0.21111013)]\n",
      "457: [(0, 0.19999619), (1, 0.042106047), (2, 0.042106166), (3, 0.04210602), (4, 0.14736676), (5, 0.0421061), (6, 0.35789454), (7, 0.04210606), (8, 0.042106062), (9, 0.042106077)]\n",
      "458: [(0, 0.08000029), (1, 0.080000296), (2, 0.08000034), (3, 0.1799998), (4, 0.08000029), (5, 0.08000031), (6, 0.080000296), (7, 0.080000296), (8, 0.0800003), (9, 0.17999776)]\n",
      "459: [(0, 0.08000077), (1, 0.08000078), (2, 0.0800009), (3, 0.08000076), (4, 0.17999767), (5, 0.08000083), (6, 0.08000078), (7, 0.17999595), (8, 0.080000795), (9, 0.08000081)]\n",
      "460: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "461: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "462: [(0, 0.03200053), (1, 0.032000534), (2, 0.11200168), (3, 0.189781), (4, 0.27200198), (5, 0.032000568), (6, 0.11199713), (7, 0.032000538), (8, 0.034216654), (9, 0.1519994)]\n",
      "463: [(0, 0.06153932), (1, 0.13845862), (2, 0.13846366), (3, 0.13845785), (4, 0.06153932), (5, 0.06153938), (6, 0.061539322), (7, 0.13846119), (8, 0.06153934), (9, 0.138462)]\n",
      "464: [(0, 0.057143923), (1, 0.12857065), (2, 0.12857023), (3, 0.057143897), (4, 0.12857196), (5, 0.057144005), (6, 0.057143934), (7, 0.1285693), (8, 0.19999814), (9, 0.05714397)]\n",
      "465: [(0, 0.08000041), (1, 0.08000041), (2, 0.18000074), (3, 0.08000039), (4, 0.08000041), (5, 0.08000044), (6, 0.08000041), (7, 0.080000415), (8, 0.17999597), (9, 0.08000042)]\n",
      "466: [(0, 0.088889204), (1, 0.088889204), (2, 0.088889256), (3, 0.0888892), (4, 0.088889204), (5, 0.08888923), (6, 0.088889204), (7, 0.19999708), (8, 0.08888921), (9, 0.08888921)]\n",
      "467: [(0, 0.08000052), (1, 0.08000053), (2, 0.08000061), (3, 0.080000505), (4, 0.08000052), (5, 0.17999819), (6, 0.08000053), (7, 0.17999752), (8, 0.080000535), (9, 0.08000054)]\n",
      "468: [(0, 0.08888899), (1, 0.08888899), (2, 0.088888995), (3, 0.08888899), (4, 0.08888899), (5, 0.088888995), (6, 0.08888899), (7, 0.08888899), (8, 0.19999911), (9, 0.088888995)]\n",
      "469: [(0, 0.088889115), (1, 0.08888912), (2, 0.08888915), (3, 0.08888911), (4, 0.088889115), (5, 0.08888914), (6, 0.19999789), (7, 0.08888912), (8, 0.08888912), (9, 0.08888913)]\n",
      "470: [(0, 0.057143714), (1, 0.05714372), (2, 0.057143852), (3, 0.12857272), (4, 0.2714283), (5, 0.057143778), (6, 0.057143718), (7, 0.12856801), (8, 0.12856847), (9, 0.05714375)]\n",
      "471: [(0, 0.08888926), (1, 0.08888926), (2, 0.08888932), (3, 0.08888925), (4, 0.19999653), (5, 0.088889286), (6, 0.08888926), (7, 0.08888927), (8, 0.08888927), (9, 0.08888928)]\n",
      "472: [(0, 0.050000917), (1, 0.11249927), (2, 0.112499), (3, 0.050000895), (4, 0.050000917), (5, 0.050000988), (6, 0.1750008), (7, 0.2374986), (8, 0.112497665), (9, 0.05000096)]\n",
      "473: [(0, 0.053333934), (1, 0.05333394), (2, 0.053334035), (3, 0.12000066), (4, 0.053333938), (5, 0.119998485), (6, 0.25333363), (7, 0.053333953), (8, 0.053333953), (9, 0.1866635)]\n",
      "474: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "475: [(0, 0.05333413), (1, 0.25332817), (2, 0.05333426), (3, 0.120000415), (4, 0.1200012), (5, 0.18666525), (6, 0.053334136), (7, 0.053334147), (8, 0.05333415), (9, 0.053334165)]\n",
      "476: [(0, 0.23332918), (1, 0.066667646), (2, 0.066667795), (3, 0.06666762), (4, 0.06666764), (5, 0.06666771), (6, 0.15000081), (7, 0.14999628), (8, 0.06666766), (9, 0.06666768)]\n",
      "477: [(0, 0.08000074), (1, 0.08000075), (2, 0.08000086), (3, 0.17999321), (4, 0.08000074), (5, 0.0800008), (6, 0.08000075), (7, 0.08000076), (8, 0.18000062), (9, 0.08000078)]\n",
      "478: [(0, 0.072727606), (1, 0.07272761), (2, 0.072727665), (3, 0.072727606), (4, 0.072727606), (5, 0.072727636), (6, 0.16363557), (7, 0.07272762), (8, 0.07272762), (9, 0.2545434)]\n",
      "479: [(0, 0.088889234), (1, 0.088889234), (2, 0.088889286), (3, 0.08888923), (4, 0.088889234), (5, 0.08888926), (6, 0.088889234), (7, 0.19999681), (8, 0.08888924), (9, 0.08888925)]\n",
      "480: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "481: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "482: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "483: [(0, 0.08888897), (1, 0.08888897), (2, 0.19999923), (3, 0.08888897), (4, 0.08888897), (5, 0.08888898), (6, 0.08888897), (7, 0.08888897), (8, 0.08888897), (9, 0.08888897)]\n",
      "484: [(0, 0.080000855), (1, 0.080000855), (2, 0.08000099), (3, 0.080000825), (4, 0.080000855), (5, 0.080000915), (6, 0.080000855), (7, 0.08000087), (8, 0.17999667), (9, 0.17999634)]\n",
      "485: [(0, 0.08000041), (1, 0.08000041), (2, 0.08000047), (3, 0.2799962), (4, 0.08000041), (5, 0.08000044), (6, 0.08000041), (7, 0.080000415), (8, 0.080000415), (9, 0.08000042)]\n",
      "486: [(0, 0.088889), (1, 0.088889), (2, 0.08888902), (3, 0.19999892), (4, 0.088889), (5, 0.08888901), (6, 0.088889), (7, 0.088889), (8, 0.088889), (9, 0.088889)]\n",
      "487: [(0, 0.08000028), (1, 0.08000028), (2, 0.080000326), (3, 0.080000274), (4, 0.08000028), (5, 0.0800003), (6, 0.27999735), (7, 0.08000029), (8, 0.08000029), (9, 0.080000296)]\n",
      "488: [(0, 0.088889234), (1, 0.08888924), (2, 0.08888929), (3, 0.08888923), (4, 0.19999677), (5, 0.08888926), (6, 0.08888924), (7, 0.08888924), (8, 0.08888925), (9, 0.08888925)]\n",
      "489: [(0, 0.08888905), (1, 0.08888905), (2, 0.08888907), (3, 0.08888904), (4, 0.08888905), (5, 0.08888906), (6, 0.19999853), (7, 0.088889055), (8, 0.088889055), (9, 0.088889055)]\n",
      "490: [(0, 0.08888897), (1, 0.08888897), (2, 0.19999923), (3, 0.08888897), (4, 0.08888897), (5, 0.08888898), (6, 0.08888897), (7, 0.08888897), (8, 0.08888897), (9, 0.08888897)]\n",
      "491: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "492: [(0, 0.1999967), (1, 0.08888925), (2, 0.08888931), (3, 0.08888924), (4, 0.08888925), (5, 0.08888927), (6, 0.08888925), (7, 0.08888925), (8, 0.088889256), (9, 0.08888926)]\n",
      "493: [(0, 0.050000746), (1, 0.17499763), (2, 0.050000865), (3, 0.11250083), (4, 0.050000746), (5, 0.0500008), (6, 0.05000075), (7, 0.2999992), (8, 0.050000764), (9, 0.11249766)]\n",
      "494: [(0, 0.04210616), (1, 0.1473645), (2, 0.09473565), (3, 0.042106133), (4, 0.04210616), (5, 0.09473598), (6, 0.042106166), (7, 0.09473369), (8, 0.35789934), (9, 0.042106196)]\n",
      "495: [(0, 0.0727275), (1, 0.16363509), (2, 0.16363657), (3, 0.16363585), (4, 0.0727275), (5, 0.07272752), (6, 0.0727275), (7, 0.0727275), (8, 0.0727275), (9, 0.07272751)]\n",
      "496: [(0, 0.2545434), (1, 0.07272793), (2, 0.07272804), (3, 0.07272791), (4, 0.072727926), (5, 0.07272798), (6, 0.07272793), (7, 0.16363299), (8, 0.07272795), (9, 0.072727956)]\n",
      "497: [(0, 0.23332773), (1, 0.06666776), (2, 0.15000013), (3, 0.06666772), (4, 0.06666775), (5, 0.066667825), (6, 0.14999774), (7, 0.066667765), (8, 0.06666777), (9, 0.066667795)]\n",
      "498: [(0, 0.058064032), (1, 0.31613755), (2, 0.058066163), (3, 0.1870996), (4, 0.025807444), (5, 0.058060132), (6, 0.058061674), (7, 0.025807463), (8, 0.15483724), (9, 0.05805873)]\n",
      "499: [(0, 0.20000117), (1, 0.057144087), (2, 0.057144273), (3, 0.12856866), (4, 0.05714408), (5, 0.12857093), (6, 0.12856953), (7, 0.057144105), (8, 0.12856905), (9, 0.05714413)]\n",
      "500: [(0, 0.08888929), (1, 0.0888893), (2, 0.08888936), (3, 0.19999622), (4, 0.08888929), (5, 0.08888933), (6, 0.08888929), (7, 0.0888893), (8, 0.08888931), (9, 0.088889316)]\n",
      "501: [(0, 0.13845778), (1, 0.061539218), (2, 0.06153933), (3, 0.13846259), (4, 0.06153921), (5, 0.061539266), (6, 0.13845974), (7, 0.2153844), (8, 0.06153923), (9, 0.06153924)]\n",
      "502: [(0, 0.09473054), (1, 0.09473372), (2, 0.042106528), (3, 0.25263304), (4, 0.14736949), (5, 0.04210644), (6, 0.042106364), (7, 0.20000114), (8, 0.042106383), (9, 0.042106405)]\n",
      "503: [(0, 0.08000038), (1, 0.080000386), (2, 0.080000445), (3, 0.18000017), (4, 0.08000038), (5, 0.17999671), (6, 0.080000386), (7, 0.080000386), (8, 0.080000386), (9, 0.08000039)]\n",
      "504: [(0, 0.061539255), (1, 0.061539263), (2, 0.06153938), (3, 0.061539233), (4, 0.061539255), (5, 0.061539315), (6, 0.2153829), (7, 0.2153827), (8, 0.1384594), (9, 0.061539292)]\n",
      "505: [(0, 0.08888922), (1, 0.08888922), (2, 0.08888927), (3, 0.088889204), (4, 0.08888922), (5, 0.19999693), (6, 0.08888922), (7, 0.08888923), (8, 0.08888923), (9, 0.088889234)]\n",
      "506: [(0, 0.13333426), (1, 0.22856629), (2, 0.08571762), (3, 0.11133761), (4, 0.038096495), (5, 0.03809659), (6, 0.08571219), (7, 0.08571272), (8, 0.15532966), (9, 0.03809655)]\n",
      "507: [(0, 0.07272771), (1, 0.07272772), (2, 0.072727785), (3, 0.0727277), (4, 0.1636367), (5, 0.16363305), (6, 0.07272772), (7, 0.072727725), (8, 0.16363615), (9, 0.07272773)]\n",
      "508: [(0, 0.088889144), (1, 0.088889144), (2, 0.08888918), (3, 0.08888914), (4, 0.088889144), (5, 0.19999771), (6, 0.088889144), (7, 0.088889144), (8, 0.088889144), (9, 0.08888915)]\n",
      "509: [(0, 0.038096257), (1, 0.13332975), (2, 0.27619794), (3, 0.038096227), (4, 0.08571226), (5, 0.08571387), (6, 0.13333121), (7, 0.08571352), (8, 0.03809628), (9, 0.0857127)]\n",
      "510: [(0, 0.053333774), (1, 0.053333778), (2, 0.12000083), (3, 0.12000009), (4, 0.25333217), (5, 0.18666421), (6, 0.053333778), (7, 0.05333378), (8, 0.053333785), (9, 0.053333793)]\n",
      "511: [(0, 0.088889375), (1, 0.08888938), (2, 0.08888946), (3, 0.08888937), (4, 0.19999549), (5, 0.08888941), (6, 0.08888938), (7, 0.08888939), (8, 0.08888939), (9, 0.088889405)]\n",
      "512: [(0, 0.088889465), (1, 0.19999465), (2, 0.08888956), (3, 0.08888945), (4, 0.088889465), (5, 0.08888951), (6, 0.088889465), (7, 0.08888947), (8, 0.08888948), (9, 0.08888949)]\n",
      "513: [(0, 0.072727874), (1, 0.072727874), (2, 0.16363749), (3, 0.07272785), (4, 0.072727874), (5, 0.07272792), (6, 0.16363364), (7, 0.07272788), (8, 0.16363372), (9, 0.072727896)]\n",
      "514: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "515: [(0, 0.05000058), (1, 0.4249991), (2, 0.05000067), (3, 0.050000563), (4, 0.05000058), (5, 0.17499618), (6, 0.050000582), (7, 0.05000059), (8, 0.050000593), (9, 0.050000604)]\n",
      "516: [(0, 0.080000676), (1, 0.08000068), (2, 0.08000079), (3, 0.08000066), (4, 0.080000676), (5, 0.08000073), (6, 0.27999374), (7, 0.08000069), (8, 0.0800007), (9, 0.080000706)]\n",
      "517: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "518: [(0, 0.08888911), (1, 0.08888911), (2, 0.08888914), (3, 0.0888891), (4, 0.08888911), (5, 0.08888912), (6, 0.19999799), (7, 0.088889115), (8, 0.088889115), (9, 0.088889115)]\n",
      "519: [(0, 0.16362952), (1, 0.07272938), (2, 0.0727297), (3, 0.07272931), (4, 0.16362873), (5, 0.07272952), (6, 0.07272938), (7, 0.07272941), (8, 0.1636356), (9, 0.07272945)]\n",
      "520: [(0, 0.08000023), (1, 0.08000024), (2, 0.080000274), (3, 0.08000023), (4, 0.08000023), (5, 0.27999786), (6, 0.08000024), (7, 0.08000024), (8, 0.08000024), (9, 0.080000244)]\n",
      "521: [(0, 0.13846119), (1, 0.06153908), (2, 0.13846292), (3, 0.061539054), (4, 0.061539073), (5, 0.06153912), (6, 0.06153908), (7, 0.21538232), (8, 0.061539087), (9, 0.13845907)]\n",
      "522: [(0, 0.08888907), (1, 0.19999826), (2, 0.08888911), (3, 0.08888906), (4, 0.08888907), (5, 0.088889085), (6, 0.08888907), (7, 0.08888908), (8, 0.08888908), (9, 0.08888908)]\n",
      "523: [(0, 0.17999682), (1, 0.08000043), (2, 0.0800005), (3, 0.17999963), (4, 0.08000042), (5, 0.08000046), (6, 0.08000043), (7, 0.08000044), (8, 0.08000044), (9, 0.080000445)]\n",
      "524: [(0, 0.08888917), (1, 0.08888917), (2, 0.19999745), (3, 0.08888916), (4, 0.08888917), (5, 0.08888919), (6, 0.08888917), (7, 0.088889174), (8, 0.088889174), (9, 0.08888918)]\n",
      "525: [(0, 0.13846415), (1, 0.061539967), (2, 0.06154019), (3, 0.06153991), (4, 0.29229438), (5, 0.13846146), (6, 0.061539963), (7, 0.061539985), (8, 0.06153999), (9, 0.061540015)]\n",
      "526: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "527: [(0, 0.061539214), (1, 0.061539225), (2, 0.061539337), (3, 0.1384589), (4, 0.061539214), (5, 0.2923073), (6, 0.1384591), (7, 0.061539233), (8, 0.061539236), (9, 0.061539248)]\n",
      "528: [(0, 0.061539102), (1, 0.061539106), (2, 0.061539207), (3, 0.21538582), (4, 0.061539102), (5, 0.13845912), (6, 0.061539106), (7, 0.2153812), (8, 0.06153912), (9, 0.06153913)]\n",
      "529: [(0, 0.23332836), (1, 0.06666732), (2, 0.066667415), (3, 0.066667296), (4, 0.15000068), (5, 0.06666736), (6, 0.06666732), (7, 0.066667326), (8, 0.06666733), (9, 0.14999963)]\n",
      "530: [(0, 0.057144128), (1, 0.05714414), (2, 0.05714433), (3, 0.05714409), (4, 0.12856984), (5, 0.057144225), (6, 0.12856287), (7, 0.05714415), (8, 0.2000039), (9, 0.19999833)]\n",
      "531: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "532: [(0, 0.19999722), (1, 0.08888919), (2, 0.08888924), (3, 0.08888918), (4, 0.08888919), (5, 0.08888921), (6, 0.08888919), (7, 0.0888892), (8, 0.0888892), (9, 0.088889204)]\n",
      "533: [(0, 0.034783605), (1, 0.0782542), (2, 0.03478377), (3, 0.20869955), (4, 0.07826151), (5, 0.034783684), (6, 0.12174121), (7, 0.12173798), (8, 0.16521613), (9, 0.121738315)]\n",
      "534: [(0, 0.08888926), (1, 0.08888926), (2, 0.08888932), (3, 0.08888925), (4, 0.19999653), (5, 0.088889286), (6, 0.08888926), (7, 0.08888927), (8, 0.08888927), (9, 0.08888928)]\n",
      "535: [(0, 0.13846204), (1, 0.1384621), (2, 0.061539225), (3, 0.061539102), (4, 0.06153912), (5, 0.13845891), (6, 0.061539125), (7, 0.21538211), (8, 0.061539136), (9, 0.061539147)]\n",
      "536: [(0, 0.03809567), (1, 0.038095675), (2, 0.03809574), (3, 0.04299081), (4, 0.08571453), (5, 0.0380957), (6, 0.038095675), (7, 0.27619046), (8, 0.36653), (9, 0.03809569)]\n",
      "537: [(0, 0.16363734), (1, 0.07272811), (2, 0.07272824), (3, 0.16363378), (4, 0.072728105), (5, 0.16363192), (6, 0.072728105), (7, 0.07272812), (8, 0.07272813), (9, 0.07272814)]\n",
      "538: [(0, 0.088888995), (1, 0.088889), (2, 0.08888902), (3, 0.088888995), (4, 0.088889), (5, 0.08888901), (6, 0.088889), (7, 0.088889), (8, 0.19999897), (9, 0.088889)]\n",
      "539: [(0, 0.17999718), (1, 0.080000594), (2, 0.08000068), (3, 0.08000057), (4, 0.080000594), (5, 0.08000063), (6, 0.17999795), (7, 0.0800006), (8, 0.0800006), (9, 0.08000062)]\n",
      "540: [(0, 0.06666733), (1, 0.14999837), (2, 0.066667445), (3, 0.23333192), (4, 0.06666733), (5, 0.14999814), (6, 0.06666734), (7, 0.06666735), (8, 0.066667356), (9, 0.06666736)]\n",
      "541: [(0, 0.08888893), (1, 0.08888893), (2, 0.19999963), (3, 0.08888893), (4, 0.08888893), (5, 0.08888893), (6, 0.08888893), (7, 0.08888893), (8, 0.08888893), (9, 0.08888893)]\n",
      "542: [(0, 0.17999715), (1, 0.08000057), (2, 0.080000654), (3, 0.17999817), (4, 0.080000564), (5, 0.0800006), (6, 0.08000057), (7, 0.08000057), (8, 0.08000057), (9, 0.08000059)]\n",
      "543: [(0, 0.25333303), (1, 0.053334426), (2, 0.05333459), (3, 0.053334385), (4, 0.05333442), (5, 0.053334497), (6, 0.18665929), (7, 0.18666646), (8, 0.053334445), (9, 0.053334467)]\n",
      "544: [(0, 0.08888906), (1, 0.08888906), (2, 0.088889085), (3, 0.088889055), (4, 0.19999838), (5, 0.08888908), (6, 0.08888906), (7, 0.08888906), (8, 0.08888906), (9, 0.08888907)]\n",
      "545: [(0, 0.07199613), (1, 0.23199904), (2, 0.032000925), (3, 0.111997604), (4, 0.071999416), (5, 0.19200304), (6, 0.032000802), (7, 0.07199932), (8, 0.15200287), (9, 0.032000832)]\n",
      "546: [(0, 0.08888897), (1, 0.08888897), (2, 0.19999921), (3, 0.08888897), (4, 0.08888897), (5, 0.08888898), (6, 0.08888897), (7, 0.08888897), (8, 0.08888897), (9, 0.08888897)]\n",
      "547: [(0, 0.08888895), (1, 0.08888895), (2, 0.1999995), (3, 0.08888895), (4, 0.08888895), (5, 0.08888895), (6, 0.08888895), (7, 0.08888895), (8, 0.08888895), (9, 0.08888895)]\n",
      "548: [(0, 0.08888919), (1, 0.08888919), (2, 0.08888924), (3, 0.08888918), (4, 0.08888919), (5, 0.08888921), (6, 0.08888919), (7, 0.19999722), (8, 0.0888892), (9, 0.088889204)]\n",
      "549: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "550: [(0, 0.17999828), (1, 0.080000386), (2, 0.17999868), (3, 0.08000037), (4, 0.08000038), (5, 0.08000041), (6, 0.080000386), (7, 0.080000386), (8, 0.080000386), (9, 0.08000039)]\n",
      "551: [(0, 0.08888927), (1, 0.08888928), (2, 0.08888934), (3, 0.08888926), (4, 0.08888927), (5, 0.19999646), (6, 0.08888928), (7, 0.08888928), (8, 0.088889286), (9, 0.08888929)]\n",
      "552: [(0, 0.07272774), (1, 0.25454256), (2, 0.072727814), (3, 0.072727725), (4, 0.1636354), (5, 0.07272777), (6, 0.07272774), (7, 0.07272775), (8, 0.07272775), (9, 0.07272776)]\n",
      "553: [(0, 0.030770045), (1, 0.14615162), (2, 0.0692321), (3, 0.107694305), (4, 0.06922764), (5, 0.18461907), (6, 0.14615305), (7, 0.03077006), (8, 0.10769091), (9, 0.10769119)]\n",
      "554: [(0, 0.042106465), (1, 0.14736529), (2, 0.042106654), (3, 0.14736992), (4, 0.09473286), (5, 0.042106554), (6, 0.14736918), (7, 0.20000277), (8, 0.042106494), (9, 0.09473378)]\n",
      "555: [(0, 0.08888939), (1, 0.0888894), (2, 0.08888948), (3, 0.088889375), (4, 0.08888939), (5, 0.088889435), (6, 0.0888894), (7, 0.19999526), (8, 0.088889405), (9, 0.08888942)]\n",
      "556: [(0, 0.06666695), (1, 0.06666696), (2, 0.15000018), (3, 0.14999963), (4, 0.14999896), (5, 0.066666976), (6, 0.14999947), (7, 0.06666696), (8, 0.06666697), (9, 0.06666697)]\n",
      "557: [(0, 0.080000736), (1, 0.17999747), (2, 0.080000855), (3, 0.08000072), (4, 0.080000736), (5, 0.08000079), (6, 0.08000074), (7, 0.08000075), (8, 0.08000075), (9, 0.17999645)]\n",
      "558: [(0, 0.08000071), (1, 0.08000072), (2, 0.08000083), (3, 0.0800007), (4, 0.08000071), (5, 0.17999732), (6, 0.08000072), (7, 0.080000736), (8, 0.1799968), (9, 0.08000075)]\n",
      "559: [(0, 0.08888934), (1, 0.1999958), (2, 0.08888942), (3, 0.08888933), (4, 0.08888934), (5, 0.08888938), (6, 0.088889346), (7, 0.08888935), (8, 0.08888935), (9, 0.08888936)]\n",
      "560: [(0, 0.08888895), (1, 0.08888895), (2, 0.08888896), (3, 0.19999944), (4, 0.08888895), (5, 0.08888896), (6, 0.08888895), (7, 0.08888895), (8, 0.08888895), (9, 0.08888895)]\n",
      "561: [(0, 0.11249951), (1, 0.05000057), (2, 0.050000656), (3, 0.17500117), (4, 0.11250042), (5, 0.23749965), (6, 0.11249628), (7, 0.05000058), (8, 0.05000058), (9, 0.05000059)]\n",
      "562: [(0, 0.08888903), (1, 0.08888904), (2, 0.088889055), (3, 0.08888903), (4, 0.1999987), (5, 0.08888904), (6, 0.08888904), (7, 0.08888904), (8, 0.08888904), (9, 0.08888904)]\n",
      "563: [(0, 0.08888908), (1, 0.08888908), (2, 0.088889115), (3, 0.08888908), (4, 0.19999827), (5, 0.08888909), (6, 0.08888908), (7, 0.088889085), (8, 0.088889085), (9, 0.088889085)]\n",
      "564: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "565: [(0, 0.08888925), (1, 0.08888925), (2, 0.08888931), (3, 0.08888924), (4, 0.08888925), (5, 0.08888928), (6, 0.19999668), (7, 0.088889256), (8, 0.088889256), (9, 0.08888926)]\n",
      "566: [(0, 0.03809598), (1, 0.18094862), (2, 0.0380961), (3, 0.03809596), (4, 0.085715145), (5, 0.08571424), (6, 0.37143156), (7, 0.038095996), (8, 0.038096), (9, 0.08571038)]\n",
      "567: [(0, 0.08888919), (1, 0.08888919), (2, 0.08888924), (3, 0.08888918), (4, 0.08888919), (5, 0.19999719), (6, 0.08888919), (7, 0.0888892), (8, 0.0888892), (9, 0.088889204)]\n",
      "568: [(0, 0.08000024), (1, 0.080000244), (2, 0.18000023), (3, 0.08000024), (4, 0.080000244), (5, 0.17999783), (6, 0.080000244), (7, 0.080000244), (8, 0.08000025), (9, 0.08000025)]\n",
      "569: [(0, 0.16363615), (1, 0.072727636), (2, 0.072727695), (3, 0.07272762), (4, 0.072727636), (5, 0.07272766), (6, 0.072727636), (7, 0.25454262), (8, 0.07272764), (9, 0.07272765)]\n",
      "570: [(0, 0.17500219), (1, 0.17499991), (2, 0.05000126), (3, 0.05000106), (4, 0.05000109), (5, 0.11249912), (6, 0.11250144), (7, 0.11249423), (8, 0.050001115), (9, 0.11249855)]\n",
      "571: [(0, 0.08000073), (1, 0.17999747), (2, 0.08000085), (3, 0.080000706), (4, 0.08000073), (5, 0.08000078), (6, 0.080000736), (7, 0.17999652), (8, 0.08000074), (9, 0.08000076)]\n",
      "572: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "573: [(0, 0.16470367), (1, 0.047059692), (2, 0.105882354), (3, 0.10588363), (4, 0.1058773), (5, 0.04705975), (6, 0.28235447), (7, 0.0470597), (8, 0.047059704), (9, 0.047059722)]\n",
      "574: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "575: [(0, 0.080000535), (1, 0.08000054), (2, 0.17999916), (3, 0.08000052), (4, 0.080000535), (5, 0.08000058), (6, 0.08000054), (7, 0.17999648), (8, 0.08000055), (9, 0.08000056)]\n",
      "576: [(0, 0.08000038), (1, 0.080000386), (2, 0.18000065), (3, 0.08000037), (4, 0.08000038), (5, 0.17999628), (6, 0.080000386), (7, 0.080000386), (8, 0.080000386), (9, 0.08000039)]\n",
      "577: [(0, 0.088889), (1, 0.088889), (2, 0.08888902), (3, 0.19999892), (4, 0.088889), (5, 0.08888901), (6, 0.088889), (7, 0.088889), (8, 0.088889), (9, 0.088889)]\n",
      "578: [(0, 0.0727281), (1, 0.072728105), (2, 0.072728224), (3, 0.16363694), (4, 0.0727281), (5, 0.07272816), (6, 0.072728105), (7, 0.1636341), (8, 0.07272812), (9, 0.1636321)]\n",
      "579: [(0, 0.09000032), (1, 0.040000927), (2, 0.040001065), (3, 0.1399986), (4, 0.04000092), (5, 0.19000031), (6, 0.089997075), (7, 0.040000938), (8, 0.24000452), (9, 0.08999534)]\n",
      "580: [(0, 0.23749849), (1, 0.1749983), (2, 0.05000084), (3, 0.050000705), (4, 0.11250101), (5, 0.05000078), (6, 0.05000073), (7, 0.1125007), (8, 0.050000742), (9, 0.1124977)]\n",
      "581: [(0, 0.0727278), (1, 0.07272781), (2, 0.07272788), (3, 0.072727785), (4, 0.16363692), (5, 0.072727844), (6, 0.07272781), (7, 0.072727814), (8, 0.16363229), (9, 0.16363604)]\n",
      "582: [(0, 0.07272792), (1, 0.1636342), (2, 0.07272802), (3, 0.0727279), (4, 0.07272792), (5, 0.07272797), (6, 0.25454226), (7, 0.07272793), (8, 0.07272793), (9, 0.07272795)]\n",
      "583: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "584: [(0, 0.088889316), (1, 0.19999605), (2, 0.088889375), (3, 0.0888893), (4, 0.088889316), (5, 0.08888934), (6, 0.088889316), (7, 0.088889316), (8, 0.088889316), (9, 0.08888932)]\n",
      "585: [(0, 0.12000109), (1, 0.05333441), (2, 0.05333458), (3, 0.053334378), (4, 0.053334404), (5, 0.11999855), (6, 0.11999595), (7, 0.18666615), (8, 0.11999988), (9, 0.1200006)]\n",
      "586: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "587: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "588: [(0, 0.17999642), (1, 0.080000736), (2, 0.08000084), (3, 0.080000706), (4, 0.08000073), (5, 0.08000078), (6, 0.080000736), (7, 0.17999756), (8, 0.08000074), (9, 0.08000076)]\n",
      "589: [(0, 0.17272654), (1, 0.08181568), (2, 0.08182119), (3, 0.036364764), (4, 0.036364794), (5, 0.03636488), (6, 0.17272797), (7, 0.21818438), (8, 0.036364824), (9, 0.12726499)]\n",
      "590: [(0, 0.1999883), (1, 0.08889016), (2, 0.08889035), (3, 0.08889011), (4, 0.08889014), (5, 0.08889025), (6, 0.08889016), (7, 0.08889017), (8, 0.08889018), (9, 0.0888902)]\n",
      "591: [(0, 0.07272757), (1, 0.07272757), (2, 0.2545463), (3, 0.07272756), (4, 0.07272757), (5, 0.07272759), (6, 0.07272757), (7, 0.072727576), (8, 0.072727576), (9, 0.1636331)]\n",
      "592: [(0, 0.04444547), (1, 0.15555254), (2, 0.044445638), (3, 0.21111053), (4, 0.04444547), (5, 0.15555492), (6, 0.09999789), (7, 0.044445496), (8, 0.0444455), (9, 0.15555653)]\n",
      "593: [(0, 0.033334054), (1, 0.03333406), (2, 0.03333417), (3, 0.11666311), (4, 0.074999906), (5, 0.07499781), (6, 0.15833314), (7, 0.03333407), (8, 0.11666538), (9, 0.32500428)]\n",
      "594: [(0, 0.08000018), (1, 0.08000018), (2, 0.18000004), (3, 0.08000018), (4, 0.08000018), (5, 0.08000019), (6, 0.17999847), (7, 0.08000018), (8, 0.08000018), (9, 0.080000184)]\n",
      "595: [(0, 0.07272798), (1, 0.072727986), (2, 0.0727281), (3, 0.1636331), (4, 0.16363738), (5, 0.07272803), (6, 0.1636334), (7, 0.07272799), (8, 0.07272799), (9, 0.07272801)]\n",
      "596: [(0, 0.1217416), (1, 0.20869684), (2, 0.078261845), (3, 0.07825257), (4, 0.078261904), (5, 0.034784146), (6, 0.121739715), (7, 0.078259125), (8, 0.16521813), (9, 0.034784097)]\n",
      "597: [(0, 0.072727926), (1, 0.07272793), (2, 0.16363584), (3, 0.07272791), (4, 0.072727926), (5, 0.07272798), (6, 0.07272793), (7, 0.1636323), (8, 0.07272794), (9, 0.16363633)]\n",
      "598: [(0, 0.13845448), (1, 0.21538346), (2, 0.13846451), (3, 0.061539557), (4, 0.06153959), (5, 0.13845992), (6, 0.061539594), (7, 0.061539613), (8, 0.061539616), (9, 0.061539635)]\n",
      "599: [(0, 0.04000053), (1, 0.04000053), (2, 0.09000049), (3, 0.09000015), (4, 0.14000116), (5, 0.04000057), (6, 0.08999709), (7, 0.08999759), (8, 0.34000134), (9, 0.04000055)]\n",
      "600: [(0, 0.06666702), (1, 0.14999983), (2, 0.06666708), (3, 0.23333368), (4, 0.06666702), (5, 0.14999726), (6, 0.06666702), (7, 0.06666702), (8, 0.06666703), (9, 0.066667035)]\n",
      "601: [(0, 0.047060125), (1, 0.10586924), (2, 0.10588501), (3, 0.047060087), (4, 0.105883755), (5, 0.04706022), (6, 0.10588144), (7, 0.04706015), (8, 0.28236046), (9, 0.10587946)]\n",
      "602: [(0, 0.061539207), (1, 0.13846152), (2, 0.061539326), (3, 0.06153918), (4, 0.061539207), (5, 0.06153926), (6, 0.06153921), (7, 0.13846071), (8, 0.13845922), (9, 0.2153832)]\n",
      "603: [(0, 0.17999268), (1, 0.08000107), (2, 0.08000124), (3, 0.080001034), (4, 0.08000106), (5, 0.08000115), (6, 0.1799984), (7, 0.08000109), (8, 0.08000109), (9, 0.080001116)]\n",
      "604: [(0, 0.08000018), (1, 0.08000018), (2, 0.27999836), (3, 0.08000018), (4, 0.08000018), (5, 0.08000019), (6, 0.08000018), (7, 0.08000018), (8, 0.08000018), (9, 0.080000184)]\n",
      "605: [(0, 0.19999751), (1, 0.08888916), (2, 0.0888892), (3, 0.088889144), (4, 0.08888916), (5, 0.088889174), (6, 0.08888916), (7, 0.08888916), (8, 0.08888917), (9, 0.08888917)]\n",
      "606: [(0, 0.06666764), (1, 0.14999865), (2, 0.066667795), (3, 0.14999668), (4, 0.06666764), (5, 0.2333309), (6, 0.066667646), (7, 0.06666766), (8, 0.06666766), (9, 0.06666768)]\n",
      "607: [(0, 0.080000475), (1, 0.080000475), (2, 0.08000056), (3, 0.08000047), (4, 0.080000475), (5, 0.08000051), (6, 0.080000475), (7, 0.17999688), (8, 0.08000049), (9, 0.17999922)]\n",
      "608: [(0, 0.25454178), (1, 0.16363429), (2, 0.07272807), (3, 0.07272794), (4, 0.07272796), (5, 0.07272801), (6, 0.07272797), (7, 0.07272797), (8, 0.07272798), (9, 0.07272799)]\n",
      "609: [(0, 0.2000004), (1, 0.12857072), (2, 0.05714408), (3, 0.12857321), (4, 0.1285679), (5, 0.05714399), (6, 0.05714392), (7, 0.057143934), (8, 0.12856786), (9, 0.05714396)]\n",
      "610: [(0, 0.06666756), (1, 0.14999728), (2, 0.0666677), (3, 0.066667534), (4, 0.066667564), (5, 0.066667624), (6, 0.23333219), (7, 0.14999738), (8, 0.06666758), (9, 0.066667594)]\n",
      "611: [(0, 0.08888917), (1, 0.08888917), (2, 0.19999745), (3, 0.08888916), (4, 0.08888917), (5, 0.08888919), (6, 0.08888917), (7, 0.088889174), (8, 0.088889174), (9, 0.08888918)]\n",
      "612: [(0, 0.057143968), (1, 0.12856694), (2, 0.12857434), (3, 0.20000258), (4, 0.05714397), (5, 0.057144053), (6, 0.12856394), (7, 0.057143994), (8, 0.057143997), (9, 0.12857223)]\n",
      "613: [(0, 0.0800006), (1, 0.0800006), (2, 0.0800007), (3, 0.08000059), (4, 0.0800006), (5, 0.08000065), (6, 0.17999686), (7, 0.17999817), (8, 0.08000062), (9, 0.08000063)]\n",
      "614: [(0, 0.18823807), (1, 0.041171778), (2, 0.04117801), (3, 0.14412075), (4, 0.070589036), (5, 0.041174576), (6, 0.12941197), (7, 0.17353247), (8, 0.055877093), (9, 0.11470625)]\n",
      "615: [(0, 0.08888933), (1, 0.08888934), (2, 0.088889405), (3, 0.088889316), (4, 0.08888933), (5, 0.08888937), (6, 0.08888934), (7, 0.19999588), (8, 0.088889346), (9, 0.08888935)]\n",
      "616: [(0, 0.08888976), (1, 0.19999187), (2, 0.088889904), (3, 0.08888974), (4, 0.08888976), (5, 0.08888983), (6, 0.08888977), (7, 0.088889785), (8, 0.088889785), (9, 0.0888898)]\n",
      "617: [(0, 0.061539844), (1, 0.21537474), (2, 0.061540067), (3, 0.13845947), (4, 0.13846423), (5, 0.061539948), (6, 0.061539855), (7, 0.06153987), (8, 0.06153988), (9, 0.13846207)]\n",
      "618: [(0, 0.08888907), (1, 0.19999826), (2, 0.08888911), (3, 0.08888906), (4, 0.08888907), (5, 0.088889085), (6, 0.08888907), (7, 0.08888908), (8, 0.08888908), (9, 0.08888908)]\n",
      "619: [(0, 0.044445198), (1, 0.09999627), (2, 0.10000121), (3, 0.09999518), (4, 0.044445198), (5, 0.044445254), (6, 0.37778145), (7, 0.044445217), (8, 0.04444522), (9, 0.09999978)]\n",
      "620: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "621: [(0, 0.1384614), (1, 0.06153886), (2, 0.06153892), (3, 0.06153885), (4, 0.29230604), (5, 0.061538883), (6, 0.061538856), (7, 0.061538864), (8, 0.061538864), (9, 0.13846046)]\n",
      "622: [(0, 0.10768706), (1, 0.030770117), (2, 0.069230676), (3, 0.030770088), (4, 0.14615338), (5, 0.1461535), (6, 0.2615415), (7, 0.107692406), (8, 0.030770132), (9, 0.069231175)]\n",
      "623: [(0, 0.112492494), (1, 0.112500794), (2, 0.050001252), (3, 0.05000105), (4, 0.17500201), (5, 0.11249864), (6, 0.05000109), (7, 0.17500003), (8, 0.11250154), (9, 0.05000113)]\n",
      "624: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "625: [(0, 0.11999364), (1, 0.12000213), (2, 0.053334814), (3, 0.11999731), (4, 0.053334612), (5, 0.11999962), (6, 0.18666808), (7, 0.12000051), (8, 0.053334642), (9, 0.05333467)]\n",
      "626: [(0, 0.08888925), (1, 0.08888925), (2, 0.08888931), (3, 0.08888924), (4, 0.08888925), (5, 0.19999672), (6, 0.08888925), (7, 0.088889256), (8, 0.088889256), (9, 0.08888926)]\n",
      "627: [(0, 0.072727375), (1, 0.072727375), (2, 0.1636362), (3, 0.25454473), (4, 0.072727375), (5, 0.07272739), (6, 0.072727375), (7, 0.07272738), (8, 0.07272738), (9, 0.07272738)]\n",
      "628: [(0, 0.08888895), (1, 0.08888895), (2, 0.08888896), (3, 0.1999994), (4, 0.08888895), (5, 0.08888896), (6, 0.08888895), (7, 0.08888895), (8, 0.08888895), (9, 0.08888895)]\n",
      "629: [(0, 0.13845941), (1, 0.13846044), (2, 0.06153904), (3, 0.061538946), (4, 0.06153896), (5, 0.061538998), (6, 0.061538965), (7, 0.06153897), (8, 0.2923073), (9, 0.06153898)]\n",
      "630: [(0, 0.118750036), (1, 0.11874982), (2, 0.056251794), (3, 0.08750111), (4, 0.11874805), (5, 0.025000904), (6, 0.056245156), (7, 0.025000859), (8, 0.30625162), (9, 0.087500654)]\n",
      "631: [(0, 0.08888926), (1, 0.08888927), (2, 0.08888932), (3, 0.088889256), (4, 0.08888926), (5, 0.19999649), (6, 0.08888927), (7, 0.08888927), (8, 0.08888928), (9, 0.08888928)]\n",
      "632: [(0, 0.09999715), (1, 0.0444452), (2, 0.044445314), (3, 0.044445176), (4, 0.21111344), (5, 0.09999907), (6, 0.044445198), (7, 0.15555331), (8, 0.21111098), (9, 0.044445224)]\n",
      "633: [(0, 0.1285725), (1, 0.19999635), (2, 0.05714408), (3, 0.057143882), (4, 0.05714391), (5, 0.1285663), (6, 0.05714392), (7, 0.057143934), (8, 0.057143938), (9, 0.2000012)]\n",
      "634: [(0, 0.038095858), (1, 0.03809586), (2, 0.03809596), (3, 0.1333322), (4, 0.2285699), (5, 0.038095903), (6, 0.03809586), (7, 0.22857189), (8, 0.038095877), (9, 0.1809507)]\n",
      "635: [(0, 0.03528908), (1, 0.21176763), (2, 0.07451075), (3, 0.25098604), (4, 0.11372374), (5, 0.03529178), (6, 0.015686788), (7, 0.13333312), (8, 0.113724254), (9, 0.015686806)]\n",
      "636: [(0, 0.061539087), (1, 0.061539095), (2, 0.13846295), (3, 0.061539073), (4, 0.06153909), (5, 0.06153914), (6, 0.21538387), (7, 0.13845971), (8, 0.1384589), (9, 0.06153912)]\n",
      "637: [(0, 0.27200338), (1, 0.071997404), (2, 0.15200341), (3, 0.19199677), (4, 0.032000855), (5, 0.03200092), (6, 0.072000794), (7, 0.071996525), (8, 0.032000877), (9, 0.07199907)]\n",
      "638: [(0, 0.061538797), (1, 0.0615388), (2, 0.061538853), (3, 0.21538395), (4, 0.061538797), (5, 0.061538827), (6, 0.0615388), (7, 0.061538808), (8, 0.061538808), (9, 0.2923056)]\n",
      "639: [(0, 0.21110873), (1, 0.15555564), (2, 0.04444585), (3, 0.099993534), (4, 0.044445656), (5, 0.044445746), (6, 0.10000097), (7, 0.044445682), (8, 0.044445686), (9, 0.2111125)]\n",
      "640: [(0, 0.0444456), (1, 0.09999409), (2, 0.15555876), (3, 0.09999725), (4, 0.10000188), (5, 0.10000198), (6, 0.099999815), (7, 0.09999694), (8, 0.04444563), (9, 0.15555808)]\n",
      "641: [(0, 0.11525361), (1, 0.13220294), (2, 0.013559887), (3, 0.09830772), (4, 0.030508624), (5, 0.14915256), (6, 0.25085267), (7, 0.064403), (8, 0.13219915), (9, 0.013559831)]\n",
      "642: [(0, 0.088889286), (1, 0.088889286), (2, 0.088889346), (3, 0.08888927), (4, 0.088889286), (5, 0.088889316), (6, 0.088889286), (7, 0.19999635), (8, 0.08888929), (9, 0.0888893)]\n",
      "643: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "644: [(0, 0.08888913), (1, 0.08888913), (2, 0.08888917), (3, 0.08888912), (4, 0.08888913), (5, 0.088889144), (6, 0.08888913), (7, 0.19999781), (8, 0.08888914), (9, 0.08888914)]\n",
      "645: [(0, 0.08888927), (1, 0.08888928), (2, 0.08888934), (3, 0.08888926), (4, 0.08888927), (5, 0.0888893), (6, 0.19999638), (7, 0.088889286), (8, 0.088889286), (9, 0.08888929)]\n",
      "646: [(0, 0.08888915), (1, 0.08888916), (2, 0.0888892), (3, 0.088889144), (4, 0.08888915), (5, 0.19999756), (6, 0.08888916), (7, 0.08888916), (8, 0.08888917), (9, 0.08888917)]\n",
      "647: [(0, 0.053333927), (1, 0.12000037), (2, 0.11999831), (3, 0.12000033), (4, 0.053333927), (5, 0.053333968), (6, 0.18666705), (7, 0.053333938), (8, 0.05333394), (9, 0.18666425)]\n",
      "648: [(0, 0.061538942), (1, 0.061538946), (2, 0.06153902), (3, 0.13845667), (4, 0.061538942), (5, 0.06153898), (6, 0.061538946), (7, 0.061538953), (8, 0.061538957), (9, 0.36923167)]\n",
      "649: [(0, 0.15172404), (1, 0.1517255), (2, 0.013793921), (3, 0.04827391), (4, 0.08275639), (5, 0.09999828), (6, 0.15172896), (7, 0.11724136), (8, 0.117242485), (9, 0.06551513)]\n",
      "650: [(0, 0.053334154), (1, 0.120000936), (2, 0.11999901), (3, 0.18666738), (4, 0.053334154), (5, 0.120001085), (6, 0.11999698), (7, 0.05333417), (8, 0.053334173), (9, 0.11999796)]\n",
      "651: [(0, 0.1799991), (1, 0.08000034), (2, 0.08000039), (3, 0.080000326), (4, 0.08000034), (5, 0.08000036), (6, 0.17999811), (7, 0.08000035), (8, 0.08000035), (9, 0.08000035)]\n",
      "652: [(0, 0.027586626), (1, 0.09655052), (2, 0.062069673), (3, 0.096547015), (4, 0.027586626), (5, 0.37241933), (6, 0.13103424), (7, 0.027586633), (8, 0.027586635), (9, 0.13103268)]\n",
      "653: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "654: [(0, 0.0666669), (1, 0.0666669), (2, 0.23333171), (3, 0.23333305), (4, 0.0666669), (5, 0.066666916), (6, 0.0666669), (7, 0.0666669), (8, 0.0666669), (9, 0.06666691)]\n",
      "655: [(0, 0.0999995), (1, 0.044445258), (2, 0.10000194), (3, 0.15555689), (4, 0.04444525), (5, 0.21111071), (6, 0.10000071), (7, 0.09999831), (8, 0.099996194), (9, 0.044445287)]\n",
      "656: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "657: [(0, 0.17999277), (1, 0.08000077), (2, 0.08000089), (3, 0.18000083), (4, 0.080000766), (5, 0.080000825), (6, 0.08000077), (7, 0.08000078), (8, 0.08000078), (9, 0.0800008)]\n",
      "658: [(0, 0.066667475), (1, 0.066667475), (2, 0.15000196), (3, 0.066667445), (4, 0.066667475), (5, 0.15000106), (6, 0.14999534), (7, 0.14999677), (8, 0.06666749), (9, 0.066667505)]\n",
      "659: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "660: [(0, 0.028572578), (1, 0.06428397), (2, 0.028572759), (3, 0.13571224), (4, 0.38572797), (5, 0.028572664), (6, 0.09999044), (7, 0.064284906), (8, 0.099997565), (9, 0.06428489)]\n",
      "661: [(0, 0.08888896), (1, 0.088888966), (2, 0.08888897), (3, 0.08888896), (4, 0.19999929), (5, 0.088888966), (6, 0.08888896), (7, 0.088888966), (8, 0.088888966), (9, 0.088888966)]\n",
      "662: [(0, 0.08000116), (1, 0.080001175), (2, 0.080001354), (3, 0.08000113), (4, 0.17999025), (5, 0.08000125), (6, 0.08000117), (7, 0.18000014), (8, 0.08000119), (9, 0.08000121)]\n",
      "663: [(0, 0.22352824), (1, 0.16470622), (2, 0.047060158), (3, 0.047059946), (4, 0.047059976), (5, 0.10588095), (6, 0.16470468), (7, 0.04706), (8, 0.10587985), (9, 0.047060024)]\n",
      "664: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "665: [(0, 0.080000326), (1, 0.08000033), (2, 0.18000051), (3, 0.08000032), (4, 0.08000033), (5, 0.080000356), (6, 0.08000033), (7, 0.08000034), (8, 0.08000034), (9, 0.17999685)]\n",
      "666: [(0, 0.088889), (1, 0.088889), (2, 0.08888902), (3, 0.19999892), (4, 0.088889), (5, 0.08888901), (6, 0.088889), (7, 0.088889), (8, 0.088889), (9, 0.088889)]\n",
      "667: [(0, 0.08000065), (1, 0.27999392), (2, 0.08000076), (3, 0.08000063), (4, 0.08000065), (5, 0.0800007), (6, 0.080000654), (7, 0.08000066), (8, 0.08000067), (9, 0.080000676)]\n",
      "668: [(0, 0.039128106), (1, 0.16956532), (2, 0.10435068), (3, 0.14782886), (4, 0.19130613), (5, 0.017391965), (6, 0.039124846), (7, 0.104347356), (8, 0.10434763), (9, 0.08260911)]\n",
      "669: [(0, 0.06666785), (1, 0.31666824), (2, 0.06666803), (3, 0.06666781), (4, 0.06666785), (5, 0.06666794), (6, 0.066667855), (7, 0.06666787), (8, 0.06666788), (9, 0.14998868)]\n",
      "670: [(0, 0.088889465), (1, 0.088889465), (2, 0.08888956), (3, 0.19999458), (4, 0.088889465), (5, 0.08888951), (6, 0.088889465), (7, 0.08888947), (8, 0.08888948), (9, 0.08888949)]\n",
      "671: [(0, 0.08888905), (1, 0.08888905), (2, 0.08888907), (3, 0.08888904), (4, 0.08888905), (5, 0.19999854), (6, 0.08888905), (7, 0.08888905), (8, 0.088889055), (9, 0.088889055)]\n",
      "672: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "673: [(0, 0.088889934), (1, 0.08888995), (2, 0.08889011), (3, 0.08888991), (4, 0.19999024), (5, 0.08889002), (6, 0.08888995), (7, 0.088889964), (8, 0.08888997), (9, 0.08888999)]\n",
      "674: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "675: [(0, 0.08888926), (1, 0.08888926), (2, 0.08888932), (3, 0.08888925), (4, 0.19999653), (5, 0.088889286), (6, 0.08888926), (7, 0.08888927), (8, 0.08888927), (9, 0.08888928)]\n",
      "676: [(0, 0.07272754), (1, 0.07272754), (2, 0.16363607), (3, 0.07272753), (4, 0.07272754), (5, 0.25454363), (6, 0.07272754), (7, 0.07272754), (8, 0.07272754), (9, 0.072727546)]\n",
      "677: [(0, 0.02857211), (1, 0.028572114), (2, 0.064283915), (3, 0.06428571), (4, 0.02857211), (5, 0.09999867), (6, 0.09999832), (7, 0.17142609), (8, 0.028572127), (9, 0.38571888)]\n",
      "678: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "679: [(0, 0.16363297), (1, 0.0727277), (2, 0.16363718), (3, 0.16363594), (4, 0.072727695), (5, 0.072727725), (6, 0.0727277), (7, 0.0727277), (8, 0.0727277), (9, 0.07272772)]\n",
      "680: [(0, 0.04705973), (1, 0.105881624), (2, 0.047059875), (3, 0.2235275), (4, 0.1058828), (5, 0.16470537), (6, 0.047059737), (7, 0.04705975), (8, 0.16470385), (9, 0.04705977)]\n",
      "681: [(0, 0.25982094), (1, 0.119625844), (2, 0.016823016), (3, 0.14766286), (4, 0.13832174), (5, 0.04485748), (6, 0.035507437), (7, 0.06354873), (8, 0.11028082), (9, 0.06355115)]\n",
      "682: [(0, 0.08888917), (1, 0.08888917), (2, 0.19999745), (3, 0.08888916), (4, 0.08888917), (5, 0.08888919), (6, 0.08888917), (7, 0.088889174), (8, 0.088889174), (9, 0.08888918)]\n",
      "683: [(0, 0.066667266), (1, 0.06666728), (2, 0.06666737), (3, 0.06666725), (4, 0.06666727), (5, 0.14999746), (6, 0.06666727), (7, 0.31666422), (8, 0.06666729), (9, 0.066667296)]\n",
      "684: [(0, 0.07272853), (1, 0.07272854), (2, 0.07272873), (3, 0.0727285), (4, 0.07272853), (5, 0.1636357), (6, 0.07272854), (7, 0.16363664), (8, 0.07272856), (9, 0.1636277)]\n",
      "685: [(0, 0.12856676), (1, 0.12857248), (2, 0.057143852), (3, 0.12857272), (4, 0.057143714), (5, 0.057143778), (6, 0.057143718), (7, 0.057143733), (8, 0.057143737), (9, 0.27142555)]\n",
      "686: [(0, 0.088889115), (1, 0.08888912), (2, 0.08888915), (3, 0.08888911), (4, 0.088889115), (5, 0.08888914), (6, 0.19999789), (7, 0.08888912), (8, 0.08888912), (9, 0.08888913)]\n",
      "687: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "688: [(0, 0.08888919), (1, 0.08888919), (2, 0.08888924), (3, 0.08888918), (4, 0.08888919), (5, 0.08888921), (6, 0.08888919), (7, 0.0888892), (8, 0.19999722), (9, 0.088889204)]\n",
      "689: [(0, 0.08000038), (1, 0.080000386), (2, 0.080000445), (3, 0.08000037), (4, 0.08000038), (5, 0.08000041), (6, 0.080000386), (7, 0.17999755), (8, 0.080000386), (9, 0.1799993)]\n",
      "690: [(0, 0.08000035), (1, 0.080000356), (2, 0.08000041), (3, 0.18000008), (4, 0.08000035), (5, 0.08000038), (6, 0.080000356), (7, 0.080000356), (8, 0.080000356), (9, 0.17999704)]\n",
      "691: [(0, 0.16361128), (1, 0.072729655), (2, 0.16364016), (3, 0.07272957), (4, 0.07272964), (5, 0.16364095), (6, 0.072729655), (7, 0.072729684), (8, 0.0727297), (9, 0.072729744)]\n",
      "692: [(0, 0.08888893), (1, 0.08888893), (2, 0.19999962), (3, 0.08888893), (4, 0.08888893), (5, 0.08888893), (6, 0.08888893), (7, 0.08888893), (8, 0.08888893), (9, 0.08888893)]\n",
      "693: [(0, 0.088889465), (1, 0.088889465), (2, 0.088889554), (3, 0.1999947), (4, 0.088889465), (5, 0.08888951), (6, 0.088889465), (7, 0.08888947), (8, 0.08888947), (9, 0.08888949)]\n",
      "694: [(0, 0.08888923), (1, 0.08888923), (2, 0.08888928), (3, 0.08888921), (4, 0.08888923), (5, 0.08888925), (6, 0.08888923), (7, 0.088889234), (8, 0.088889234), (9, 0.19999693)]\n",
      "695: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "696: [(0, 0.08888954), (1, 0.19999395), (2, 0.08888964), (3, 0.08888952), (4, 0.08888954), (5, 0.088889584), (6, 0.08888955), (7, 0.088889554), (8, 0.088889554), (9, 0.08888956)]\n",
      "697: [(0, 0.07272824), (1, 0.07272825), (2, 0.0727284), (3, 0.07272822), (4, 0.16363645), (5, 0.16363226), (6, 0.07272825), (7, 0.16363335), (8, 0.07272826), (9, 0.07272828)]\n",
      "698: [(0, 0.08888945), (1, 0.19999477), (2, 0.08888955), (3, 0.08888944), (4, 0.08888945), (5, 0.0888895), (6, 0.08888946), (7, 0.088889465), (8, 0.088889465), (9, 0.08888948)]\n",
      "699: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "700: [(0, 0.22307993), (1, 0.069227085), (2, 0.03077015), (3, 0.22307883), (4, 0.06923054), (5, 0.030770084), (6, 0.03077003), (7, 0.107688874), (8, 0.1461535), (9, 0.069231)]\n",
      "701: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "702: [(0, 0.08888893), (1, 0.08888893), (2, 0.19999962), (3, 0.08888893), (4, 0.08888893), (5, 0.08888893), (6, 0.08888893), (7, 0.08888893), (8, 0.08888893), (9, 0.08888893)]\n",
      "703: [(0, 0.072727375), (1, 0.072727375), (2, 0.34545356), (3, 0.072727375), (4, 0.072727375), (5, 0.07272739), (6, 0.072727375), (7, 0.07272738), (8, 0.07272738), (9, 0.07272738)]\n",
      "704: [(0, 0.08888908), (1, 0.088889085), (2, 0.19999816), (3, 0.08888908), (4, 0.08888908), (5, 0.0888891), (6, 0.088889085), (7, 0.088889085), (8, 0.088889085), (9, 0.08888909)]\n",
      "705: [(0, 0.08000019), (1, 0.08000019), (2, 0.08000022), (3, 0.08000019), (4, 0.17999956), (5, 0.08000021), (6, 0.08000019), (7, 0.0800002), (8, 0.0800002), (9, 0.17999883)]\n",
      "706: [(0, 0.25128675), (1, 0.097436145), (2, 0.04615422), (3, 0.16786511), (4, 0.12307775), (5, 0.02051344), (6, 0.07179331), (7, 0.07179121), (8, 0.10393119), (9, 0.046150908)]\n",
      "707: [(0, 0.14999051), (1, 0.23333494), (2, 0.06666793), (3, 0.06666773), (4, 0.06666776), (5, 0.06666783), (6, 0.14999992), (7, 0.06666777), (8, 0.06666779), (9, 0.0666678)]\n",
      "708: [(0, 0.08000035), (1, 0.080000356), (2, 0.08000041), (3, 0.08000034), (4, 0.08000035), (5, 0.08000038), (6, 0.080000356), (7, 0.1799983), (8, 0.080000356), (9, 0.17999883)]\n",
      "709: [(0, 0.057143483), (1, 0.057143487), (2, 0.057143584), (3, 0.05714347), (4, 0.1285722), (5, 0.12856874), (6, 0.057143487), (7, 0.0571435), (8, 0.19999997), (9, 0.19999808)]\n",
      "710: [(0, 0.08888897), (1, 0.08888897), (2, 0.08888899), (3, 0.08888897), (4, 0.08888897), (5, 0.19999917), (6, 0.08888897), (7, 0.08888897), (8, 0.08888897), (9, 0.08888897)]\n",
      "711: [(0, 0.08888897), (1, 0.08888897), (2, 0.08888899), (3, 0.08888897), (4, 0.08888897), (5, 0.08888898), (6, 0.08888897), (7, 0.19999915), (8, 0.08888897), (9, 0.08888897)]\n",
      "712: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "713: [(0, 0.053334013), (1, 0.05333402), (2, 0.053334124), (3, 0.12000054), (4, 0.11999784), (5, 0.053334065), (6, 0.05333402), (7, 0.053334028), (8, 0.2533312), (9, 0.18666616)]\n",
      "714: [(0, 0.0800005), (1, 0.27999535), (2, 0.08000057), (3, 0.08000048), (4, 0.0800005), (5, 0.080000535), (6, 0.0800005), (7, 0.080000505), (8, 0.08000051), (9, 0.08000051)]\n",
      "715: [(0, 0.05714344), (1, 0.05714345), (2, 0.1285727), (3, 0.057143427), (4, 0.2714277), (5, 0.12856984), (6, 0.05714345), (7, 0.057143454), (8, 0.12856908), (9, 0.05714347)]\n",
      "716: [(0, 0.08888925), (1, 0.19999665), (2, 0.08888931), (3, 0.08888924), (4, 0.08888925), (5, 0.08888928), (6, 0.08888925), (7, 0.088889256), (8, 0.088889256), (9, 0.08888926)]\n",
      "717: [(0, 0.080000594), (1, 0.080000594), (2, 0.08000068), (3, 0.08000057), (4, 0.080000594), (5, 0.08000064), (6, 0.080000594), (7, 0.17999788), (8, 0.1799972), (9, 0.08000062)]\n",
      "718: [(0, 0.08181646), (1, 0.036364373), (2, 0.03636448), (3, 0.17272641), (4, 0.30909583), (5, 0.08181095), (6, 0.03636437), (7, 0.03636438), (8, 0.17272836), (9, 0.036364395)]\n",
      "719: [(0, 0.12666814), (1, 0.15999986), (2, 0.059997838), (3, 0.12666713), (4, 0.026667269), (5, 0.05999873), (6, 0.12666667), (7, 0.02666728), (8, 0.16000037), (9, 0.12666672)]\n",
      "720: [(0, 0.19999851), (1, 0.0571435), (2, 0.12857285), (3, 0.057143476), (4, 0.05714349), (5, 0.05714354), (6, 0.0571435), (7, 0.1999982), (8, 0.05714351), (9, 0.12856945)]\n",
      "721: [(0, 0.088889115), (1, 0.08888912), (2, 0.08888915), (3, 0.08888911), (4, 0.088889115), (5, 0.08888914), (6, 0.08888912), (7, 0.08888912), (8, 0.08888912), (9, 0.19999786)]\n",
      "722: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "723: [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "724: [(0, 0.07272774), (1, 0.07272774), (2, 0.072727814), (3, 0.072727725), (4, 0.16363284), (5, 0.25454512), (6, 0.07272774), (7, 0.07272775), (8, 0.07272775), (9, 0.07272776)]\n",
      "725: [(0, 0.07272787), (1, 0.072727874), (2, 0.07272796), (3, 0.16363357), (4, 0.07272787), (5, 0.07272791), (6, 0.072727874), (7, 0.2545433), (8, 0.07272788), (9, 0.07272789)]\n",
      "726: [(0, 0.080000065), (1, 0.080000065), (2, 0.08000008), (3, 0.080000065), (4, 0.2799994), (5, 0.08000007), (6, 0.080000065), (7, 0.080000065), (8, 0.08000007), (9, 0.08000007)]\n",
      "727: [(0, 0.1384566), (1, 0.06153919), (2, 0.061539304), (3, 0.13846253), (4, 0.061539188), (5, 0.061539244), (6, 0.06153919), (7, 0.0615392), (8, 0.061539207), (9, 0.29230636)]\n",
      "728: [(0, 0.12857023), (1, 0.12856895), (2, 0.20000175), (3, 0.12857218), (4, 0.057143506), (5, 0.05714355), (6, 0.12856929), (7, 0.057143517), (8, 0.05714352), (9, 0.057143528)]\n",
      "729: [(0, 0.08888911), (1, 0.08888911), (2, 0.08888914), (3, 0.0888891), (4, 0.08888911), (5, 0.19999796), (6, 0.08888911), (7, 0.088889115), (8, 0.088889115), (9, 0.08888912)]\n",
      "730: [(0, 0.099998705), (1, 0.044445064), (2, 0.100000575), (3, 0.32222328), (4, 0.15555364), (5, 0.099998415), (6, 0.044445064), (7, 0.044445068), (8, 0.04444507), (9, 0.044445083)]\n",
      "731: [(0, 0.030769523), (1, 0.030769529), (2, 0.030769572), (3, 0.18461503), (4, 0.030769525), (5, 0.10769127), (6, 0.30000016), (7, 0.22307628), (8, 0.030769533), (9, 0.030769536)]\n",
      "732: [(0, 0.026667278), (1, 0.026667286), (2, 0.06000049), (3, 0.15999822), (4, 0.12666734), (5, 0.059998646), (6, 0.1266647), (7, 0.15999968), (8, 0.22666904), (9, 0.026667306)]\n",
      "733: [(0, 0.08888896), (1, 0.088888966), (2, 0.08888897), (3, 0.08888896), (4, 0.19999929), (5, 0.088888966), (6, 0.08888896), (7, 0.088888966), (8, 0.088888966), (9, 0.088888966)]\n",
      "734: [(0, 0.08888903), (1, 0.08888904), (2, 0.088889055), (3, 0.08888903), (4, 0.08888903), (5, 0.08888904), (6, 0.08888904), (7, 0.08888904), (8, 0.08888904), (9, 0.1999987)]\n",
      "735: [(0, 0.06666709), (1, 0.06666709), (2, 0.15000018), (3, 0.06666707), (4, 0.06666709), (5, 0.23333086), (6, 0.14999932), (7, 0.06666709), (8, 0.066667095), (9, 0.0666671)]\n",
      "0 {0: 0.08000048, 1: 0.08000049, 2: 0.080000564, 3: 0.080000475, 4: 0.08000048, 5: 0.08000052, 6: 0.08000049, 7: 0.0800005, 8: 0.2799955, 9: 0.080000505}\n",
      "1 {0: 0.057143595, 1: 0.12857138, 2: 0.12857297, 3: 0.19999672, 4: 0.057143595, 5: 0.05714365, 6: 0.0571436, 7: 0.057143614, 8: 0.19999726, 9: 0.05714363}\n",
      "2 {0: 0.053333893, 1: 0.0533339, 2: 0.053333987, 3: 0.12000056, 4: 0.053333897, 5: 0.053333938, 6: 0.0533339, 7: 0.25333062, 8: 0.25333136, 9: 0.053333923}\n",
      "3 {0: 0.08888917, 1: 0.08888917, 2: 0.19999745, 3: 0.08888916, 4: 0.08888917, 5: 0.08888919, 6: 0.08888917, 7: 0.088889174, 8: 0.088889174, 9: 0.08888918}\n",
      "4 {0: 0.07499685, 1: 0.20000008, 2: 0.0749997, 3: 0.15833752, 4: 0.03333436, 5: 0.1583345, 6: 0.033334367, 7: 0.1583316, 8: 0.07499665, 9: 0.033334404}\n",
      "5 {0: 0.114284866, 1: 0.019048514, 2: 0.019048648, 3: 0.06666253, 4: 0.066665046, 5: 0.04285642, 6: 0.18571873, 7: 0.3761922, 8: 0.09047446, 9: 0.019048546}\n",
      "6 {0: 0.12857011, 1: 0.12856984, 2: 0.057144865, 3: 0.12856841, 4: 0.12857138, 5: 0.057144724, 6: 0.1285738, 7: 0.12856755, 8: 0.05714464, 9: 0.057144668}\n",
      "7 {0: 0.08888903, 1: 0.08888904, 2: 0.088889055, 3: 0.08888903, 4: 0.08888903, 5: 0.08888904, 6: 0.08888904, 7: 0.08888904, 8: 0.08888904, 9: 0.1999987}\n",
      "8 {0: 0.08888917, 1: 0.08888917, 2: 0.19999745, 3: 0.08888916, 4: 0.08888917, 5: 0.08888919, 6: 0.08888917, 7: 0.088889174, 8: 0.088889174, 9: 0.08888918}\n",
      "9 {0: 0.040000964, 1: 0.08999582, 2: 0.090002194, 3: 0.18999758, 4: 0.040000964, 5: 0.13999864, 6: 0.040000968, 7: 0.09000125, 8: 0.09000118, 9: 0.19000039}\n",
      "10 {0: 0.08000018, 1: 0.08000018, 2: 0.27999842, 3: 0.08000017, 4: 0.08000018, 5: 0.080000184, 6: 0.08000018, 7: 0.08000018, 8: 0.08000018, 9: 0.08000018}\n",
      "11 {0: 0.029630495, 1: 0.0666648, 2: 0.029630635, 3: 0.3599722, 4: 0.10370344, 5: 0.06666609, 6: 0.029630503, 7: 0.1407405, 8: 0.06966484, 9: 0.103696495}\n",
      "12 {0: 0.15262662, 1: 0.07368315, 2: 0.12632348, 3: 0.100002974, 4: 0.12631446, 5: 0.07368311, 6: 0.047367483, 7: 0.04736975, 8: 0.07368074, 9: 0.17894827}\n",
      "13 {0: 0.08000046, 1: 0.08000047, 2: 0.08000054, 3: 0.08000045, 4: 0.08000046, 5: 0.0800005, 6: 0.17999855, 7: 0.080000475, 8: 0.17999762, 9: 0.08000049}\n",
      "14 {0: 0.105879754, 1: 0.16470788, 2: 0.04705976, 3: 0.16470593, 4: 0.047059633, 5: 0.047059692, 6: 0.1647041, 7: 0.105883256, 8: 0.04705965, 9: 0.1058803}\n",
      "15 {0: 0.14146034, 1: 0.04389886, 2: 0.043901335, 3: 0.06829458, 4: 0.14146625, 5: 0.04390033, 6: 0.0926837, 7: 0.068290465, 8: 0.19024867, 9: 0.16585544}\n",
      "16 {0: 0.14999734, 1: 0.23333101, 2: 0.06666749, 3: 0.066667356, 4: 0.06666738, 5: 0.06666743, 6: 0.066667385, 7: 0.06666739, 8: 0.0666674, 9: 0.1499998}\n",
      "17 {0: 0.03200091, 1: 0.0720011, 2: 0.032001056, 3: 0.071994245, 4: 0.03200091, 5: 0.1519978, 6: 0.032000918, 7: 0.23200096, 8: 0.07199848, 9: 0.2720036}\n",
      "18 {0: 0.054540467, 1: 0.11515254, 2: 0.20607007, 3: 0.11514865, 4: 0.14545643, 5: 0.11515087, 6: 0.08484564, 7: 0.11514877, 8: 0.024243264, 9: 0.024243278}\n",
      "19 {0: 0.13999644, 1: 0.29000092, 2: 0.14000249, 3: 0.09000171, 4: 0.08999622, 5: 0.040001206, 6: 0.08999756, 7: 0.040001143, 8: 0.04000115, 9: 0.04000117}\n",
      "20 {0: 0.22308469, 1: 0.0692139, 2: 0.030770635, 3: 0.14615709, 4: 0.06922603, 5: 0.14615668, 6: 0.06923247, 7: 0.18461755, 8: 0.030770473, 9: 0.030770496}\n",
      "21 {0: 0.21250036, 1: 0.02500122, 2: 0.056252908, 3: 0.025001178, 4: 0.087499976, 5: 0.118747346, 6: 0.11875251, 7: 0.11875091, 8: 0.11874232, 9: 0.11875129}\n",
      "22 {0: 0.07272767, 1: 0.07272767, 2: 0.1636369, 3: 0.07272766, 4: 0.2545417, 5: 0.0727277, 6: 0.07272767, 7: 0.07272768, 8: 0.07272768, 9: 0.072727695}\n",
      "23 {0: 0.03809587, 1: 0.18094927, 2: 0.13333574, 3: 0.085714675, 4: 0.2761912, 5: 0.038095918, 6: 0.038095877, 7: 0.085713215, 8: 0.038095888, 9: 0.08571231}\n",
      "24 {0: 0.14146423, 1: 0.33658952, 2: 0.04390048, 3: 0.23902832, 4: 0.043901395, 5: 0.04389849, 6: 0.04390029, 7: 0.019512698, 8: 0.019512702, 9: 0.06829186}\n",
      "25 {0: 0.049995143, 1: 0.07776624, 2: 0.077780336, 3: 0.050001457, 4: 0.3833483, 5: 0.05000067, 6: 0.10555541, 7: 0.077773914, 8: 0.022223284, 9: 0.10555527}\n",
      "26 {0: 0.03478367, 1: 0.16521896, 2: 0.034783836, 3: 0.0782623, 4: 0.07825363, 5: 0.0782608, 6: 0.07826138, 7: 0.078254685, 8: 0.339137, 9: 0.034783717}\n",
      "27 {0: 0.1368429, 1: 0.031573378, 2: 0.01403581, 3: 0.3649243, 4: 0.03157532, 5: 0.1368439, 6: 0.08421046, 7: 0.0666647, 8: 0.066661574, 9: 0.06666762}\n",
      "28 {0: 0.08888895, 1: 0.08888895, 2: 0.1999995, 3: 0.08888895, 4: 0.08888895, 5: 0.08888895, 6: 0.08888895, 7: 0.08888895, 8: 0.08888895, 9: 0.08888895}\n",
      "29 {0: 0.11999689, 1: 0.11999568, 2: 0.120004, 3: 0.11999613, 4: 0.12000153, 5: 0.053334914, 6: 0.053334814, 7: 0.12000012, 8: 0.12000107, 9: 0.053334866}\n",
      "30 {0: 0.09600125, 1: 0.21600415, 2: 0.01600069, 3: 0.095992535, 4: 0.095998436, 5: 0.01600064, 6: 0.1159989, 7: 0.016000608, 8: 0.055995267, 9: 0.2760075}\n",
      "31 {0: 0.075671546, 1: 0.07567719, 2: 0.075676635, 3: 0.21081421, 4: 0.21081457, 5: 0.048643634, 6: 0.02162234, 7: 0.102703005, 8: 0.048648212, 9: 0.12972863}\n",
      "32 {0: 0.1285716, 1: 0.12856857, 2: 0.12857509, 3: 0.057144243, 4: 0.12857084, 5: 0.05714439, 6: 0.057144288, 7: 0.12856771, 8: 0.12856893, 9: 0.05714434}\n",
      "33 {0: 0.06666802, 1: 0.14999849, 2: 0.06666824, 3: 0.06666799, 4: 0.06666802, 5: 0.06666812, 6: 0.14998853, 7: 0.06666805, 8: 0.23333648, 9: 0.06666808}\n",
      "34 {0: 0.16363636, 1: 0.072728194, 2: 0.072728336, 3: 0.07272816, 4: 0.07272819, 5: 0.072728254, 6: 0.25453788, 7: 0.0727282, 8: 0.07272821, 9: 0.072728224}\n",
      "35 {0: 0.08888955, 1: 0.088889554, 2: 0.08888965, 3: 0.088889524, 4: 0.08888955, 5: 0.0888896, 6: 0.088889554, 7: 0.19999391, 8: 0.08888957, 9: 0.08888958}\n",
      "36 {0: 0.08888895, 1: 0.08888895, 2: 0.088888966, 3: 0.08888895, 4: 0.1999994, 5: 0.08888896, 6: 0.08888895, 7: 0.08888895, 8: 0.08888895, 9: 0.08888895}\n",
      "37 {0: 0.08888896, 1: 0.08888896, 2: 0.08888897, 3: 0.1999993, 4: 0.08888896, 5: 0.088888966, 6: 0.08888896, 7: 0.088888966, 8: 0.088888966, 9: 0.088888966}\n",
      "38 {0: 0.04210609, 1: 0.09473316, 2: 0.14736997, 3: 0.094737254, 4: 0.09473417, 5: 0.04210615, 6: 0.0421061, 7: 0.30526602, 8: 0.04210611, 9: 0.09473497}\n",
      "39 {0: 0.1263181, 1: 0.047368318, 2: 0.02105353, 3: 0.07367495, 4: 0.17895205, 5: 0.04736763, 6: 0.099999726, 7: 0.25790063, 8: 0.12631159, 9: 0.021053443}\n",
      "40 {0: 0.08888917, 1: 0.08888917, 2: 0.19999745, 3: 0.08888916, 4: 0.08888917, 5: 0.08888919, 6: 0.08888917, 7: 0.088889174, 8: 0.088889174, 9: 0.08888918}\n",
      "41 {0: 0.0844438, 1: 0.039999977, 2: 0.039997526, 3: 0.084442645, 4: 0.06222293, 5: 0.106664926, 6: 0.06222134, 7: 0.03999694, 8: 0.039999865, 9: 0.44001007}\n",
      "42 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "43 {0: 0.18095265, 1: 0.32381365, 2: 0.038096275, 3: 0.08571546, 4: 0.038096134, 5: 0.038096197, 6: 0.038096137, 7: 0.03809615, 8: 0.1809412, 9: 0.03809617}\n",
      "44 {0: 0.22564457, 1: 0.1487166, 2: 0.02051363, 3: 0.071796596, 4: 0.12307848, 5: 0.07179252, 6: 0.04615151, 7: 0.020513535, 8: 0.20000274, 9: 0.071789764}\n",
      "45 {0: 0.13333279, 1: 0.1333309, 2: 0.03809639, 3: 0.1333307, 4: 0.22857374, 5: 0.03809631, 6: 0.08571208, 7: 0.13333453, 8: 0.038096257, 9: 0.03809628}\n",
      "46 {0: 0.08888917, 1: 0.08888917, 2: 0.08888921, 3: 0.08888916, 4: 0.08888917, 5: 0.08888919, 6: 0.08888917, 7: 0.088889174, 8: 0.19999741, 9: 0.08888918}\n",
      "47 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "48 {0: 0.08000052, 1: 0.17999515, 2: 0.18000057, 3: 0.080000505, 4: 0.08000053, 5: 0.080000564, 6: 0.08000053, 7: 0.080000535, 8: 0.080000535, 9: 0.08000054}\n",
      "49 {0: 0.080000445, 1: 0.080000445, 2: 0.08000052, 3: 0.08000044, 4: 0.080000445, 5: 0.17999685, 6: 0.080000445, 7: 0.08000045, 8: 0.08000046, 9: 0.17999947}\n",
      "50 {0: 0.08888895, 1: 0.08888895, 2: 0.1999995, 3: 0.08888895, 4: 0.08888895, 5: 0.08888895, 6: 0.08888895, 7: 0.08888895, 8: 0.08888895, 9: 0.08888895}\n",
      "51 {0: 0.080000244, 1: 0.080000244, 2: 0.08000028, 3: 0.08000024, 4: 0.080000244, 5: 0.27999774, 6: 0.080000244, 7: 0.080000244, 8: 0.08000025, 9: 0.08000025}\n",
      "52 {0: 0.06666819, 1: 0.14999954, 2: 0.06666843, 3: 0.066668145, 4: 0.14999668, 5: 0.0666683, 6: 0.0666682, 7: 0.06666822, 8: 0.23332603, 9: 0.06666826}\n",
      "53 {0: 0.23333287, 1: 0.06666716, 2: 0.14999782, 3: 0.06666715, 4: 0.06666716, 5: 0.06666719, 6: 0.06666716, 7: 0.06666717, 8: 0.06666717, 9: 0.14999917}\n",
      "54 {0: 0.08888925, 1: 0.08888925, 2: 0.08888931, 3: 0.08888924, 4: 0.08888925, 5: 0.08888928, 6: 0.08888925, 7: 0.19999671, 8: 0.088889256, 9: 0.08888926}\n",
      "55 {0: 0.053334553, 1: 0.18666771, 2: 0.053334747, 3: 0.18666416, 4: 0.053334553, 5: 0.119996734, 6: 0.12000103, 7: 0.053334575, 8: 0.1199973, 9: 0.053334605}\n",
      "56 {0: 0.08000091, 1: 0.08000092, 2: 0.080001056, 3: 0.1558035, 4: 0.08000091, 5: 0.17999786, 6: 0.080000915, 7: 0.08000093, 8: 0.104192026, 9: 0.08000095}\n",
      "57 {0: 0.19999747, 1: 0.08888917, 2: 0.088889204, 3: 0.08888916, 4: 0.08888917, 5: 0.08888919, 6: 0.08888917, 7: 0.088889174, 8: 0.088889174, 9: 0.088889174}\n",
      "58 {0: 0.08888919, 1: 0.08888919, 2: 0.08888924, 3: 0.08888918, 4: 0.08888919, 5: 0.08888921, 6: 0.08888919, 7: 0.0888892, 8: 0.0888892, 9: 0.19999723}\n",
      "59 {0: 0.08000033, 1: 0.08000033, 2: 0.080000386, 3: 0.1799994, 4: 0.08000033, 5: 0.17999789, 6: 0.08000033, 7: 0.08000034, 8: 0.08000034, 9: 0.08000034}\n",
      "60 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "61 {0: 0.08000063, 1: 0.08000063, 2: 0.080000736, 3: 0.08000061, 4: 0.08000063, 5: 0.080000676, 6: 0.17999637, 7: 0.1799984, 8: 0.08000065, 9: 0.08000066}\n",
      "62 {0: 0.17999722, 1: 0.17999779, 2: 0.0800007, 3: 0.08000059, 4: 0.0800006, 5: 0.08000065, 6: 0.0800006, 7: 0.08000061, 8: 0.08000062, 9: 0.08000063}\n",
      "63 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "64 {0: 0.27999735, 1: 0.08000028, 2: 0.080000326, 3: 0.080000274, 4: 0.08000028, 5: 0.0800003, 6: 0.08000028, 7: 0.08000029, 8: 0.08000029, 9: 0.080000296}\n",
      "65 {0: 0.080000825, 1: 0.080000825, 2: 0.08000095, 3: 0.080000795, 4: 0.080000825, 5: 0.1799938, 6: 0.080000825, 7: 0.17999947, 8: 0.08000085, 9: 0.080000855}\n",
      "66 {0: 0.06666762, 1: 0.23333082, 2: 0.06666777, 3: 0.066667594, 4: 0.06666762, 5: 0.06666769, 6: 0.14999756, 7: 0.14999801, 8: 0.066667646, 9: 0.06666766}\n",
      "67 {0: 0.088889204, 1: 0.088889204, 2: 0.088889256, 3: 0.0888892, 4: 0.088889204, 5: 0.08888923, 6: 0.088889204, 7: 0.08888921, 8: 0.08888921, 9: 0.19999708}\n",
      "68 {0: 0.17999734, 1: 0.08000028, 2: 0.18000035, 3: 0.080000274, 4: 0.08000028, 5: 0.0800003, 6: 0.08000028, 7: 0.08000029, 8: 0.08000029, 9: 0.080000296}\n",
      "69 {0: 0.08000031, 1: 0.08000032, 2: 0.17999758, 3: 0.17999983, 4: 0.08000031, 5: 0.08000033, 6: 0.08000032, 7: 0.08000032, 8: 0.08000032, 9: 0.080000326}\n",
      "70 {0: 0.08888895, 1: 0.08888895, 2: 0.088888966, 3: 0.08888895, 4: 0.1999994, 5: 0.08888896, 6: 0.08888895, 7: 0.08888895, 8: 0.08888895, 9: 0.08888895}\n",
      "71 {0: 0.0727279, 1: 0.0727279, 2: 0.07272801, 3: 0.16363713, 4: 0.0727279, 5: 0.07272795, 6: 0.0727279, 7: 0.16363345, 8: 0.07272792, 9: 0.16363394}\n",
      "72 {0: 0.08000017, 1: 0.08000017, 2: 0.08000019, 3: 0.17999938, 4: 0.08000017, 5: 0.08000018, 6: 0.17999923, 7: 0.08000017, 8: 0.08000017, 9: 0.08000017}\n",
      "73 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "74 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "75 {0: 0.08888903, 1: 0.08888904, 2: 0.088889055, 3: 0.08888903, 4: 0.1999987, 5: 0.08888904, 6: 0.08888904, 7: 0.08888904, 8: 0.08888904, 9: 0.08888904}\n",
      "76 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "77 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "78 {0: 0.12857293, 1: 0.05714392, 2: 0.05714408, 3: 0.057143882, 4: 0.12856996, 5: 0.12856966, 6: 0.19999473, 7: 0.057143934, 8: 0.12857294, 9: 0.05714396}\n",
      "79 {0: 0.4638446, 1: 0.017022016, 2: 0.03829955, 3: 0.0382916, 4: 0.10213172, 5: 0.080850326, 6: 0.03829424, 7: 0.14468016, 8: 0.038292855, 9: 0.038292967}\n",
      "80 {0: 0.072727926, 1: 0.07272793, 2: 0.07272804, 3: 0.25454262, 4: 0.072727926, 5: 0.07272798, 6: 0.16363372, 7: 0.07272794, 8: 0.07272795, 9: 0.072727956}\n",
      "81 {0: 0.28387725, 1: 0.09032037, 2: 0.09032733, 3: 0.12258391, 4: 0.05806288, 5: 0.09032094, 6: 0.058063958, 7: 0.05806466, 8: 0.090314955, 9: 0.05806378}\n",
      "82 {0: 0.179998, 1: 0.08000052, 2: 0.080000594, 3: 0.0800005, 4: 0.08000051, 5: 0.08000056, 6: 0.08000052, 7: 0.08000053, 8: 0.08000053, 9: 0.17999773}\n",
      "83 {0: 0.08888917, 1: 0.08888917, 2: 0.19999745, 3: 0.08888916, 4: 0.08888917, 5: 0.08888919, 6: 0.08888917, 7: 0.088889174, 8: 0.088889174, 9: 0.08888918}\n",
      "84 {0: 0.088889614, 1: 0.08888962, 2: 0.08888973, 3: 0.0888896, 4: 0.088889614, 5: 0.08888967, 6: 0.08888962, 7: 0.088889636, 8: 0.19999318, 9: 0.08888965}\n",
      "85 {0: 0.10588084, 1: 0.047059804, 2: 0.047059953, 3: 0.04705977, 4: 0.28235215, 5: 0.047059868, 6: 0.0470598, 7: 0.10588204, 8: 0.105879135, 9: 0.16470668}\n",
      "86 {0: 0.06222284, 1: 0.15110897, 2: 0.017778497, 3: 0.040000644, 4: 0.04000069, 5: 0.039996818, 6: 0.41778815, 7: 0.08444162, 8: 0.10666437, 9: 0.03999744}\n",
      "87 {0: 0.107685745, 1: 0.030769808, 2: 0.06923197, 3: 0.2230802, 4: 0.06923118, 5: 0.030769845, 6: 0.030769806, 7: 0.06922594, 8: 0.33846566, 9: 0.030769827}\n",
      "88 {0: 0.1555565, 1: 0.26666525, 2: 0.0444455, 3: 0.044445332, 4: 0.10000146, 5: 0.04444542, 6: 0.1555532, 7: 0.04444537, 8: 0.099996574, 9: 0.044445395}\n",
      "89 {0: 0.08888925, 1: 0.08888925, 2: 0.08888931, 3: 0.08888924, 4: 0.08888925, 5: 0.08888927, 6: 0.19999671, 7: 0.08888925, 8: 0.088889256, 9: 0.08888926}\n",
      "90 {0: 0.08000013, 1: 0.08000013, 2: 0.17999937, 3: 0.17999955, 4: 0.08000013, 5: 0.08000014, 6: 0.08000013, 7: 0.08000013, 8: 0.08000013, 9: 0.08000014}\n",
      "91 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "92 {0: 0.038096093, 1: 0.085710295, 2: 0.085713334, 3: 0.22857614, 4: 0.1333339, 5: 0.03809616, 6: 0.085710675, 7: 0.03809611, 8: 0.22857112, 9: 0.038096134}\n",
      "93 {0: 0.06153938, 1: 0.13845809, 2: 0.061539527, 3: 0.13846111, 4: 0.06153938, 5: 0.06153945, 6: 0.13846113, 7: 0.13846278, 8: 0.061539404, 9: 0.13845974}\n",
      "94 {0: 0.08888917, 1: 0.08888917, 2: 0.19999745, 3: 0.08888916, 4: 0.08888917, 5: 0.08888919, 6: 0.08888917, 7: 0.088889174, 8: 0.088889174, 9: 0.08888918}\n",
      "95 {0: 0.02963004, 1: 0.06666346, 2: 0.029630106, 3: 0.02963003, 4: 0.02963004, 5: 0.1777779, 6: 0.25185055, 7: 0.02963005, 8: 0.029630052, 9: 0.32592776}\n",
      "96 {0: 0.07777395, 1: 0.13333401, 2: 0.16111663, 3: 0.022222927, 4: 0.16111, 5: 0.0777758, 6: 0.077774696, 7: 0.21666914, 8: 0.022222964, 9: 0.049999915}\n",
      "97 {0: 0.03461401, 1: 0.111535914, 2: 0.053846676, 3: 0.130768, 4: 0.073075235, 5: 0.30385998, 6: 0.053843353, 7: 0.16923368, 8: 0.034610253, 9: 0.03461288}\n",
      "98 {0: 0.08888917, 1: 0.08888917, 2: 0.19999745, 3: 0.08888916, 4: 0.08888917, 5: 0.08888919, 6: 0.08888917, 7: 0.088889174, 8: 0.088889174, 9: 0.08888918}\n",
      "99 {0: 0.08888895, 1: 0.08888895, 2: 0.088888966, 3: 0.08888895, 4: 0.1999994, 5: 0.08888896, 6: 0.08888895, 7: 0.08888895, 8: 0.08888895, 9: 0.08888895}\n",
      "100 {0: 0.08888917, 1: 0.08888917, 2: 0.19999745, 3: 0.08888916, 4: 0.08888917, 5: 0.08888919, 6: 0.08888917, 7: 0.088889174, 8: 0.088889174, 9: 0.08888918}\n",
      "101 {0: 0.08889004, 1: 0.088890046, 2: 0.088890225, 3: 0.08889001, 4: 0.08889004, 5: 0.08889013, 6: 0.088890046, 7: 0.08889006, 8: 0.08889007, 9: 0.19998933}\n",
      "102 {0: 0.06666778, 1: 0.06666779, 2: 0.1500027, 3: 0.06666775, 4: 0.06666778, 5: 0.06666786, 6: 0.06666779, 7: 0.14999439, 8: 0.1499975, 9: 0.1499986}\n",
      "103 {0: 0.19999516, 1: 0.088889405, 2: 0.08888949, 3: 0.0888894, 4: 0.088889405, 5: 0.08888945, 6: 0.088889405, 7: 0.08888941, 8: 0.08888942, 9: 0.08888943}\n",
      "104 {0: 0.072727785, 1: 0.07272779, 2: 0.16363744, 3: 0.07272777, 4: 0.072727785, 5: 0.07272782, 6: 0.072727785, 7: 0.16363245, 8: 0.0727278, 9: 0.1636356}\n",
      "105 {0: 0.14999661, 1: 0.06666744, 2: 0.15000185, 3: 0.06666741, 4: 0.06666743, 5: 0.06666749, 6: 0.14999692, 7: 0.06666745, 8: 0.06666745, 9: 0.1499999}\n",
      "106 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "107 {0: 0.088889204, 1: 0.088889204, 2: 0.088889256, 3: 0.0888892, 4: 0.088889204, 5: 0.08888923, 6: 0.19999708, 7: 0.08888921, 8: 0.08888921, 9: 0.08888921}\n",
      "108 {0: 0.06666753, 1: 0.31666452, 2: 0.06666766, 3: 0.149995, 4: 0.06666753, 5: 0.06666759, 6: 0.06666753, 7: 0.06666754, 8: 0.06666755, 9: 0.066667564}\n",
      "109 {0: 0.08000052, 1: 0.17999808, 2: 0.08000061, 3: 0.080000505, 4: 0.08000053, 5: 0.080000564, 6: 0.08000053, 7: 0.17999758, 8: 0.080000535, 9: 0.08000054}\n",
      "110 {0: 0.053333957, 1: 0.2533331, 2: 0.053334057, 3: 0.12000072, 4: 0.053333957, 5: 0.053334, 6: 0.05333396, 7: 0.053333968, 8: 0.18666358, 9: 0.11999866}\n",
      "111 {0: 0.07272747, 1: 0.07272747, 2: 0.25454432, 3: 0.072727464, 4: 0.1636359, 5: 0.07272748, 6: 0.07272747, 7: 0.07272747, 8: 0.07272747, 9: 0.07272747}\n",
      "112 {0: 0.13846195, 1: 0.13845706, 2: 0.06153937, 3: 0.06153922, 4: 0.061539248, 5: 0.061539307, 6: 0.06153925, 7: 0.29230604, 8: 0.061539263, 9: 0.061539277}\n",
      "113 {0: 0.08888938, 1: 0.08888938, 2: 0.088889465, 3: 0.08888937, 4: 0.08888938, 5: 0.08888941, 6: 0.08888938, 7: 0.08888939, 8: 0.19999546, 9: 0.088889405}\n",
      "114 {0: 0.08000035, 1: 0.080000356, 2: 0.08000041, 3: 0.08000034, 4: 0.08000035, 5: 0.08000038, 6: 0.080000356, 7: 0.27999678, 8: 0.080000356, 9: 0.08000036}\n",
      "115 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "116 {0: 0.088889, 1: 0.088889, 2: 0.08888902, 3: 0.19999892, 4: 0.088889, 5: 0.08888901, 6: 0.088889, 7: 0.088889, 8: 0.088889, 9: 0.088889}\n",
      "117 {0: 0.08888917, 1: 0.08888917, 2: 0.19999745, 3: 0.08888916, 4: 0.08888917, 5: 0.08888919, 6: 0.08888917, 7: 0.088889174, 8: 0.088889174, 9: 0.08888918}\n",
      "118 {0: 0.08000026, 1: 0.08000027, 2: 0.08000031, 3: 0.08000025, 4: 0.08000026, 5: 0.08000028, 6: 0.27999753, 7: 0.08000027, 8: 0.080000274, 9: 0.080000274}\n",
      "119 {0: 0.053333625, 1: 0.05333363, 2: 0.053333674, 3: 0.11999923, 4: 0.31999955, 5: 0.053333648, 6: 0.05333363, 7: 0.18666576, 8: 0.053333633, 9: 0.05333364}\n",
      "120 {0: 0.07272781, 1: 0.072727814, 2: 0.16363697, 3: 0.16363621, 4: 0.07272781, 5: 0.07272785, 6: 0.072727814, 7: 0.16363208, 8: 0.07272782, 9: 0.07272783}\n",
      "121 {0: 0.1384601, 1: 0.06153924, 2: 0.061539356, 3: 0.061539214, 4: 0.2153842, 5: 0.061539292, 6: 0.061539236, 7: 0.1384608, 8: 0.061539255, 9: 0.1384593}\n",
      "122 {0: 0.06666706, 1: 0.066667065, 2: 0.066667125, 3: 0.06666705, 4: 0.066667065, 5: 0.3166656, 6: 0.14999785, 7: 0.06666707, 8: 0.06666707, 9: 0.06666708}\n",
      "123 {0: 0.061539095, 1: 0.13845623, 2: 0.0615392, 3: 0.3692308, 4: 0.061539095, 5: 0.061539143, 6: 0.061539102, 7: 0.06153911, 8: 0.061539114, 9: 0.061539125}\n",
      "124 {0: 0.053333905, 1: 0.119997546, 2: 0.053333994, 3: 0.120000295, 4: 0.053333905, 5: 0.05333395, 6: 0.18666537, 7: 0.120000295, 8: 0.05333392, 9: 0.18666685}\n",
      "125 {0: 0.10768507, 1: 0.030770173, 2: 0.10769623, 3: 0.10769249, 4: 0.22307895, 5: 0.06922845, 6: 0.22308037, 7: 0.030770184, 8: 0.06922792, 9: 0.030770207}\n",
      "126 {0: 0.072727606, 1: 0.072727606, 2: 0.07272765, 3: 0.07272759, 4: 0.072727606, 5: 0.16363464, 6: 0.2545445, 7: 0.072727606, 8: 0.072727606, 9: 0.07272761}\n",
      "127 {0: 0.08000305, 1: 0.08000308, 2: 0.08000355, 3: 0.08000297, 4: 0.17996874, 5: 0.08000329, 6: 0.18000582, 7: 0.08000312, 8: 0.08000314, 9: 0.080003195}\n",
      "128 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "129 {0: 0.072727896, 1: 0.1636342, 2: 0.072728, 3: 0.07272788, 4: 0.072727896, 5: 0.16363615, 6: 0.0727279, 7: 0.07272791, 8: 0.16363424, 9: 0.072727926}\n",
      "130 {0: 0.17499624, 1: 0.050000537, 2: 0.05000062, 3: 0.112500496, 4: 0.050000533, 5: 0.050000574, 6: 0.050000537, 7: 0.050000545, 8: 0.36249936, 9: 0.050000556}\n",
      "131 {0: 0.050000895, 1: 0.2375001, 2: 0.050001036, 3: 0.05000087, 4: 0.050000895, 5: 0.050000962, 6: 0.11249544, 7: 0.112498194, 8: 0.050000917, 9: 0.23750067}\n",
      "132 {0: 0.34544593, 1: 0.07272821, 2: 0.07272835, 3: 0.07272817, 4: 0.0727282, 5: 0.07272827, 6: 0.07272821, 7: 0.07272822, 8: 0.072728224, 9: 0.07272824}\n",
      "133 {0: 0.17999509, 1: 0.08000056, 2: 0.08000065, 3: 0.080000535, 4: 0.08000056, 5: 0.080000594, 6: 0.08000056, 7: 0.18000033, 8: 0.080000564, 9: 0.08000058}\n",
      "134 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "135 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "136 {0: 0.057143405, 1: 0.05714341, 2: 0.05714349, 3: 0.057143386, 4: 0.12856828, 5: 0.12856975, 6: 0.05714341, 7: 0.057143413, 8: 0.34285802, 9: 0.057143427}\n",
      "137 {0: 0.08888925, 1: 0.08888925, 2: 0.08888931, 3: 0.08888924, 4: 0.08888925, 5: 0.19999664, 6: 0.08888925, 7: 0.088889256, 8: 0.088889256, 9: 0.08888926}\n",
      "138 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "139 {0: 0.080000475, 1: 0.08000048, 2: 0.08000056, 3: 0.18000025, 4: 0.080000475, 5: 0.08000051, 6: 0.080000475, 7: 0.08000049, 8: 0.08000049, 9: 0.17999578}\n",
      "140 {0: 0.1799931, 1: 0.08000082, 2: 0.080000944, 3: 0.08000079, 4: 0.08000082, 5: 0.08000088, 6: 0.18000014, 7: 0.08000083, 8: 0.08000083, 9: 0.08000085}\n",
      "141 {0: 0.05333362, 1: 0.053333625, 2: 0.05333367, 3: 0.053333614, 4: 0.05333362, 5: 0.053333644, 6: 0.053333625, 7: 0.05333363, 8: 0.05333363, 9: 0.5199973}\n",
      "142 {0: 0.14999303, 1: 0.06666743, 2: 0.23333654, 3: 0.06666741, 4: 0.06666742, 5: 0.06666748, 6: 0.06666743, 7: 0.06666744, 8: 0.066667445, 9: 0.14999834}\n",
      "143 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "144 {0: 0.08000012, 1: 0.08000012, 2: 0.17999965, 3: 0.08000012, 4: 0.17999937, 5: 0.080000125, 6: 0.08000012, 7: 0.080000125, 8: 0.080000125, 9: 0.080000125}\n",
      "145 {0: 0.07272753, 1: 0.07272753, 2: 0.34545222, 3: 0.072727524, 4: 0.07272753, 5: 0.072727546, 6: 0.07272753, 7: 0.07272754, 8: 0.07272754, 9: 0.07272754}\n",
      "146 {0: 0.08888893, 1: 0.08888893, 2: 0.19999963, 3: 0.08888893, 4: 0.08888893, 5: 0.08888893, 6: 0.08888893, 7: 0.08888893, 8: 0.08888893, 9: 0.08888893}\n",
      "147 {0: 0.058060605, 1: 0.025806952, 2: 0.025807029, 3: 0.02697258, 4: 0.025806949, 5: 0.058060743, 6: 0.025806952, 7: 0.0903175, 8: 0.63755375, 9: 0.02580697}\n",
      "148 {0: 0.08888938, 1: 0.08888938, 2: 0.088889465, 3: 0.08888937, 4: 0.08888938, 5: 0.08888941, 6: 0.08888938, 7: 0.08888939, 8: 0.19999543, 9: 0.088889405}\n",
      "149 {0: 0.08888932, 1: 0.08888932, 2: 0.0888894, 3: 0.08888931, 4: 0.08888932, 5: 0.08888935, 6: 0.08888932, 7: 0.08888933, 8: 0.19999595, 9: 0.088889346}\n",
      "150 {0: 0.06666739, 1: 0.0666674, 2: 0.066667505, 3: 0.1499991, 4: 0.06666739, 5: 0.2333309, 6: 0.0666674, 7: 0.06666741, 8: 0.14999807, 9: 0.06666742}\n",
      "151 {0: 0.053334165, 1: 0.18666178, 2: 0.25333506, 3: 0.053334147, 4: 0.11999773, 5: 0.05333423, 6: 0.053334173, 7: 0.053334188, 8: 0.05333419, 9: 0.120000325}\n",
      "152 {0: 0.061538815, 1: 0.06153882, 2: 0.13846193, 3: 0.061538808, 4: 0.06153882, 5: 0.13845812, 6: 0.29230818, 7: 0.061538827, 8: 0.06153883, 9: 0.061538834}\n",
      "153 {0: 0.072728135, 1: 0.07272814, 2: 0.072728276, 3: 0.07272811, 4: 0.07272814, 5: 0.0727282, 6: 0.07272814, 7: 0.07272816, 8: 0.25453952, 9: 0.16363516}\n",
      "154 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "155 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "156 {0: 0.13845596, 1: 0.06153969, 2: 0.061539873, 3: 0.21538134, 4: 0.06153968, 5: 0.061539773, 6: 0.21538453, 7: 0.061539702, 8: 0.061539713, 9: 0.061539732}\n",
      "157 {0: 0.0533341, 1: 0.05333411, 2: 0.053334225, 3: 0.18666549, 4: 0.0533341, 5: 0.053334158, 6: 0.31999886, 7: 0.053334117, 8: 0.11999673, 9: 0.05333413}\n",
      "158 {0: 0.08888896, 1: 0.088888966, 2: 0.08888897, 3: 0.08888896, 4: 0.19999929, 5: 0.088888966, 6: 0.08888896, 7: 0.088888966, 8: 0.088888966, 9: 0.088888966}\n",
      "159 {0: 0.20000024, 1: 0.042105857, 2: 0.09473812, 3: 0.19612087, 4: 0.042105854, 5: 0.0421059, 6: 0.14736882, 7: 0.14736623, 8: 0.045982257, 9: 0.04210588}\n",
      "160 {0: 0.11147374, 1: 0.22623536, 2: 0.029509187, 3: 0.09508202, 4: 0.09508234, 5: 0.013115382, 6: 0.19344798, 7: 0.111475326, 8: 0.045893423, 9: 0.07868523}\n",
      "161 {0: 0.08000039, 1: 0.08000039, 2: 0.08000045, 3: 0.08000038, 4: 0.18000007, 5: 0.08000042, 6: 0.17999673, 7: 0.08000039, 8: 0.08000039, 9: 0.08000041}\n",
      "162 {0: 0.053333815, 1: 0.05333382, 2: 0.053333897, 3: 0.25333196, 4: 0.053333815, 5: 0.18666455, 6: 0.18666665, 7: 0.05333383, 8: 0.05333383, 9: 0.05333384}\n",
      "163 {0: 0.088889204, 1: 0.088889204, 2: 0.088889256, 3: 0.0888892, 4: 0.088889204, 5: 0.08888923, 6: 0.088889204, 7: 0.1999971, 8: 0.08888921, 9: 0.08888921}\n",
      "164 {0: 0.061538797, 1: 0.0615388, 2: 0.061538853, 3: 0.13846157, 4: 0.061538797, 5: 0.061538827, 6: 0.0615388, 7: 0.13846125, 8: 0.061538808, 9: 0.2923055}\n",
      "165 {0: 0.07272772, 1: 0.16363642, 2: 0.16363724, 3: 0.07272771, 4: 0.07272772, 5: 0.072727755, 6: 0.072727725, 7: 0.07272773, 8: 0.16363223, 9: 0.07272774}\n",
      "166 {0: 0.16363542, 1: 0.07272761, 2: 0.072727665, 3: 0.16363628, 4: 0.16363491, 5: 0.072727636, 6: 0.07272761, 7: 0.07272761, 8: 0.07272762, 9: 0.07272763}\n",
      "167 {0: 0.072732754, 1: 0.16364801, 2: 0.07273364, 3: 0.072732605, 4: 0.25448826, 5: 0.07273317, 6: 0.0727328, 7: 0.072732866, 8: 0.0727329, 9: 0.072733}\n",
      "168 {0: 0.08888925, 1: 0.08888925, 2: 0.08888931, 3: 0.08888924, 4: 0.08888925, 5: 0.08888928, 6: 0.08888925, 7: 0.088889256, 8: 0.1999967, 9: 0.08888926}\n",
      "169 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "170 {0: 0.061539385, 1: 0.061539397, 2: 0.06153954, 3: 0.13845472, 4: 0.1384629, 5: 0.2153865, 6: 0.061539397, 7: 0.13845935, 8: 0.061539408, 9: 0.061539426}\n",
      "171 {0: 0.31666017, 1: 0.06666737, 2: 0.066667475, 3: 0.06666735, 4: 0.06666737, 5: 0.066667415, 6: 0.06666737, 7: 0.15000068, 8: 0.066667385, 9: 0.0666674}\n",
      "172 {0: 0.08000036, 1: 0.17999987, 2: 0.08000042, 3: 0.08000035, 4: 0.08000036, 5: 0.08000039, 6: 0.08000036, 7: 0.17999712, 8: 0.08000036, 9: 0.08000038}\n",
      "173 {0: 0.0666675, 1: 0.14999823, 2: 0.06666763, 3: 0.06666748, 4: 0.14999774, 5: 0.066667564, 6: 0.066667505, 7: 0.06666752, 8: 0.23333134, 9: 0.066667534}\n",
      "174 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "175 {0: 0.08888897, 1: 0.08888897, 2: 0.08888899, 3: 0.08888897, 4: 0.08888897, 5: 0.08888898, 6: 0.08888897, 7: 0.19999915, 8: 0.08888897, 9: 0.08888897}\n",
      "176 {0: 0.07272751, 1: 0.07272751, 2: 0.16363658, 3: 0.2545433, 4: 0.07272751, 5: 0.072727524, 6: 0.07272751, 7: 0.07272751, 8: 0.07272751, 9: 0.07272752}\n",
      "177 {0: 0.099997714, 1: 0.04444541, 2: 0.044445552, 3: 0.21111463, 4: 0.044445403, 5: 0.09999785, 6: 0.21111088, 7: 0.099997945, 8: 0.044445425, 9: 0.09999926}\n",
      "178 {0: 0.17999978, 1: 0.080000356, 2: 0.08000041, 3: 0.08000034, 4: 0.1799973, 5: 0.08000038, 6: 0.080000356, 7: 0.080000356, 8: 0.080000356, 9: 0.08000036}\n",
      "179 {0: 0.08000149, 1: 0.0800015, 2: 0.080001734, 3: 0.08000145, 4: 0.17999102, 5: 0.08000161, 6: 0.0800015, 7: 0.08000152, 8: 0.08000153, 9: 0.17999658}\n",
      "180 {0: 0.0666675, 1: 0.066667505, 2: 0.14999913, 3: 0.15000053, 4: 0.0666675, 5: 0.066667564, 6: 0.066667505, 7: 0.06666751, 8: 0.14999652, 9: 0.1499988}\n",
      "181 {0: 0.0666675, 1: 0.06666751, 2: 0.06666763, 3: 0.14999734, 4: 0.14999746, 5: 0.066667564, 6: 0.066667505, 7: 0.06666752, 8: 0.06666753, 9: 0.23333243}\n",
      "182 {0: 0.08000016, 1: 0.08000016, 2: 0.17999925, 3: 0.080000155, 4: 0.17999946, 5: 0.08000017, 6: 0.08000016, 7: 0.08000016, 8: 0.08000016, 9: 0.08000016}\n",
      "183 {0: 0.07825786, 1: 0.07825905, 2: 0.034784008, 3: 0.16521893, 4: 0.034783814, 5: 0.0782601, 6: 0.12173961, 7: 0.16522053, 8: 0.121735625, 9: 0.12174048}\n",
      "184 {0: 0.13845402, 1: 0.13846274, 2: 0.061540075, 3: 0.06153981, 4: 0.13846055, 5: 0.061539955, 6: 0.061539862, 7: 0.13846213, 8: 0.13846093, 9: 0.06153991}\n",
      "185 {0: 0.08888954, 1: 0.19999395, 2: 0.08888964, 3: 0.08888952, 4: 0.08888954, 5: 0.088889584, 6: 0.08888955, 7: 0.088889554, 8: 0.088889554, 9: 0.08888956}\n",
      "186 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "187 {0: 0.088889204, 1: 0.088889204, 2: 0.088889256, 3: 0.0888892, 4: 0.088889204, 5: 0.08888923, 6: 0.088889204, 7: 0.08888921, 8: 0.08888921, 9: 0.19999708}\n",
      "188 {0: 0.116664246, 1: 0.32499865, 2: 0.03333402, 3: 0.03333391, 4: 0.24166937, 5: 0.03333397, 6: 0.03333393, 7: 0.03333394, 8: 0.03333394, 9: 0.11666402}\n",
      "189 {0: 0.08000134, 1: 0.17999552, 2: 0.08000156, 3: 0.08000131, 4: 0.08000135, 5: 0.08000145, 6: 0.080001354, 7: 0.08000137, 8: 0.17999338, 9: 0.0800014}\n",
      "190 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "191 {0: 0.06666693, 1: 0.06666693, 2: 0.23333378, 3: 0.14999913, 4: 0.14999853, 5: 0.066666946, 6: 0.06666693, 7: 0.06666693, 8: 0.06666693, 9: 0.06666694}\n",
      "192 {0: 0.07272776, 1: 0.16363253, 2: 0.16363737, 3: 0.07272775, 4: 0.07272776, 5: 0.16363573, 6: 0.07272777, 7: 0.07272777, 8: 0.07272777, 9: 0.072727785}\n",
      "193 {0: 0.29230624, 1: 0.06153968, 2: 0.13846166, 3: 0.06153964, 4: 0.061539672, 5: 0.06153976, 6: 0.06153968, 7: 0.0615397, 8: 0.13845421, 9: 0.06153973}\n",
      "194 {0: 0.1799952, 1: 0.0800005, 2: 0.18000083, 3: 0.08000048, 4: 0.08000049, 5: 0.080000535, 6: 0.0800005, 7: 0.080000505, 8: 0.080000505, 9: 0.08000051}\n",
      "195 {0: 0.05333399, 1: 0.12000054, 2: 0.0533341, 3: 0.12000052, 4: 0.25333184, 5: 0.11999982, 6: 0.053333998, 7: 0.05333401, 8: 0.11999713, 9: 0.053334024}\n",
      "196 {0: 0.08000023, 1: 0.08000024, 2: 0.080000274, 3: 0.08000023, 4: 0.08000023, 5: 0.17999955, 6: 0.08000024, 7: 0.08000024, 8: 0.08000024, 9: 0.17999849}\n",
      "197 {0: 0.17999905, 1: 0.08000053, 2: 0.08000061, 3: 0.080000505, 4: 0.08000053, 5: 0.080000564, 6: 0.08000053, 7: 0.17999661, 8: 0.080000535, 9: 0.08000054}\n",
      "198 {0: 0.07272786, 1: 0.07272787, 2: 0.07272795, 3: 0.072727844, 4: 0.16363707, 5: 0.0727279, 6: 0.07272787, 7: 0.16363291, 8: 0.072727874, 9: 0.16363482}\n",
      "199 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "200 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "201 {0: 0.08888904, 1: 0.1999986, 2: 0.088889055, 3: 0.08888903, 4: 0.08888904, 5: 0.08888904, 6: 0.08888904, 7: 0.08888904, 8: 0.08888904, 9: 0.08888904}\n",
      "202 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "203 {0: 0.072727844, 1: 0.072727844, 2: 0.07272793, 3: 0.16363698, 4: 0.072727844, 5: 0.07272788, 6: 0.072727844, 7: 0.07272785, 8: 0.16363421, 9: 0.16363372}\n",
      "204 {0: 0.06666733, 1: 0.06666733, 2: 0.06666744, 3: 0.06666731, 4: 0.1500009, 5: 0.14999987, 6: 0.06666733, 7: 0.14999919, 8: 0.14999597, 9: 0.06666736}\n",
      "205 {0: 0.16363439, 1: 0.07272797, 2: 0.07272807, 3: 0.07272794, 4: 0.07272796, 5: 0.07272801, 6: 0.07272796, 7: 0.25454178, 8: 0.07272798, 9: 0.07272799}\n",
      "206 {0: 0.06153905, 1: 0.29230565, 2: 0.061539143, 3: 0.06153903, 4: 0.06153905, 5: 0.06153909, 6: 0.21538174, 7: 0.06153906, 8: 0.061539065, 9: 0.061539073}\n",
      "207 {0: 0.18666507, 1: 0.053334463, 2: 0.05333463, 3: 0.053334422, 4: 0.053334452, 5: 0.053334538, 6: 0.11999632, 7: 0.05333448, 8: 0.11999717, 9: 0.25333446}\n",
      "208 {0: 0.08888939, 1: 0.19999526, 2: 0.08888948, 3: 0.088889375, 4: 0.08888939, 5: 0.088889435, 6: 0.0888894, 7: 0.088889405, 8: 0.088889405, 9: 0.08888942}\n",
      "209 {0: 0.08000034, 1: 0.08000034, 2: 0.08000039, 3: 0.080000326, 4: 0.08000034, 5: 0.08000036, 6: 0.27999687, 7: 0.08000035, 8: 0.08000035, 9: 0.08000035}\n",
      "210 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "211 {0: 0.06153886, 1: 0.061538864, 2: 0.1384617, 3: 0.061538853, 4: 0.06153886, 5: 0.061538894, 6: 0.061538864, 7: 0.36922735, 8: 0.061538875, 9: 0.06153888}\n",
      "212 {0: 0.08000039, 1: 0.08000039, 2: 0.08000045, 3: 0.08000038, 4: 0.08000039, 5: 0.08000042, 6: 0.08000039, 7: 0.27999642, 8: 0.08000039, 9: 0.08000041}\n",
      "213 {0: 0.07272832, 1: 0.16363318, 2: 0.0727285, 3: 0.0727283, 4: 0.16363667, 5: 0.0727284, 6: 0.072728336, 7: 0.07272835, 8: 0.07272836, 9: 0.16363163}\n",
      "214 {0: 0.042106036, 1: 0.19999951, 2: 0.14737143, 3: 0.09473033, 4: 0.25263095, 5: 0.042106096, 6: 0.094737455, 7: 0.04210605, 8: 0.042106055, 9: 0.04210607}\n",
      "215 {0: 0.13999996, 1: 0.13999902, 2: 0.04000124, 3: 0.14000286, 4: 0.04000107, 5: 0.1899994, 6: 0.14000036, 7: 0.08999392, 8: 0.040001094, 9: 0.040001117}\n",
      "216 {0: 0.08000052, 1: 0.08000053, 2: 0.08000061, 3: 0.080000505, 4: 0.08000053, 5: 0.080000564, 6: 0.08000053, 7: 0.080000535, 8: 0.17999598, 9: 0.17999971}\n",
      "217 {0: 0.040001467, 1: 0.08999702, 2: 0.14000422, 3: 0.13999842, 4: 0.19000255, 5: 0.040001575, 6: 0.13999109, 7: 0.040001497, 8: 0.09000251, 9: 0.08999967}\n",
      "218 {0: 0.08000043, 1: 0.08000044, 2: 0.18000081, 3: 0.08000042, 4: 0.08000043, 5: 0.08000047, 6: 0.08000044, 7: 0.080000445, 8: 0.080000445, 9: 0.17999567}\n",
      "219 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "220 {0: 0.088889115, 1: 0.08888912, 2: 0.08888915, 3: 0.08888911, 4: 0.088889115, 5: 0.19999792, 6: 0.088889115, 7: 0.08888912, 8: 0.08888912, 9: 0.08888913}\n",
      "221 {0: 0.13845754, 1: 0.061539356, 2: 0.061539486, 3: 0.061539322, 4: 0.061539344, 5: 0.061539415, 6: 0.1384585, 7: 0.061539363, 8: 0.061539367, 9: 0.29230833}\n",
      "222 {0: 0.08888895, 1: 0.08888895, 2: 0.1999995, 3: 0.08888895, 4: 0.08888895, 5: 0.08888895, 6: 0.08888895, 7: 0.08888895, 8: 0.08888895, 9: 0.08888895}\n",
      "223 {0: 0.044445135, 1: 0.044445142, 2: 0.044445246, 3: 0.4333329, 4: 0.044445135, 5: 0.044445187, 6: 0.044445142, 7: 0.15555134, 8: 0.044445153, 9: 0.099999584}\n",
      "224 {0: 0.080000564, 1: 0.08000057, 2: 0.18000104, 3: 0.08000054, 4: 0.17999434, 5: 0.0800006, 6: 0.08000057, 7: 0.08000057, 8: 0.08000058, 9: 0.08000059}\n",
      "225 {0: 0.08888908, 1: 0.08888908, 2: 0.088889115, 3: 0.08888908, 4: 0.19999824, 5: 0.08888909, 6: 0.08888908, 7: 0.088889085, 8: 0.088889085, 9: 0.088889085}\n",
      "226 {0: 0.0615395, 1: 0.13845785, 2: 0.06153967, 3: 0.06153947, 4: 0.13846193, 5: 0.13845804, 6: 0.06153951, 7: 0.13846307, 8: 0.1384614, 9: 0.061539546}\n",
      "227 {0: 0.08888895, 1: 0.08888895, 2: 0.1999995, 3: 0.08888895, 4: 0.08888895, 5: 0.08888895, 6: 0.08888895, 7: 0.08888895, 8: 0.08888895, 9: 0.08888895}\n",
      "228 {0: 0.13846113, 1: 0.061539058, 2: 0.1384601, 3: 0.06153903, 4: 0.06153905, 5: 0.06153909, 6: 0.061539058, 7: 0.29230538, 8: 0.061539065, 9: 0.061539073}\n",
      "229 {0: 0.072728075, 1: 0.16363734, 2: 0.0727282, 3: 0.16363221, 4: 0.072728075, 5: 0.072728135, 6: 0.16363366, 7: 0.07272809, 8: 0.0727281, 9: 0.07272811}\n",
      "230 {0: 0.12856849, 1: 0.05714355, 2: 0.05714366, 3: 0.12857203, 4: 0.12857237, 5: 0.12856911, 6: 0.20000005, 7: 0.05714356, 8: 0.057143565, 9: 0.057143576}\n",
      "231 {0: 0.047059096, 1: 0.0470591, 2: 0.10588267, 3: 0.16470602, 4: 0.047059096, 5: 0.04705912, 6: 0.39999762, 7: 0.047059104, 8: 0.047059104, 9: 0.047059108}\n",
      "232 {0: 0.12857185, 1: 0.12857412, 2: 0.05714459, 3: 0.057144314, 4: 0.12857422, 5: 0.057144463, 6: 0.19998738, 7: 0.12857027, 8: 0.057144392, 9: 0.057144415}\n",
      "233 {0: 0.08888895, 1: 0.08888895, 2: 0.1999995, 3: 0.08888895, 4: 0.08888895, 5: 0.08888895, 6: 0.08888895, 7: 0.08888895, 8: 0.08888895, 9: 0.08888895}\n",
      "234 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "235 {0: 0.16363037, 1: 0.0727282, 2: 0.07272834, 3: 0.072728164, 4: 0.16363697, 5: 0.07272826, 6: 0.0727282, 7: 0.16363508, 8: 0.07272822, 9: 0.07272823}\n",
      "236 {0: 0.061539017, 1: 0.06153902, 2: 0.13846251, 3: 0.061539, 4: 0.061539017, 5: 0.061539058, 6: 0.21538216, 7: 0.061539028, 8: 0.061539028, 9: 0.21538214}\n",
      "237 {0: 0.15555535, 1: 0.04444487, 2: 0.044444934, 3: 0.044444855, 4: 0.09999853, 5: 0.21110965, 6: 0.04444487, 7: 0.044444874, 8: 0.2666672, 9: 0.044444885}\n",
      "238 {0: 0.08888925, 1: 0.08888925, 2: 0.08888931, 3: 0.08888924, 4: 0.08888925, 5: 0.08888928, 6: 0.08888925, 7: 0.088889256, 8: 0.19999671, 9: 0.08888926}\n",
      "239 {0: 0.057143863, 1: 0.12856805, 2: 0.05714403, 3: 0.057143837, 4: 0.057143867, 5: 0.05714394, 6: 0.12856796, 7: 0.19999865, 8: 0.057143893, 9: 0.20000187}\n",
      "240 {0: 0.06153904, 1: 0.061539043, 2: 0.061539132, 3: 0.36922926, 4: 0.06153904, 5: 0.061539084, 6: 0.1384582, 7: 0.06153905, 8: 0.061539058, 9: 0.061539065}\n",
      "241 {0: 0.06666709, 1: 0.06666709, 2: 0.06666715, 3: 0.06666707, 4: 0.06666709, 5: 0.23332974, 6: 0.06666709, 7: 0.06666709, 8: 0.23333348, 9: 0.0666671}\n",
      "242 {0: 0.08000088, 1: 0.17999539, 2: 0.08000103, 3: 0.080000855, 4: 0.08000088, 5: 0.08000095, 6: 0.17999727, 7: 0.0800009, 8: 0.08000091, 9: 0.08000092}\n",
      "243 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "244 {0: 0.17999384, 1: 0.08000084, 2: 0.08000097, 3: 0.08000081, 4: 0.08000083, 5: 0.17999925, 6: 0.08000084, 7: 0.080000855, 8: 0.080000855, 9: 0.08000087}\n",
      "245 {0: 0.08888897, 1: 0.19999918, 2: 0.08888899, 3: 0.08888897, 4: 0.08888897, 5: 0.08888898, 6: 0.08888897, 7: 0.08888897, 8: 0.08888897, 9: 0.08888897}\n",
      "246 {0: 0.08000094, 1: 0.17999864, 2: 0.08000109, 3: 0.080000915, 4: 0.08000094, 5: 0.08000101, 6: 0.08000095, 7: 0.08000096, 8: 0.17999355, 9: 0.08000098}\n",
      "247 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "248 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "249 {0: 0.08888908, 1: 0.088889085, 2: 0.19999816, 3: 0.08888908, 4: 0.08888908, 5: 0.0888891, 6: 0.088889085, 7: 0.088889085, 8: 0.088889085, 9: 0.08888909}\n",
      "250 {0: 0.10586892, 1: 0.0470599, 2: 0.04706006, 3: 0.3411822, 4: 0.04705989, 5: 0.04705997, 6: 0.22352923, 7: 0.047059916, 8: 0.04705992, 9: 0.04705994}\n",
      "251 {0: 0.05714347, 1: 0.057143472, 2: 0.057143565, 3: 0.05714345, 4: 0.05714347, 5: 0.057143517, 6: 0.19999765, 7: 0.12857044, 8: 0.057143487, 9: 0.27142745}\n",
      "252 {0: 0.09999821, 1: 0.09999709, 2: 0.15555649, 3: 0.044445127, 4: 0.10000092, 5: 0.044445198, 6: 0.044445153, 7: 0.32222143, 8: 0.04444517, 9: 0.044445176}\n",
      "253 {0: 0.0666675, 1: 0.14999817, 2: 0.06666763, 3: 0.06666748, 4: 0.0666675, 5: 0.14999893, 6: 0.066667505, 7: 0.06666752, 8: 0.06666753, 9: 0.23333025}\n",
      "254 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "255 {0: 0.06153908, 1: 0.21538585, 2: 0.13846233, 3: 0.06153906, 4: 0.06153908, 5: 0.13846037, 6: 0.061539084, 7: 0.06153909, 8: 0.061539095, 9: 0.13845699}\n",
      "256 {0: 0.080000594, 1: 0.080000594, 2: 0.08000068, 3: 0.18000042, 4: 0.17999464, 5: 0.08000063, 6: 0.080000594, 7: 0.0800006, 8: 0.08000061, 9: 0.08000062}\n",
      "257 {0: 0.080000475, 1: 0.08000048, 2: 0.08000056, 3: 0.08000047, 4: 0.080000475, 5: 0.08000051, 6: 0.08000048, 7: 0.27999556, 8: 0.08000049, 9: 0.0800005}\n",
      "258 {0: 0.4333309, 1: 0.044444904, 2: 0.04444497, 3: 0.044444885, 4: 0.0444449, 5: 0.044444934, 6: 0.044444904, 7: 0.044444907, 8: 0.21110974, 9: 0.04444492}\n",
      "259 {0: 0.07272761, 1: 0.07272762, 2: 0.07272767, 3: 0.16363646, 4: 0.16363648, 5: 0.16363364, 6: 0.07272762, 7: 0.07272763, 8: 0.07272763, 9: 0.07272763}\n",
      "260 {0: 0.08888918, 1: 0.08888919, 2: 0.08888924, 3: 0.08888918, 4: 0.19999725, 5: 0.08888921, 6: 0.08888919, 7: 0.08888919, 8: 0.0888892, 9: 0.088889204}\n",
      "261 {0: 0.040000867, 1: 0.24000305, 2: 0.040001005, 3: 0.04000084, 4: 0.189998, 5: 0.04000093, 6: 0.13999423, 7: 0.18999925, 8: 0.04000089, 9: 0.040000904}\n",
      "262 {0: 0.16363162, 1: 0.07272793, 2: 0.07272804, 3: 0.07272791, 4: 0.072727926, 5: 0.07272798, 6: 0.25454476, 7: 0.07272794, 8: 0.07272795, 9: 0.072727956}\n",
      "263 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "264 {0: 0.18666707, 1: 0.18666574, 2: 0.05333493, 3: 0.053334672, 4: 0.05333471, 5: 0.053334814, 6: 0.05333472, 7: 0.11999951, 8: 0.11999434, 9: 0.11999952}\n",
      "265 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "266 {0: 0.17272744, 1: 0.08181883, 2: 0.08181561, 3: 0.03636435, 4: 0.03636437, 5: 0.35455072, 6: 0.08181533, 7: 0.08181455, 8: 0.036364388, 9: 0.036364403}\n",
      "267 {0: 0.07272815, 1: 0.16363057, 2: 0.0727283, 3: 0.07272813, 4: 0.16363686, 5: 0.072728224, 6: 0.07272816, 7: 0.07272817, 8: 0.07272817, 9: 0.16363525}\n",
      "268 {0: 0.08888927, 1: 0.08888928, 2: 0.08888934, 3: 0.08888926, 4: 0.08888927, 5: 0.0888893, 6: 0.19999638, 7: 0.088889286, 8: 0.088889286, 9: 0.08888929}\n",
      "269 {0: 0.07272761, 1: 0.07272762, 2: 0.07272767, 3: 0.072727606, 4: 0.16363548, 5: 0.07272764, 6: 0.07272762, 7: 0.2545435, 8: 0.07272763, 9: 0.07272763}\n",
      "270 {0: 0.16363034, 1: 0.072728574, 2: 0.07272877, 3: 0.25454074, 4: 0.07272856, 5: 0.072728656, 6: 0.072728574, 7: 0.07272859, 8: 0.07272859, 9: 0.07272861}\n",
      "271 {0: 0.06153921, 1: 0.13845772, 2: 0.06153933, 3: 0.06153919, 4: 0.06153921, 5: 0.061539266, 6: 0.29230866, 7: 0.061539225, 8: 0.13845895, 9: 0.06153924}\n",
      "272 {0: 0.07272771, 1: 0.07272771, 2: 0.07272778, 3: 0.16363639, 4: 0.07272771, 5: 0.07272774, 6: 0.07272771, 7: 0.07272772, 8: 0.25454178, 9: 0.072727725}\n",
      "273 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "274 {0: 0.072727926, 1: 0.07272793, 2: 0.07272804, 3: 0.1636329, 4: 0.072727926, 5: 0.16363707, 6: 0.07272793, 7: 0.07272794, 8: 0.16363442, 9: 0.072727956}\n",
      "275 {0: 0.06666696, 1: 0.3999973, 2: 0.06666701, 3: 0.06666695, 4: 0.06666696, 5: 0.06666698, 6: 0.06666697, 7: 0.06666697, 8: 0.06666697, 9: 0.066666976}\n",
      "276 {0: 0.08888907, 1: 0.19999826, 2: 0.08888911, 3: 0.08888906, 4: 0.08888907, 5: 0.088889085, 6: 0.08888907, 7: 0.08888908, 8: 0.08888908, 9: 0.08888908}\n",
      "277 {0: 0.061538834, 1: 0.061538838, 2: 0.13846195, 3: 0.061538823, 4: 0.21538433, 5: 0.06153886, 6: 0.061538834, 7: 0.21538183, 8: 0.06153884, 9: 0.06153885}\n",
      "278 {0: 0.08000071, 1: 0.17999582, 2: 0.08000083, 3: 0.0800007, 4: 0.08000071, 5: 0.080000766, 6: 0.17999826, 7: 0.080000736, 8: 0.080000736, 9: 0.08000075}\n",
      "279 {0: 0.05000073, 1: 0.050000735, 2: 0.050000846, 3: 0.05000071, 4: 0.05000073, 5: 0.30000204, 6: 0.112498775, 7: 0.17499729, 8: 0.05000075, 9: 0.11249737}\n",
      "280 {0: 0.14999013, 1: 0.06666786, 2: 0.23333687, 3: 0.066667825, 4: 0.066667855, 5: 0.066667944, 6: 0.06666786, 7: 0.14999783, 8: 0.066667885, 9: 0.06666791}\n",
      "281 {0: 0.06153999, 1: 0.061540004, 2: 0.061540235, 3: 0.13845983, 4: 0.13845617, 5: 0.06154011, 6: 0.061540004, 7: 0.1384583, 8: 0.21538532, 9: 0.061540056}\n",
      "282 {0: 0.06666733, 1: 0.06666733, 2: 0.06666744, 3: 0.1499965, 4: 0.06666733, 5: 0.06666738, 6: 0.149998, 7: 0.06666735, 8: 0.06666735, 9: 0.23333396}\n",
      "283 {0: 0.08888903, 1: 0.08888904, 2: 0.088889055, 3: 0.08888903, 4: 0.08888903, 5: 0.08888904, 6: 0.08888904, 7: 0.08888904, 8: 0.08888904, 9: 0.1999987}\n",
      "284 {0: 0.07272774, 1: 0.07272775, 2: 0.16363674, 3: 0.07272773, 4: 0.07272774, 5: 0.16363569, 6: 0.07272775, 7: 0.072727755, 8: 0.16363336, 9: 0.07272776}\n",
      "285 {0: 0.088888995, 1: 0.088889, 2: 0.08888902, 3: 0.088888995, 4: 0.088889, 5: 0.08888901, 6: 0.088889, 7: 0.088889, 8: 0.19999897, 9: 0.088889}\n",
      "286 {0: 0.16470997, 1: 0.105879694, 2: 0.04706051, 3: 0.16470976, 4: 0.047060277, 5: 0.10587825, 6: 0.10588097, 7: 0.105882026, 8: 0.047060315, 9: 0.105878256}\n",
      "287 {0: 0.16362582, 1: 0.072729275, 2: 0.07272958, 3: 0.07272921, 4: 0.07272926, 5: 0.072729416, 6: 0.072729275, 7: 0.072729304, 8: 0.25453952, 9: 0.07272935}\n",
      "288 {0: 0.066667214, 1: 0.066667214, 2: 0.066667296, 3: 0.0666672, 4: 0.066667214, 5: 0.15000036, 6: 0.14999615, 7: 0.2333329, 8: 0.06666723, 9: 0.06666724}\n",
      "289 {0: 0.057143435, 1: 0.12856999, 2: 0.05714353, 3: 0.27143055, 4: 0.057143435, 5: 0.057143483, 6: 0.057143442, 7: 0.12856711, 8: 0.12857156, 9: 0.05714346}\n",
      "290 {0: 0.080000155, 1: 0.2799985, 2: 0.080000184, 3: 0.080000155, 4: 0.080000155, 5: 0.08000016, 6: 0.08000016, 7: 0.08000016, 8: 0.08000016, 9: 0.08000016}\n",
      "291 {0: 0.066667244, 1: 0.23333192, 2: 0.06666733, 3: 0.15000065, 4: 0.14999656, 5: 0.06666729, 6: 0.066667244, 7: 0.06666726, 8: 0.06666726, 9: 0.066667266}\n",
      "292 {0: 0.29000184, 1: 0.040000867, 2: 0.040001, 3: 0.14000018, 4: 0.09000133, 5: 0.089997984, 6: 0.13999856, 7: 0.040000882, 8: 0.040000882, 9: 0.08999645}\n",
      "293 {0: 0.088889204, 1: 0.088889204, 2: 0.088889256, 3: 0.0888892, 4: 0.088889204, 5: 0.08888923, 6: 0.088889204, 7: 0.19999708, 8: 0.08888921, 9: 0.08888921}\n",
      "294 {0: 0.057143804, 1: 0.057143815, 2: 0.05714396, 3: 0.05714378, 4: 0.128569, 5: 0.12857205, 6: 0.19999799, 7: 0.1999979, 8: 0.05714383, 9: 0.05714385}\n",
      "295 {0: 0.08000046, 1: 0.08000047, 2: 0.08000054, 3: 0.08000045, 4: 0.08000046, 5: 0.2799957, 6: 0.08000047, 7: 0.080000475, 8: 0.080000475, 9: 0.08000049}\n",
      "296 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "297 {0: 0.08000094, 1: 0.17999555, 2: 0.08000109, 3: 0.080000915, 4: 0.08000094, 5: 0.08000101, 6: 0.080000944, 7: 0.1799967, 8: 0.08000096, 9: 0.08000098}\n",
      "298 {0: 0.08888905, 1: 0.08888905, 2: 0.08888907, 3: 0.08888904, 4: 0.08888905, 5: 0.19999856, 6: 0.08888905, 7: 0.08888905, 8: 0.088889055, 9: 0.088889055}\n",
      "299 {0: 0.08888932, 1: 0.08888932, 2: 0.08888939, 3: 0.19999601, 4: 0.08888932, 5: 0.08888935, 6: 0.08888932, 7: 0.08888933, 8: 0.08888933, 9: 0.088889346}\n",
      "300 {0: 0.08888911, 1: 0.08888911, 2: 0.088889144, 3: 0.0888891, 4: 0.08888911, 5: 0.08888913, 6: 0.08888911, 7: 0.088889115, 8: 0.19999793, 9: 0.08888912}\n",
      "301 {0: 0.08888934, 1: 0.1999958, 2: 0.08888942, 3: 0.08888933, 4: 0.08888934, 5: 0.08888938, 6: 0.088889346, 7: 0.08888935, 8: 0.08888935, 9: 0.08888936}\n",
      "302 {0: 0.11249804, 1: 0.050000656, 2: 0.050000753, 3: 0.050000634, 4: 0.11250063, 5: 0.17499985, 6: 0.11249988, 7: 0.050000664, 8: 0.050000668, 9: 0.23749821}\n",
      "303 {0: 0.15000032, 1: 0.066667765, 2: 0.15000263, 3: 0.06666773, 4: 0.06666776, 5: 0.06666783, 6: 0.06666776, 7: 0.06666777, 8: 0.23332265, 9: 0.0666678}\n",
      "304 {0: 0.08888905, 1: 0.08888905, 2: 0.08888907, 3: 0.08888904, 4: 0.08888905, 5: 0.08888906, 6: 0.19999853, 7: 0.088889055, 8: 0.088889055, 9: 0.088889055}\n",
      "305 {0: 0.0666673, 1: 0.1500005, 2: 0.14999847, 3: 0.15000048, 4: 0.0666673, 5: 0.06666735, 6: 0.14999662, 7: 0.06666731, 8: 0.06666732, 9: 0.066667326}\n",
      "306 {0: 0.08888897, 1: 0.19999918, 2: 0.08888899, 3: 0.08888897, 4: 0.08888897, 5: 0.08888898, 6: 0.08888897, 7: 0.08888897, 8: 0.08888897, 9: 0.08888897}\n",
      "307 {0: 0.088889, 1: 0.088889, 2: 0.08888902, 3: 0.19999892, 4: 0.088889, 5: 0.08888901, 6: 0.088889, 7: 0.088889, 8: 0.088889, 9: 0.088889}\n",
      "308 {0: 0.07272784, 1: 0.16363595, 2: 0.07272792, 3: 0.07272782, 4: 0.16363323, 5: 0.07272788, 6: 0.16363584, 7: 0.072727844, 8: 0.072727844, 9: 0.07272786}\n",
      "309 {0: 0.08888932, 1: 0.08888933, 2: 0.0888894, 3: 0.08888931, 4: 0.08888932, 5: 0.08888935, 6: 0.08888932, 7: 0.08888933, 8: 0.19999593, 9: 0.088889346}\n",
      "310 {0: 0.13845232, 1: 0.061539765, 2: 0.06153997, 3: 0.06153972, 4: 0.061539758, 5: 0.061539855, 6: 0.21538053, 7: 0.061539784, 8: 0.21538846, 9: 0.061539814}\n",
      "311 {0: 0.119998164, 1: 0.11999837, 2: 0.053334646, 3: 0.053334437, 4: 0.053334463, 5: 0.11999993, 6: 0.119997494, 7: 0.05333449, 8: 0.053334497, 9: 0.2533335}\n",
      "312 {0: 0.35789263, 1: 0.04210554, 2: 0.042105578, 3: 0.09473598, 4: 0.25263253, 5: 0.04210556, 6: 0.04210554, 7: 0.04210554, 8: 0.04210554, 9: 0.042105548}\n",
      "313 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "314 {0: 0.17999989, 1: 0.17999685, 2: 0.08000045, 3: 0.08000038, 4: 0.08000039, 5: 0.08000042, 6: 0.08000039, 7: 0.08000039, 8: 0.0800004, 9: 0.08000041}\n",
      "315 {0: 0.08888905, 1: 0.08888905, 2: 0.08888907, 3: 0.08888904, 4: 0.08888905, 5: 0.08888906, 6: 0.19999851, 7: 0.088889055, 8: 0.088889055, 9: 0.088889055}\n",
      "316 {0: 0.1999967, 1: 0.08888924, 2: 0.08888929, 3: 0.08888923, 4: 0.088889234, 5: 0.08888926, 6: 0.08888924, 7: 0.08888924, 8: 0.08888925, 9: 0.088889256}\n",
      "317 {0: 0.053333875, 1: 0.053333882, 2: 0.053333964, 3: 0.053333864, 4: 0.253332, 5: 0.05333392, 6: 0.2533305, 7: 0.12000022, 8: 0.053333893, 9: 0.0533339}\n",
      "318 {0: 0.08888895, 1: 0.08888895, 2: 0.1999995, 3: 0.08888895, 4: 0.08888895, 5: 0.08888895, 6: 0.08888895, 7: 0.08888895, 8: 0.08888895, 9: 0.08888895}\n",
      "319 {0: 0.08000039, 1: 0.17999673, 2: 0.18000016, 3: 0.08000038, 4: 0.08000039, 5: 0.08000042, 6: 0.08000039, 7: 0.08000039, 8: 0.08000039, 9: 0.08000041}\n",
      "320 {0: 0.08888916, 1: 0.08888916, 2: 0.19999754, 3: 0.088889144, 4: 0.08888916, 5: 0.088889174, 6: 0.08888916, 7: 0.08888916, 8: 0.08888917, 9: 0.08888917}\n",
      "321 {0: 0.10269897, 1: 0.048647482, 2: 0.12973496, 3: 0.1837868, 4: 0.12972963, 5: 0.021622514, 6: 0.048645247, 7: 0.1837862, 8: 0.102703795, 9: 0.048644397}\n",
      "322 {0: 0.16470194, 1: 0.047059663, 2: 0.04705979, 3: 0.047059637, 4: 0.40000272, 5: 0.10587751, 6: 0.047059663, 7: 0.047059674, 8: 0.04705968, 9: 0.047059696}\n",
      "323 {0: 0.08000037, 1: 0.08000038, 2: 0.08000043, 3: 0.080000356, 4: 0.18000017, 5: 0.0800004, 6: 0.08000037, 7: 0.17999679, 8: 0.08000038, 9: 0.080000386}\n",
      "324 {0: 0.11999704, 1: 0.18666747, 2: 0.05333451, 3: 0.05333432, 4: 0.11999805, 5: 0.053334422, 6: 0.053334355, 7: 0.18666695, 8: 0.053334378, 9: 0.1199985}\n",
      "325 {0: 0.12173805, 1: 0.03478381, 2: 0.07826377, 3: 0.12173718, 4: 0.2087004, 5: 0.078260034, 6: 0.078259036, 7: 0.12173861, 8: 0.07825944, 9: 0.07825963}\n",
      "326 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "327 {0: 0.061540823, 1: 0.061540846, 2: 0.061541207, 3: 0.061540764, 4: 0.36920834, 5: 0.061541006, 6: 0.061540846, 7: 0.06154087, 8: 0.061540887, 9: 0.13846444}\n",
      "328 {0: 0.057143204, 1: 0.057143204, 2: 0.12857197, 3: 0.12857147, 4: 0.057143204, 5: 0.05714323, 6: 0.057143204, 7: 0.057143208, 8: 0.05714321, 9: 0.3428541}\n",
      "329 {0: 0.07272763, 1: 0.07272763, 2: 0.16363697, 3: 0.16363646, 4: 0.07272763, 5: 0.07272766, 6: 0.07272763, 7: 0.072727636, 8: 0.16363312, 9: 0.07272764}\n",
      "330 {0: 0.08888932, 1: 0.08888932, 2: 0.0888894, 3: 0.08888931, 4: 0.08888932, 5: 0.08888935, 6: 0.08888932, 7: 0.08888933, 8: 0.19999596, 9: 0.088889346}\n",
      "331 {0: 0.14999749, 1: 0.06666761, 2: 0.06666775, 3: 0.14999492, 4: 0.0666676, 5: 0.23333414, 6: 0.06666761, 7: 0.06666762, 8: 0.066667624, 9: 0.066667646}\n",
      "332 {0: 0.19999827, 1: 0.08888908, 2: 0.088889115, 3: 0.08888908, 4: 0.08888908, 5: 0.08888909, 6: 0.08888908, 7: 0.088889085, 8: 0.088889085, 9: 0.088889085}\n",
      "333 {0: 0.12857383, 1: 0.057144288, 2: 0.057144508, 3: 0.057144243, 4: 0.05714428, 5: 0.12857106, 6: 0.057144288, 7: 0.12857148, 8: 0.19998895, 9: 0.12857306}\n",
      "334 {0: 0.078261085, 1: 0.034783356, 2: 0.034783468, 3: 0.12173542, 4: 0.20869538, 5: 0.034783404, 6: 0.16521738, 7: 0.25217378, 8: 0.034783367, 9: 0.034783382}\n",
      "335 {0: 0.08000052, 1: 0.08000053, 2: 0.0800006, 3: 0.080000505, 4: 0.18000038, 5: 0.17999534, 6: 0.08000053, 7: 0.080000535, 8: 0.080000535, 9: 0.08000054}\n",
      "336 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "337 {0: 0.080000155, 1: 0.08000016, 2: 0.17999998, 3: 0.080000155, 4: 0.08000016, 5: 0.08000017, 6: 0.17999874, 7: 0.08000016, 8: 0.08000016, 9: 0.08000016}\n",
      "338 {0: 0.088889, 1: 0.088889, 2: 0.08888902, 3: 0.19999892, 4: 0.088889, 5: 0.08888901, 6: 0.088889, 7: 0.088889, 8: 0.088889, 9: 0.088889}\n",
      "339 {0: 0.088889144, 1: 0.088889144, 2: 0.08888918, 3: 0.08888914, 4: 0.088889144, 5: 0.08888916, 6: 0.088889144, 7: 0.088889144, 8: 0.088889144, 9: 0.19999768}\n",
      "340 {0: 0.072727896, 1: 0.0727279, 2: 0.072728, 3: 0.07272788, 4: 0.16363576, 5: 0.16363396, 6: 0.0727279, 7: 0.07272791, 8: 0.07272791, 9: 0.16363491}\n",
      "341 {0: 0.0571434, 1: 0.057143405, 2: 0.12857237, 3: 0.057143383, 4: 0.3428566, 5: 0.05714344, 6: 0.057143405, 7: 0.05714341, 8: 0.057143413, 9: 0.12856714}\n",
      "342 {0: 0.16363516, 1: 0.07272751, 2: 0.072727546, 3: 0.0727275, 4: 0.25454468, 5: 0.072727524, 6: 0.07272751, 7: 0.07272751, 8: 0.07272751, 9: 0.07272752}\n",
      "343 {0: 0.1124955, 1: 0.050000474, 2: 0.05000055, 3: 0.36250073, 4: 0.17500035, 5: 0.050000507, 6: 0.050000474, 7: 0.05000048, 8: 0.050000485, 9: 0.050000492}\n",
      "344 {0: 0.08888929, 1: 0.0888893, 2: 0.08888936, 3: 0.088889286, 4: 0.08888929, 5: 0.08888933, 6: 0.08888929, 7: 0.0888893, 8: 0.19999622, 9: 0.088889316}\n",
      "345 {0: 0.17999639, 1: 0.08000062, 2: 0.080000706, 3: 0.080000594, 4: 0.08000062, 5: 0.080000654, 6: 0.17999858, 7: 0.080000624, 8: 0.080000624, 9: 0.08000064}\n",
      "346 {0: 0.08000082, 1: 0.08000082, 2: 0.080000944, 3: 0.08000079, 4: 0.17999515, 5: 0.08000088, 6: 0.08000082, 7: 0.08000083, 8: 0.17999807, 9: 0.08000085}\n",
      "347 {0: 0.07272774, 1: 0.07272774, 2: 0.072727814, 3: 0.072727725, 4: 0.2545438, 5: 0.07272778, 6: 0.07272774, 7: 0.07272775, 8: 0.07272775, 9: 0.16363415}\n",
      "348 {0: 0.05333434, 1: 0.053334348, 2: 0.0533345, 3: 0.25333613, 4: 0.05333434, 5: 0.119996145, 6: 0.053334348, 7: 0.119996116, 8: 0.11999882, 9: 0.120000884}\n",
      "349 {0: 0.089995526, 1: 0.040000886, 2: 0.19000511, 3: 0.28778094, 4: 0.040000882, 5: 0.13999954, 6: 0.040000886, 7: 0.0899986, 8: 0.04221674, 9: 0.04000092}\n",
      "350 {0: 0.23158471, 1: 0.12631194, 2: 0.02105361, 3: 0.061537027, 4: 0.0736824, 5: 0.12631772, 6: 0.07368604, 7: 0.12631683, 8: 0.08582638, 9: 0.07368336}\n",
      "351 {0: 0.06153934, 1: 0.13845839, 2: 0.061539482, 3: 0.06153932, 4: 0.13846278, 5: 0.061539404, 6: 0.13846064, 7: 0.06153936, 8: 0.061539363, 9: 0.21538195}\n",
      "352 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "353 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "354 {0: 0.08888906, 1: 0.08888906, 2: 0.088889085, 3: 0.088889055, 4: 0.19999838, 5: 0.08888908, 6: 0.08888906, 7: 0.08888906, 8: 0.08888906, 9: 0.08888907}\n",
      "355 {0: 0.13846384, 1: 0.13845891, 2: 0.061540056, 3: 0.13845398, 4: 0.13846403, 5: 0.061539937, 6: 0.061539844, 7: 0.061539862, 8: 0.061539866, 9: 0.13845967}\n",
      "356 {0: 0.13846213, 1: 0.06153916, 2: 0.061539266, 3: 0.061539132, 4: 0.06153915, 5: 0.061539207, 6: 0.06153916, 7: 0.29230633, 8: 0.06153917, 9: 0.13845733}\n",
      "357 {0: 0.08888945, 1: 0.08888946, 2: 0.08888955, 3: 0.19999476, 4: 0.08888945, 5: 0.088889495, 6: 0.08888946, 7: 0.088889465, 8: 0.088889465, 9: 0.08888948}\n",
      "358 {0: 0.13845882, 1: 0.06153939, 2: 0.13846327, 3: 0.061539356, 4: 0.06153938, 5: 0.13845976, 6: 0.061539385, 7: 0.13846274, 8: 0.13845846, 9: 0.061539423}\n",
      "359 {0: 0.044445496, 1: 0.21110693, 2: 0.09999863, 3: 0.044445466, 4: 0.044445496, 5: 0.044445578, 6: 0.10000133, 7: 0.15555488, 8: 0.21111067, 9: 0.044445544}\n",
      "360 {0: 0.23333232, 1: 0.14999759, 2: 0.0666678, 3: 0.1499962, 4: 0.066667646, 5: 0.06666772, 6: 0.06666765, 7: 0.06666766, 8: 0.066667676, 9: 0.06666769}\n",
      "361 {0: 0.08888926, 1: 0.08888926, 2: 0.08888932, 3: 0.08888925, 4: 0.08888926, 5: 0.088889286, 6: 0.08888926, 7: 0.19999652, 8: 0.08888927, 9: 0.08888928}\n",
      "362 {0: 0.072727606, 1: 0.072727606, 2: 0.07272766, 3: 0.07272759, 4: 0.34545144, 5: 0.07272763, 6: 0.072727606, 7: 0.072727606, 8: 0.072727606, 9: 0.07272761}\n",
      "363 {0: 0.17499939, 1: 0.050000325, 2: 0.050000373, 3: 0.050000314, 4: 0.05000032, 5: 0.42499796, 6: 0.050000325, 7: 0.05000033, 8: 0.05000033, 9: 0.050000336}\n",
      "364 {0: 0.0727282, 1: 0.16363169, 2: 0.07272834, 3: 0.07272817, 4: 0.0727282, 5: 0.07272827, 6: 0.16363396, 7: 0.07272822, 8: 0.072728224, 9: 0.16363676}\n",
      "365 {0: 0.08888938, 1: 0.19999538, 2: 0.088889465, 3: 0.08888937, 4: 0.08888938, 5: 0.08888942, 6: 0.08888938, 7: 0.08888939, 8: 0.0888894, 9: 0.088889405}\n",
      "366 {0: 0.08000035, 1: 0.27999675, 2: 0.08000041, 3: 0.08000034, 4: 0.08000035, 5: 0.08000038, 6: 0.080000356, 7: 0.080000356, 8: 0.080000356, 9: 0.08000036}\n",
      "367 {0: 0.15000163, 1: 0.14999504, 2: 0.066667944, 3: 0.066667736, 4: 0.066667765, 5: 0.14999886, 6: 0.06666777, 7: 0.14999767, 8: 0.066667795, 9: 0.06666782}\n",
      "368 {0: 0.036364388, 1: 0.08181894, 2: 0.036364507, 3: 0.2636393, 4: 0.17272902, 5: 0.081819065, 6: 0.1272659, 7: 0.12727001, 8: 0.036364406, 9: 0.03636442}\n",
      "369 {0: 0.062067453, 1: 0.09654399, 2: 0.027587466, 3: 0.09655495, 4: 0.096548416, 5: 0.30345756, 6: 0.027587302, 7: 0.062064707, 8: 0.16552074, 9: 0.062067453}\n",
      "370 {0: 0.08000079, 1: 0.080000795, 2: 0.080000915, 3: 0.080000766, 4: 0.17999534, 5: 0.08000085, 6: 0.080000795, 7: 0.0800008, 8: 0.0800008, 9: 0.17999814}\n",
      "371 {0: 0.13845171, 1: 0.13846312, 2: 0.061539695, 3: 0.061539494, 4: 0.061539523, 5: 0.061539605, 6: 0.1384622, 7: 0.061539546, 8: 0.06153955, 9: 0.21538559}\n",
      "372 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "373 {0: 0.08888937, 1: 0.088889375, 2: 0.08888944, 3: 0.08888935, 4: 0.08888937, 5: 0.088889405, 6: 0.088889375, 7: 0.08888938, 8: 0.19999558, 9: 0.08888938}\n",
      "374 {0: 0.07272833, 1: 0.16363329, 2: 0.0727285, 3: 0.24435104, 4: 0.07272833, 5: 0.07272841, 6: 0.072728336, 7: 0.07272835, 8: 0.08291703, 9: 0.07272838}\n",
      "375 {0: 0.08888895, 1: 0.08888895, 2: 0.1999995, 3: 0.08888895, 4: 0.08888895, 5: 0.08888895, 6: 0.08888895, 7: 0.08888895, 8: 0.08888895, 9: 0.08888895}\n",
      "376 {0: 0.08888895, 1: 0.08888895, 2: 0.08888896, 3: 0.19999942, 4: 0.08888895, 5: 0.08888896, 6: 0.08888895, 7: 0.08888895, 8: 0.08888895, 9: 0.08888895}\n",
      "377 {0: 0.21538171, 1: 0.061539356, 2: 0.061539486, 3: 0.061539322, 4: 0.13845675, 5: 0.061539415, 6: 0.13846253, 7: 0.13846269, 8: 0.061539367, 9: 0.061539385}\n",
      "378 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "379 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "380 {0: 0.072728954, 1: 0.16363339, 2: 0.07272923, 3: 0.07272891, 4: 0.072728954, 5: 0.25453457, 6: 0.07272897, 7: 0.07272899, 8: 0.072729, 9: 0.07272903}\n",
      "381 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "382 {0: 0.13999745, 1: 0.34000105, 2: 0.04000116, 3: 0.04000097, 4: 0.040001, 5: 0.040001076, 6: 0.13999757, 7: 0.13999763, 8: 0.040001027, 9: 0.040001046}\n",
      "383 {0: 0.088888995, 1: 0.088889, 2: 0.08888902, 3: 0.088888995, 4: 0.088889, 5: 0.08888901, 6: 0.088889, 7: 0.088889, 8: 0.19999897, 9: 0.088889}\n",
      "384 {0: 0.088889286, 1: 0.19999632, 2: 0.088889346, 3: 0.08888927, 4: 0.088889286, 5: 0.088889316, 6: 0.088889286, 7: 0.08888929, 8: 0.08888929, 9: 0.0888893}\n",
      "385 {0: 0.07272867, 1: 0.16363387, 2: 0.072728895, 3: 0.16363873, 4: 0.07272868, 5: 0.072728775, 6: 0.16362622, 7: 0.0727287, 8: 0.07272871, 9: 0.07272874}\n",
      "386 {0: 0.07272764, 1: 0.07272764, 2: 0.0727277, 3: 0.07272763, 4: 0.07272764, 5: 0.16363473, 6: 0.07272764, 7: 0.07272765, 8: 0.07272765, 9: 0.2545441}\n",
      "387 {0: 0.08888897, 1: 0.19999918, 2: 0.08888899, 3: 0.08888897, 4: 0.08888897, 5: 0.08888898, 6: 0.08888897, 7: 0.08888897, 8: 0.08888897, 9: 0.08888897}\n",
      "388 {0: 0.022857932, 1: 0.10856832, 2: 0.022858057, 3: 0.19428457, 4: 0.25143608, 5: 0.022857992, 6: 0.16571295, 7: 0.02285795, 8: 0.1371417, 9: 0.051424455}\n",
      "389 {0: 0.08888895, 1: 0.08888895, 2: 0.1999995, 3: 0.08888895, 4: 0.08888895, 5: 0.08888895, 6: 0.08888895, 7: 0.08888895, 8: 0.08888895, 9: 0.08888895}\n",
      "390 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "391 {0: 0.06666738, 1: 0.066667385, 2: 0.06666749, 3: 0.066667356, 4: 0.06666738, 5: 0.23333044, 6: 0.066667385, 7: 0.06666739, 8: 0.0666674, 9: 0.23333041}\n",
      "392 {0: 0.088889115, 1: 0.08888912, 2: 0.08888915, 3: 0.08888911, 4: 0.088889115, 5: 0.19999792, 6: 0.088889115, 7: 0.08888912, 8: 0.08888912, 9: 0.08888913}\n",
      "393 {0: 0.17999771, 1: 0.08000035, 2: 0.08000039, 3: 0.17999944, 4: 0.08000034, 5: 0.08000036, 6: 0.08000034, 7: 0.08000035, 8: 0.08000035, 9: 0.080000356}\n",
      "394 {0: 0.07272763, 1: 0.07272763, 2: 0.07272768, 3: 0.07272761, 4: 0.07272763, 5: 0.07272766, 6: 0.3454512, 7: 0.072727636, 8: 0.072727636, 9: 0.07272764}\n",
      "395 {0: 0.05714357, 1: 0.057143573, 2: 0.12857307, 3: 0.12857215, 4: 0.05714357, 5: 0.19999816, 6: 0.057143573, 7: 0.12856728, 8: 0.057143584, 9: 0.12857148}\n",
      "396 {0: 0.080000825, 1: 0.080000825, 2: 0.08000096, 3: 0.080000795, 4: 0.080000825, 5: 0.080000885, 6: 0.080000825, 7: 0.08000084, 8: 0.17999676, 9: 0.17999642}\n",
      "397 {0: 0.03478304, 1: 0.034783043, 2: 0.034783106, 3: 0.20869614, 4: 0.03478304, 5: 0.034783073, 6: 0.4260891, 7: 0.078260764, 8: 0.07825562, 9: 0.034783054}\n",
      "398 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "399 {0: 0.061539404, 1: 0.061539415, 2: 0.138461, 3: 0.06153938, 4: 0.13845673, 5: 0.06153948, 6: 0.29230627, 7: 0.061539423, 8: 0.061539426, 9: 0.061539445}\n",
      "400 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "401 {0: 0.08000032, 1: 0.080000326, 2: 0.08000037, 3: 0.27999702, 4: 0.08000032, 5: 0.08000034, 6: 0.080000326, 7: 0.080000326, 8: 0.08000033, 9: 0.08000033}\n",
      "402 {0: 0.18666351, 1: 0.053334314, 2: 0.05333446, 3: 0.18666448, 4: 0.120000236, 5: 0.053334378, 6: 0.1200012, 7: 0.11999872, 8: 0.05333433, 9: 0.05333435}\n",
      "403 {0: 0.08888906, 1: 0.08888906, 2: 0.088889085, 3: 0.088889055, 4: 0.19999838, 5: 0.08888908, 6: 0.08888906, 7: 0.08888906, 8: 0.08888906, 9: 0.08888907}\n",
      "404 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "405 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "406 {0: 0.17999834, 1: 0.0800006, 2: 0.0800007, 3: 0.08000059, 4: 0.0800006, 5: 0.08000065, 6: 0.0800006, 7: 0.08000061, 8: 0.17999662, 9: 0.08000063}\n",
      "407 {0: 0.08000034, 1: 0.08000034, 2: 0.08000039, 3: 0.080000326, 4: 0.1800001, 5: 0.08000036, 6: 0.08000034, 7: 0.08000035, 8: 0.08000035, 9: 0.17999712}\n",
      "408 {0: 0.08888897, 1: 0.08888897, 2: 0.19999923, 3: 0.08888897, 4: 0.08888897, 5: 0.08888898, 6: 0.08888897, 7: 0.08888897, 8: 0.08888897, 9: 0.08888897}\n",
      "409 {0: 0.057143558, 1: 0.2714252, 2: 0.12857287, 3: 0.12857234, 4: 0.057143558, 5: 0.057143614, 6: 0.12856813, 7: 0.057143576, 8: 0.05714358, 9: 0.057143595}\n",
      "410 {0: 0.12727019, 1: 0.1727257, 2: 0.036365166, 3: 0.08182037, 4: 0.17272748, 5: 0.036365055, 6: 0.17273039, 7: 0.036364984, 8: 0.0818108, 9: 0.08181983}\n",
      "411 {0: 0.16363585, 1: 0.07272767, 2: 0.07272773, 3: 0.07272766, 4: 0.07272767, 5: 0.0727277, 6: 0.07272767, 7: 0.07272768, 8: 0.2545427, 9: 0.072727695}\n",
      "412 {0: 0.12856722, 1: 0.12856753, 2: 0.12857422, 3: 0.05714391, 4: 0.05714394, 5: 0.12857096, 6: 0.2000003, 7: 0.05714396, 8: 0.05714397, 9: 0.057143986}\n",
      "413 {0: 0.32381102, 1: 0.03809591, 2: 0.03809601, 3: 0.038095888, 4: 0.08571494, 5: 0.085711025, 6: 0.13333185, 7: 0.038095918, 8: 0.03809592, 9: 0.1809515}\n",
      "414 {0: 0.088889375, 1: 0.08888938, 2: 0.08888946, 3: 0.08888937, 4: 0.19999549, 5: 0.08888941, 6: 0.08888938, 7: 0.08888939, 8: 0.08888939, 9: 0.088889405}\n",
      "415 {0: 0.08000077, 1: 0.27999282, 2: 0.0800009, 3: 0.08000076, 4: 0.08000077, 5: 0.08000083, 6: 0.08000078, 7: 0.08000079, 8: 0.080000795, 9: 0.08000081}\n",
      "416 {0: 0.061539546, 1: 0.1384598, 2: 0.061539724, 3: 0.061539516, 4: 0.06153955, 5: 0.29229996, 6: 0.061539553, 7: 0.13846317, 8: 0.061539575, 9: 0.061539594}\n",
      "417 {0: 0.08888916, 1: 0.08888916, 2: 0.0888892, 3: 0.088889144, 4: 0.08888916, 5: 0.088889174, 6: 0.08888916, 7: 0.08888916, 8: 0.08888917, 9: 0.19999751}\n",
      "418 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "419 {0: 0.07272838, 1: 0.07272839, 2: 0.07272856, 3: 0.07272834, 4: 0.1636324, 5: 0.072728455, 6: 0.07272838, 7: 0.16363293, 8: 0.0727284, 9: 0.16363576}\n",
      "420 {0: 0.05000127, 1: 0.17500202, 2: 0.050001476, 3: 0.17499442, 4: 0.17500181, 5: 0.11249646, 6: 0.1124986, 7: 0.050001297, 8: 0.050001305, 9: 0.050001327}\n",
      "421 {0: 0.08000017, 1: 0.08000017, 2: 0.08000019, 3: 0.08000016, 4: 0.27999848, 5: 0.08000018, 6: 0.08000017, 7: 0.08000017, 8: 0.08000017, 9: 0.08000017}\n",
      "422 {0: 0.07272758, 1: 0.07272758, 2: 0.072727636, 3: 0.072727576, 4: 0.16363637, 5: 0.25454292, 6: 0.07272758, 7: 0.07272759, 8: 0.07272759, 9: 0.0727276}\n",
      "423 {0: 0.08000077, 1: 0.17999582, 2: 0.08000091, 3: 0.08000076, 4: 0.08000078, 5: 0.08000084, 6: 0.1799977, 7: 0.08000079, 8: 0.080000795, 9: 0.08000081}\n",
      "424 {0: 0.08000036, 1: 0.08000036, 2: 0.08000042, 3: 0.08000035, 4: 0.27999663, 5: 0.080000386, 6: 0.08000036, 7: 0.08000036, 8: 0.08000036, 9: 0.08000037}\n",
      "425 {0: 0.2545417, 1: 0.072727725, 2: 0.07272779, 3: 0.16363642, 4: 0.07272772, 5: 0.072727755, 6: 0.072727725, 7: 0.07272773, 8: 0.07272773, 9: 0.07272774}\n",
      "426 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "427 {0: 0.12857065, 1: 0.057143908, 2: 0.12857354, 3: 0.12856787, 4: 0.0571439, 5: 0.05714398, 6: 0.12856904, 7: 0.057143923, 8: 0.19999924, 9: 0.05714395}\n",
      "428 {0: 0.08000034, 1: 0.08000034, 2: 0.18000053, 3: 0.080000326, 4: 0.08000034, 5: 0.08000036, 6: 0.08000034, 7: 0.08000035, 8: 0.08000035, 9: 0.17999676}\n",
      "429 {0: 0.19999668, 1: 0.08888925, 2: 0.08888931, 3: 0.08888924, 4: 0.08888925, 5: 0.08888927, 6: 0.08888925, 7: 0.08888925, 8: 0.088889256, 9: 0.08888926}\n",
      "430 {0: 0.07272776, 1: 0.07272777, 2: 0.07272784, 3: 0.07272775, 4: 0.25454494, 5: 0.0727278, 6: 0.16363283, 7: 0.07272777, 8: 0.07272777, 9: 0.072727785}\n",
      "431 {0: 0.15555577, 1: 0.3222239, 2: 0.044445753, 3: 0.099995136, 4: 0.09999536, 5: 0.044445656, 6: 0.04444558, 7: 0.044445597, 8: 0.10000166, 9: 0.044445623}\n",
      "432 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "433 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "434 {0: 0.07272764, 1: 0.07272765, 2: 0.16363625, 3: 0.07272764, 4: 0.07272765, 5: 0.07272768, 6: 0.16363458, 7: 0.07272766, 8: 0.07272766, 9: 0.1636356}\n",
      "435 {0: 0.057143915, 1: 0.057143927, 2: 0.057144087, 3: 0.12856942, 4: 0.057143915, 5: 0.12857008, 6: 0.20000067, 7: 0.1285714, 8: 0.05714394, 9: 0.1285686}\n",
      "436 {0: 0.057143293, 1: 0.057143297, 2: 0.057143368, 3: 0.057143286, 4: 0.057143297, 5: 0.057143327, 6: 0.34285635, 7: 0.12857, 8: 0.05714331, 9: 0.12857045}\n",
      "437 {0: 0.06666752, 1: 0.06666753, 2: 0.06666766, 3: 0.23333278, 4: 0.14999512, 5: 0.06666759, 6: 0.14999914, 7: 0.06666754, 8: 0.06666754, 9: 0.06666756}\n",
      "438 {0: 0.08000054, 1: 0.17999722, 2: 0.08000063, 3: 0.08000053, 4: 0.17999828, 5: 0.08000059, 6: 0.08000055, 7: 0.08000056, 8: 0.08000056, 9: 0.080000564}\n",
      "439 {0: 0.11249798, 1: 0.11249875, 2: 0.050001282, 3: 0.050001077, 4: 0.050001107, 5: 0.05000119, 6: 0.050001115, 7: 0.30000383, 8: 0.11249711, 9: 0.112496585}\n",
      "440 {0: 0.04705989, 1: 0.16470532, 2: 0.04706006, 3: 0.04705986, 4: 0.16470537, 5: 0.16470607, 6: 0.047059897, 7: 0.047059912, 8: 0.16470449, 9: 0.105879135}\n",
      "441 {0: 0.07272751, 1: 0.16363588, 2: 0.072727546, 3: 0.0727275, 4: 0.25454402, 5: 0.072727524, 6: 0.07272751, 7: 0.07272751, 8: 0.07272751, 9: 0.07272752}\n",
      "442 {0: 0.057144042, 1: 0.12856923, 2: 0.12857158, 3: 0.12856899, 4: 0.12857342, 5: 0.12856995, 6: 0.057144053, 7: 0.12857057, 8: 0.057144072, 9: 0.057144094}\n",
      "443 {0: 0.06153939, 1: 0.13845757, 2: 0.061539542, 3: 0.061539367, 4: 0.13846166, 5: 0.13846073, 6: 0.0615394, 7: 0.13846014, 8: 0.13846274, 9: 0.061539434}\n",
      "444 {0: 0.06153902, 1: 0.13845725, 2: 0.06153911, 3: 0.36923042, 4: 0.06153902, 5: 0.061539065, 6: 0.061539024, 7: 0.06153903, 8: 0.06153904, 9: 0.061539046}\n",
      "445 {0: 0.08000074, 1: 0.08000076, 2: 0.08000087, 3: 0.17999753, 4: 0.08000075, 5: 0.0800008, 6: 0.08000075, 7: 0.08000076, 8: 0.080000766, 9: 0.17999627}\n",
      "446 {0: 0.06153943, 1: 0.06153944, 2: 0.061539587, 3: 0.061539408, 4: 0.06153943, 5: 0.061539505, 6: 0.13845734, 7: 0.13845758, 8: 0.06153946, 9: 0.2923088}\n",
      "447 {0: 0.08000006, 1: 0.08000006, 2: 0.08000007, 3: 0.2799994, 4: 0.08000006, 5: 0.080000065, 6: 0.08000006, 7: 0.08000006, 8: 0.080000065, 9: 0.080000065}\n",
      "448 {0: 0.08000046, 1: 0.08000047, 2: 0.08000054, 3: 0.08000045, 4: 0.08000046, 5: 0.0800005, 6: 0.08000047, 7: 0.080000475, 8: 0.2799957, 9: 0.08000049}\n",
      "449 {0: 0.08888908, 1: 0.08888908, 2: 0.088889115, 3: 0.08888908, 4: 0.19999826, 5: 0.08888909, 6: 0.08888908, 7: 0.088889085, 8: 0.088889085, 9: 0.088889085}\n",
      "450 {0: 0.088889204, 1: 0.08888921, 2: 0.088889256, 3: 0.0888892, 4: 0.19999702, 5: 0.08888923, 6: 0.08888921, 7: 0.08888921, 8: 0.08888921, 9: 0.08888921}\n",
      "451 {0: 0.2333299, 1: 0.066667385, 2: 0.066667505, 3: 0.06666736, 4: 0.066667385, 5: 0.06666744, 6: 0.066667385, 7: 0.0666674, 8: 0.0666674, 9: 0.2333308}\n",
      "452 {0: 0.057143793, 1: 0.057143804, 2: 0.05714394, 3: 0.19999783, 4: 0.057143793, 5: 0.057143863, 6: 0.19999923, 7: 0.1285681, 8: 0.05714382, 9: 0.1285718}\n",
      "453 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "454 {0: 0.08888905, 1: 0.08888905, 2: 0.08888907, 3: 0.08888904, 4: 0.08888905, 5: 0.19999856, 6: 0.08888905, 7: 0.08888905, 8: 0.088889055, 9: 0.088889055}\n",
      "455 {0: 0.13999921, 1: 0.13999744, 2: 0.040001117, 3: 0.040000938, 4: 0.1400005, 5: 0.19000015, 6: 0.09000109, 7: 0.040000983, 8: 0.08999964, 9: 0.08999893}\n",
      "456 {0: 0.044445097, 1: 0.044445105, 2: 0.044445205, 3: 0.044445083, 4: 0.21111102, 5: 0.09999973, 6: 0.1555567, 7: 0.09999677, 8: 0.044445116, 9: 0.21111013}\n",
      "457 {0: 0.19999619, 1: 0.042106047, 2: 0.042106166, 3: 0.04210602, 4: 0.14736676, 5: 0.0421061, 6: 0.35789454, 7: 0.04210606, 8: 0.042106062, 9: 0.042106077}\n",
      "458 {0: 0.08000029, 1: 0.080000296, 2: 0.08000034, 3: 0.1799998, 4: 0.08000029, 5: 0.08000031, 6: 0.080000296, 7: 0.080000296, 8: 0.0800003, 9: 0.17999776}\n",
      "459 {0: 0.08000077, 1: 0.08000078, 2: 0.0800009, 3: 0.08000076, 4: 0.17999767, 5: 0.08000083, 6: 0.08000078, 7: 0.17999595, 8: 0.080000795, 9: 0.08000081}\n",
      "460 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "461 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "462 {0: 0.03200053, 1: 0.032000534, 2: 0.11200168, 3: 0.189781, 4: 0.27200198, 5: 0.032000568, 6: 0.11199713, 7: 0.032000538, 8: 0.034216654, 9: 0.1519994}\n",
      "463 {0: 0.06153932, 1: 0.13845862, 2: 0.13846366, 3: 0.13845785, 4: 0.06153932, 5: 0.06153938, 6: 0.061539322, 7: 0.13846119, 8: 0.06153934, 9: 0.138462}\n",
      "464 {0: 0.057143923, 1: 0.12857065, 2: 0.12857023, 3: 0.057143897, 4: 0.12857196, 5: 0.057144005, 6: 0.057143934, 7: 0.1285693, 8: 0.19999814, 9: 0.05714397}\n",
      "465 {0: 0.08000041, 1: 0.08000041, 2: 0.18000074, 3: 0.08000039, 4: 0.08000041, 5: 0.08000044, 6: 0.08000041, 7: 0.080000415, 8: 0.17999597, 9: 0.08000042}\n",
      "466 {0: 0.088889204, 1: 0.088889204, 2: 0.088889256, 3: 0.0888892, 4: 0.088889204, 5: 0.08888923, 6: 0.088889204, 7: 0.19999708, 8: 0.08888921, 9: 0.08888921}\n",
      "467 {0: 0.08000052, 1: 0.08000053, 2: 0.08000061, 3: 0.080000505, 4: 0.08000052, 5: 0.17999819, 6: 0.08000053, 7: 0.17999752, 8: 0.080000535, 9: 0.08000054}\n",
      "468 {0: 0.08888899, 1: 0.08888899, 2: 0.088888995, 3: 0.08888899, 4: 0.08888899, 5: 0.088888995, 6: 0.08888899, 7: 0.08888899, 8: 0.19999911, 9: 0.088888995}\n",
      "469 {0: 0.088889115, 1: 0.08888912, 2: 0.08888915, 3: 0.08888911, 4: 0.088889115, 5: 0.08888914, 6: 0.19999789, 7: 0.08888912, 8: 0.08888912, 9: 0.08888913}\n",
      "470 {0: 0.057143714, 1: 0.05714372, 2: 0.057143852, 3: 0.12857272, 4: 0.2714283, 5: 0.057143778, 6: 0.057143718, 7: 0.12856801, 8: 0.12856847, 9: 0.05714375}\n",
      "471 {0: 0.08888926, 1: 0.08888926, 2: 0.08888932, 3: 0.08888925, 4: 0.19999653, 5: 0.088889286, 6: 0.08888926, 7: 0.08888927, 8: 0.08888927, 9: 0.08888928}\n",
      "472 {0: 0.050000917, 1: 0.11249927, 2: 0.112499, 3: 0.050000895, 4: 0.050000917, 5: 0.050000988, 6: 0.1750008, 7: 0.2374986, 8: 0.112497665, 9: 0.05000096}\n",
      "473 {0: 0.053333934, 1: 0.05333394, 2: 0.053334035, 3: 0.12000066, 4: 0.053333938, 5: 0.119998485, 6: 0.25333363, 7: 0.053333953, 8: 0.053333953, 9: 0.1866635}\n",
      "474 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "475 {0: 0.05333413, 1: 0.25332817, 2: 0.05333426, 3: 0.120000415, 4: 0.1200012, 5: 0.18666525, 6: 0.053334136, 7: 0.053334147, 8: 0.05333415, 9: 0.053334165}\n",
      "476 {0: 0.23332918, 1: 0.066667646, 2: 0.066667795, 3: 0.06666762, 4: 0.06666764, 5: 0.06666771, 6: 0.15000081, 7: 0.14999628, 8: 0.06666766, 9: 0.06666768}\n",
      "477 {0: 0.08000074, 1: 0.08000075, 2: 0.08000086, 3: 0.17999321, 4: 0.08000074, 5: 0.0800008, 6: 0.08000075, 7: 0.08000076, 8: 0.18000062, 9: 0.08000078}\n",
      "478 {0: 0.072727606, 1: 0.07272761, 2: 0.072727665, 3: 0.072727606, 4: 0.072727606, 5: 0.072727636, 6: 0.16363557, 7: 0.07272762, 8: 0.07272762, 9: 0.2545434}\n",
      "479 {0: 0.088889234, 1: 0.088889234, 2: 0.088889286, 3: 0.08888923, 4: 0.088889234, 5: 0.08888926, 6: 0.088889234, 7: 0.19999681, 8: 0.08888924, 9: 0.08888925}\n",
      "480 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "481 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "482 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "483 {0: 0.08888897, 1: 0.08888897, 2: 0.19999923, 3: 0.08888897, 4: 0.08888897, 5: 0.08888898, 6: 0.08888897, 7: 0.08888897, 8: 0.08888897, 9: 0.08888897}\n",
      "484 {0: 0.080000855, 1: 0.080000855, 2: 0.08000099, 3: 0.080000825, 4: 0.080000855, 5: 0.080000915, 6: 0.080000855, 7: 0.08000087, 8: 0.17999667, 9: 0.17999634}\n",
      "485 {0: 0.08000041, 1: 0.08000041, 2: 0.08000047, 3: 0.2799962, 4: 0.08000041, 5: 0.08000044, 6: 0.08000041, 7: 0.080000415, 8: 0.080000415, 9: 0.08000042}\n",
      "486 {0: 0.088889, 1: 0.088889, 2: 0.08888902, 3: 0.19999892, 4: 0.088889, 5: 0.08888901, 6: 0.088889, 7: 0.088889, 8: 0.088889, 9: 0.088889}\n",
      "487 {0: 0.08000028, 1: 0.08000028, 2: 0.080000326, 3: 0.080000274, 4: 0.08000028, 5: 0.0800003, 6: 0.27999735, 7: 0.08000029, 8: 0.08000029, 9: 0.080000296}\n",
      "488 {0: 0.088889234, 1: 0.08888924, 2: 0.08888929, 3: 0.08888923, 4: 0.19999677, 5: 0.08888926, 6: 0.08888924, 7: 0.08888924, 8: 0.08888925, 9: 0.08888925}\n",
      "489 {0: 0.08888905, 1: 0.08888905, 2: 0.08888907, 3: 0.08888904, 4: 0.08888905, 5: 0.08888906, 6: 0.19999853, 7: 0.088889055, 8: 0.088889055, 9: 0.088889055}\n",
      "490 {0: 0.08888897, 1: 0.08888897, 2: 0.19999923, 3: 0.08888897, 4: 0.08888897, 5: 0.08888898, 6: 0.08888897, 7: 0.08888897, 8: 0.08888897, 9: 0.08888897}\n",
      "491 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "492 {0: 0.1999967, 1: 0.08888925, 2: 0.08888931, 3: 0.08888924, 4: 0.08888925, 5: 0.08888927, 6: 0.08888925, 7: 0.08888925, 8: 0.088889256, 9: 0.08888926}\n",
      "493 {0: 0.050000746, 1: 0.17499763, 2: 0.050000865, 3: 0.11250083, 4: 0.050000746, 5: 0.0500008, 6: 0.05000075, 7: 0.2999992, 8: 0.050000764, 9: 0.11249766}\n",
      "494 {0: 0.04210616, 1: 0.1473645, 2: 0.09473565, 3: 0.042106133, 4: 0.04210616, 5: 0.09473598, 6: 0.042106166, 7: 0.09473369, 8: 0.35789934, 9: 0.042106196}\n",
      "495 {0: 0.0727275, 1: 0.16363509, 2: 0.16363657, 3: 0.16363585, 4: 0.0727275, 5: 0.07272752, 6: 0.0727275, 7: 0.0727275, 8: 0.0727275, 9: 0.07272751}\n",
      "496 {0: 0.2545434, 1: 0.07272793, 2: 0.07272804, 3: 0.07272791, 4: 0.072727926, 5: 0.07272798, 6: 0.07272793, 7: 0.16363299, 8: 0.07272795, 9: 0.072727956}\n",
      "497 {0: 0.23332773, 1: 0.06666776, 2: 0.15000013, 3: 0.06666772, 4: 0.06666775, 5: 0.066667825, 6: 0.14999774, 7: 0.066667765, 8: 0.06666777, 9: 0.066667795}\n",
      "498 {0: 0.058064032, 1: 0.31613755, 2: 0.058066163, 3: 0.1870996, 4: 0.025807444, 5: 0.058060132, 6: 0.058061674, 7: 0.025807463, 8: 0.15483724, 9: 0.05805873}\n",
      "499 {0: 0.20000117, 1: 0.057144087, 2: 0.057144273, 3: 0.12856866, 4: 0.05714408, 5: 0.12857093, 6: 0.12856953, 7: 0.057144105, 8: 0.12856905, 9: 0.05714413}\n",
      "500 {0: 0.08888929, 1: 0.0888893, 2: 0.08888936, 3: 0.19999622, 4: 0.08888929, 5: 0.08888933, 6: 0.08888929, 7: 0.0888893, 8: 0.08888931, 9: 0.088889316}\n",
      "501 {0: 0.13845778, 1: 0.061539218, 2: 0.06153933, 3: 0.13846259, 4: 0.06153921, 5: 0.061539266, 6: 0.13845974, 7: 0.2153844, 8: 0.06153923, 9: 0.06153924}\n",
      "502 {0: 0.09473054, 1: 0.09473372, 2: 0.042106528, 3: 0.25263304, 4: 0.14736949, 5: 0.04210644, 6: 0.042106364, 7: 0.20000114, 8: 0.042106383, 9: 0.042106405}\n",
      "503 {0: 0.08000038, 1: 0.080000386, 2: 0.080000445, 3: 0.18000017, 4: 0.08000038, 5: 0.17999671, 6: 0.080000386, 7: 0.080000386, 8: 0.080000386, 9: 0.08000039}\n",
      "504 {0: 0.061539255, 1: 0.061539263, 2: 0.06153938, 3: 0.061539233, 4: 0.061539255, 5: 0.061539315, 6: 0.2153829, 7: 0.2153827, 8: 0.1384594, 9: 0.061539292}\n",
      "505 {0: 0.08888922, 1: 0.08888922, 2: 0.08888927, 3: 0.088889204, 4: 0.08888922, 5: 0.19999693, 6: 0.08888922, 7: 0.08888923, 8: 0.08888923, 9: 0.088889234}\n",
      "506 {0: 0.13333426, 1: 0.22856629, 2: 0.08571762, 3: 0.11133761, 4: 0.038096495, 5: 0.03809659, 6: 0.08571219, 7: 0.08571272, 8: 0.15532966, 9: 0.03809655}\n",
      "507 {0: 0.07272771, 1: 0.07272772, 2: 0.072727785, 3: 0.0727277, 4: 0.1636367, 5: 0.16363305, 6: 0.07272772, 7: 0.072727725, 8: 0.16363615, 9: 0.07272773}\n",
      "508 {0: 0.088889144, 1: 0.088889144, 2: 0.08888918, 3: 0.08888914, 4: 0.088889144, 5: 0.19999771, 6: 0.088889144, 7: 0.088889144, 8: 0.088889144, 9: 0.08888915}\n",
      "509 {0: 0.038096257, 1: 0.13332975, 2: 0.27619794, 3: 0.038096227, 4: 0.08571226, 5: 0.08571387, 6: 0.13333121, 7: 0.08571352, 8: 0.03809628, 9: 0.0857127}\n",
      "510 {0: 0.053333774, 1: 0.053333778, 2: 0.12000083, 3: 0.12000009, 4: 0.25333217, 5: 0.18666421, 6: 0.053333778, 7: 0.05333378, 8: 0.053333785, 9: 0.053333793}\n",
      "511 {0: 0.088889375, 1: 0.08888938, 2: 0.08888946, 3: 0.08888937, 4: 0.19999549, 5: 0.08888941, 6: 0.08888938, 7: 0.08888939, 8: 0.08888939, 9: 0.088889405}\n",
      "512 {0: 0.088889465, 1: 0.19999465, 2: 0.08888956, 3: 0.08888945, 4: 0.088889465, 5: 0.08888951, 6: 0.088889465, 7: 0.08888947, 8: 0.08888948, 9: 0.08888949}\n",
      "513 {0: 0.072727874, 1: 0.072727874, 2: 0.16363749, 3: 0.07272785, 4: 0.072727874, 5: 0.07272792, 6: 0.16363364, 7: 0.07272788, 8: 0.16363372, 9: 0.072727896}\n",
      "514 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "515 {0: 0.05000058, 1: 0.4249991, 2: 0.05000067, 3: 0.050000563, 4: 0.05000058, 5: 0.17499618, 6: 0.050000582, 7: 0.05000059, 8: 0.050000593, 9: 0.050000604}\n",
      "516 {0: 0.080000676, 1: 0.08000068, 2: 0.08000079, 3: 0.08000066, 4: 0.080000676, 5: 0.08000073, 6: 0.27999374, 7: 0.08000069, 8: 0.0800007, 9: 0.080000706}\n",
      "517 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "518 {0: 0.08888911, 1: 0.08888911, 2: 0.08888914, 3: 0.0888891, 4: 0.08888911, 5: 0.08888912, 6: 0.19999799, 7: 0.088889115, 8: 0.088889115, 9: 0.088889115}\n",
      "519 {0: 0.16362952, 1: 0.07272938, 2: 0.0727297, 3: 0.07272931, 4: 0.16362873, 5: 0.07272952, 6: 0.07272938, 7: 0.07272941, 8: 0.1636356, 9: 0.07272945}\n",
      "520 {0: 0.08000023, 1: 0.08000024, 2: 0.080000274, 3: 0.08000023, 4: 0.08000023, 5: 0.27999786, 6: 0.08000024, 7: 0.08000024, 8: 0.08000024, 9: 0.080000244}\n",
      "521 {0: 0.13846119, 1: 0.06153908, 2: 0.13846292, 3: 0.061539054, 4: 0.061539073, 5: 0.06153912, 6: 0.06153908, 7: 0.21538232, 8: 0.061539087, 9: 0.13845907}\n",
      "522 {0: 0.08888907, 1: 0.19999826, 2: 0.08888911, 3: 0.08888906, 4: 0.08888907, 5: 0.088889085, 6: 0.08888907, 7: 0.08888908, 8: 0.08888908, 9: 0.08888908}\n",
      "523 {0: 0.17999682, 1: 0.08000043, 2: 0.0800005, 3: 0.17999963, 4: 0.08000042, 5: 0.08000046, 6: 0.08000043, 7: 0.08000044, 8: 0.08000044, 9: 0.080000445}\n",
      "524 {0: 0.08888917, 1: 0.08888917, 2: 0.19999745, 3: 0.08888916, 4: 0.08888917, 5: 0.08888919, 6: 0.08888917, 7: 0.088889174, 8: 0.088889174, 9: 0.08888918}\n",
      "525 {0: 0.13846415, 1: 0.061539967, 2: 0.06154019, 3: 0.06153991, 4: 0.29229438, 5: 0.13846146, 6: 0.061539963, 7: 0.061539985, 8: 0.06153999, 9: 0.061540015}\n",
      "526 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "527 {0: 0.061539214, 1: 0.061539225, 2: 0.061539337, 3: 0.1384589, 4: 0.061539214, 5: 0.2923073, 6: 0.1384591, 7: 0.061539233, 8: 0.061539236, 9: 0.061539248}\n",
      "528 {0: 0.061539102, 1: 0.061539106, 2: 0.061539207, 3: 0.21538582, 4: 0.061539102, 5: 0.13845912, 6: 0.061539106, 7: 0.2153812, 8: 0.06153912, 9: 0.06153913}\n",
      "529 {0: 0.23332836, 1: 0.06666732, 2: 0.066667415, 3: 0.066667296, 4: 0.15000068, 5: 0.06666736, 6: 0.06666732, 7: 0.066667326, 8: 0.06666733, 9: 0.14999963}\n",
      "530 {0: 0.057144128, 1: 0.05714414, 2: 0.05714433, 3: 0.05714409, 4: 0.12856984, 5: 0.057144225, 6: 0.12856287, 7: 0.05714415, 8: 0.2000039, 9: 0.19999833}\n",
      "531 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "532 {0: 0.19999722, 1: 0.08888919, 2: 0.08888924, 3: 0.08888918, 4: 0.08888919, 5: 0.08888921, 6: 0.08888919, 7: 0.0888892, 8: 0.0888892, 9: 0.088889204}\n",
      "533 {0: 0.034783605, 1: 0.0782542, 2: 0.03478377, 3: 0.20869955, 4: 0.07826151, 5: 0.034783684, 6: 0.12174121, 7: 0.12173798, 8: 0.16521613, 9: 0.121738315}\n",
      "534 {0: 0.08888926, 1: 0.08888926, 2: 0.08888932, 3: 0.08888925, 4: 0.19999653, 5: 0.088889286, 6: 0.08888926, 7: 0.08888927, 8: 0.08888927, 9: 0.08888928}\n",
      "535 {0: 0.13846204, 1: 0.1384621, 2: 0.061539225, 3: 0.061539102, 4: 0.06153912, 5: 0.13845891, 6: 0.061539125, 7: 0.21538211, 8: 0.061539136, 9: 0.061539147}\n",
      "536 {0: 0.03809567, 1: 0.038095675, 2: 0.03809574, 3: 0.04299081, 4: 0.08571453, 5: 0.0380957, 6: 0.038095675, 7: 0.27619046, 8: 0.36653, 9: 0.03809569}\n",
      "537 {0: 0.16363734, 1: 0.07272811, 2: 0.07272824, 3: 0.16363378, 4: 0.072728105, 5: 0.16363192, 6: 0.072728105, 7: 0.07272812, 8: 0.07272813, 9: 0.07272814}\n",
      "538 {0: 0.088888995, 1: 0.088889, 2: 0.08888902, 3: 0.088888995, 4: 0.088889, 5: 0.08888901, 6: 0.088889, 7: 0.088889, 8: 0.19999897, 9: 0.088889}\n",
      "539 {0: 0.17999718, 1: 0.080000594, 2: 0.08000068, 3: 0.08000057, 4: 0.080000594, 5: 0.08000063, 6: 0.17999795, 7: 0.0800006, 8: 0.0800006, 9: 0.08000062}\n",
      "540 {0: 0.06666733, 1: 0.14999837, 2: 0.066667445, 3: 0.23333192, 4: 0.06666733, 5: 0.14999814, 6: 0.06666734, 7: 0.06666735, 8: 0.066667356, 9: 0.06666736}\n",
      "541 {0: 0.08888893, 1: 0.08888893, 2: 0.19999963, 3: 0.08888893, 4: 0.08888893, 5: 0.08888893, 6: 0.08888893, 7: 0.08888893, 8: 0.08888893, 9: 0.08888893}\n",
      "542 {0: 0.17999715, 1: 0.08000057, 2: 0.080000654, 3: 0.17999817, 4: 0.080000564, 5: 0.0800006, 6: 0.08000057, 7: 0.08000057, 8: 0.08000057, 9: 0.08000059}\n",
      "543 {0: 0.25333303, 1: 0.053334426, 2: 0.05333459, 3: 0.053334385, 4: 0.05333442, 5: 0.053334497, 6: 0.18665929, 7: 0.18666646, 8: 0.053334445, 9: 0.053334467}\n",
      "544 {0: 0.08888906, 1: 0.08888906, 2: 0.088889085, 3: 0.088889055, 4: 0.19999838, 5: 0.08888908, 6: 0.08888906, 7: 0.08888906, 8: 0.08888906, 9: 0.08888907}\n",
      "545 {0: 0.07199613, 1: 0.23199904, 2: 0.032000925, 3: 0.111997604, 4: 0.071999416, 5: 0.19200304, 6: 0.032000802, 7: 0.07199932, 8: 0.15200287, 9: 0.032000832}\n",
      "546 {0: 0.08888897, 1: 0.08888897, 2: 0.19999921, 3: 0.08888897, 4: 0.08888897, 5: 0.08888898, 6: 0.08888897, 7: 0.08888897, 8: 0.08888897, 9: 0.08888897}\n",
      "547 {0: 0.08888895, 1: 0.08888895, 2: 0.1999995, 3: 0.08888895, 4: 0.08888895, 5: 0.08888895, 6: 0.08888895, 7: 0.08888895, 8: 0.08888895, 9: 0.08888895}\n",
      "548 {0: 0.08888919, 1: 0.08888919, 2: 0.08888924, 3: 0.08888918, 4: 0.08888919, 5: 0.08888921, 6: 0.08888919, 7: 0.19999722, 8: 0.0888892, 9: 0.088889204}\n",
      "549 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "550 {0: 0.17999828, 1: 0.080000386, 2: 0.17999868, 3: 0.08000037, 4: 0.08000038, 5: 0.08000041, 6: 0.080000386, 7: 0.080000386, 8: 0.080000386, 9: 0.08000039}\n",
      "551 {0: 0.08888927, 1: 0.08888928, 2: 0.08888934, 3: 0.08888926, 4: 0.08888927, 5: 0.19999646, 6: 0.08888928, 7: 0.08888928, 8: 0.088889286, 9: 0.08888929}\n",
      "552 {0: 0.07272774, 1: 0.25454256, 2: 0.072727814, 3: 0.072727725, 4: 0.1636354, 5: 0.07272777, 6: 0.07272774, 7: 0.07272775, 8: 0.07272775, 9: 0.07272776}\n",
      "553 {0: 0.030770045, 1: 0.14615162, 2: 0.0692321, 3: 0.107694305, 4: 0.06922764, 5: 0.18461907, 6: 0.14615305, 7: 0.03077006, 8: 0.10769091, 9: 0.10769119}\n",
      "554 {0: 0.042106465, 1: 0.14736529, 2: 0.042106654, 3: 0.14736992, 4: 0.09473286, 5: 0.042106554, 6: 0.14736918, 7: 0.20000277, 8: 0.042106494, 9: 0.09473378}\n",
      "555 {0: 0.08888939, 1: 0.0888894, 2: 0.08888948, 3: 0.088889375, 4: 0.08888939, 5: 0.088889435, 6: 0.0888894, 7: 0.19999526, 8: 0.088889405, 9: 0.08888942}\n",
      "556 {0: 0.06666695, 1: 0.06666696, 2: 0.15000018, 3: 0.14999963, 4: 0.14999896, 5: 0.066666976, 6: 0.14999947, 7: 0.06666696, 8: 0.06666697, 9: 0.06666697}\n",
      "557 {0: 0.080000736, 1: 0.17999747, 2: 0.080000855, 3: 0.08000072, 4: 0.080000736, 5: 0.08000079, 6: 0.08000074, 7: 0.08000075, 8: 0.08000075, 9: 0.17999645}\n",
      "558 {0: 0.08000071, 1: 0.08000072, 2: 0.08000083, 3: 0.0800007, 4: 0.08000071, 5: 0.17999732, 6: 0.08000072, 7: 0.080000736, 8: 0.1799968, 9: 0.08000075}\n",
      "559 {0: 0.08888934, 1: 0.1999958, 2: 0.08888942, 3: 0.08888933, 4: 0.08888934, 5: 0.08888938, 6: 0.088889346, 7: 0.08888935, 8: 0.08888935, 9: 0.08888936}\n",
      "560 {0: 0.08888895, 1: 0.08888895, 2: 0.08888896, 3: 0.19999944, 4: 0.08888895, 5: 0.08888896, 6: 0.08888895, 7: 0.08888895, 8: 0.08888895, 9: 0.08888895}\n",
      "561 {0: 0.11249951, 1: 0.05000057, 2: 0.050000656, 3: 0.17500117, 4: 0.11250042, 5: 0.23749965, 6: 0.11249628, 7: 0.05000058, 8: 0.05000058, 9: 0.05000059}\n",
      "562 {0: 0.08888903, 1: 0.08888904, 2: 0.088889055, 3: 0.08888903, 4: 0.1999987, 5: 0.08888904, 6: 0.08888904, 7: 0.08888904, 8: 0.08888904, 9: 0.08888904}\n",
      "563 {0: 0.08888908, 1: 0.08888908, 2: 0.088889115, 3: 0.08888908, 4: 0.19999827, 5: 0.08888909, 6: 0.08888908, 7: 0.088889085, 8: 0.088889085, 9: 0.088889085}\n",
      "564 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "565 {0: 0.08888925, 1: 0.08888925, 2: 0.08888931, 3: 0.08888924, 4: 0.08888925, 5: 0.08888928, 6: 0.19999668, 7: 0.088889256, 8: 0.088889256, 9: 0.08888926}\n",
      "566 {0: 0.03809598, 1: 0.18094862, 2: 0.0380961, 3: 0.03809596, 4: 0.085715145, 5: 0.08571424, 6: 0.37143156, 7: 0.038095996, 8: 0.038096, 9: 0.08571038}\n",
      "567 {0: 0.08888919, 1: 0.08888919, 2: 0.08888924, 3: 0.08888918, 4: 0.08888919, 5: 0.19999719, 6: 0.08888919, 7: 0.0888892, 8: 0.0888892, 9: 0.088889204}\n",
      "568 {0: 0.08000024, 1: 0.080000244, 2: 0.18000023, 3: 0.08000024, 4: 0.080000244, 5: 0.17999783, 6: 0.080000244, 7: 0.080000244, 8: 0.08000025, 9: 0.08000025}\n",
      "569 {0: 0.16363615, 1: 0.072727636, 2: 0.072727695, 3: 0.07272762, 4: 0.072727636, 5: 0.07272766, 6: 0.072727636, 7: 0.25454262, 8: 0.07272764, 9: 0.07272765}\n",
      "570 {0: 0.17500219, 1: 0.17499991, 2: 0.05000126, 3: 0.05000106, 4: 0.05000109, 5: 0.11249912, 6: 0.11250144, 7: 0.11249423, 8: 0.050001115, 9: 0.11249855}\n",
      "571 {0: 0.08000073, 1: 0.17999747, 2: 0.08000085, 3: 0.080000706, 4: 0.08000073, 5: 0.08000078, 6: 0.080000736, 7: 0.17999652, 8: 0.08000074, 9: 0.08000076}\n",
      "572 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "573 {0: 0.16470367, 1: 0.047059692, 2: 0.105882354, 3: 0.10588363, 4: 0.1058773, 5: 0.04705975, 6: 0.28235447, 7: 0.0470597, 8: 0.047059704, 9: 0.047059722}\n",
      "574 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "575 {0: 0.080000535, 1: 0.08000054, 2: 0.17999916, 3: 0.08000052, 4: 0.080000535, 5: 0.08000058, 6: 0.08000054, 7: 0.17999648, 8: 0.08000055, 9: 0.08000056}\n",
      "576 {0: 0.08000038, 1: 0.080000386, 2: 0.18000065, 3: 0.08000037, 4: 0.08000038, 5: 0.17999628, 6: 0.080000386, 7: 0.080000386, 8: 0.080000386, 9: 0.08000039}\n",
      "577 {0: 0.088889, 1: 0.088889, 2: 0.08888902, 3: 0.19999892, 4: 0.088889, 5: 0.08888901, 6: 0.088889, 7: 0.088889, 8: 0.088889, 9: 0.088889}\n",
      "578 {0: 0.0727281, 1: 0.072728105, 2: 0.072728224, 3: 0.16363694, 4: 0.0727281, 5: 0.07272816, 6: 0.072728105, 7: 0.1636341, 8: 0.07272812, 9: 0.1636321}\n",
      "579 {0: 0.09000032, 1: 0.040000927, 2: 0.040001065, 3: 0.1399986, 4: 0.04000092, 5: 0.19000031, 6: 0.089997075, 7: 0.040000938, 8: 0.24000452, 9: 0.08999534}\n",
      "580 {0: 0.23749849, 1: 0.1749983, 2: 0.05000084, 3: 0.050000705, 4: 0.11250101, 5: 0.05000078, 6: 0.05000073, 7: 0.1125007, 8: 0.050000742, 9: 0.1124977}\n",
      "581 {0: 0.0727278, 1: 0.07272781, 2: 0.07272788, 3: 0.072727785, 4: 0.16363692, 5: 0.072727844, 6: 0.07272781, 7: 0.072727814, 8: 0.16363229, 9: 0.16363604}\n",
      "582 {0: 0.07272792, 1: 0.1636342, 2: 0.07272802, 3: 0.0727279, 4: 0.07272792, 5: 0.07272797, 6: 0.25454226, 7: 0.07272793, 8: 0.07272793, 9: 0.07272795}\n",
      "583 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "584 {0: 0.088889316, 1: 0.19999605, 2: 0.088889375, 3: 0.0888893, 4: 0.088889316, 5: 0.08888934, 6: 0.088889316, 7: 0.088889316, 8: 0.088889316, 9: 0.08888932}\n",
      "585 {0: 0.12000109, 1: 0.05333441, 2: 0.05333458, 3: 0.053334378, 4: 0.053334404, 5: 0.11999855, 6: 0.11999595, 7: 0.18666615, 8: 0.11999988, 9: 0.1200006}\n",
      "586 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "587 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "588 {0: 0.17999642, 1: 0.080000736, 2: 0.08000084, 3: 0.080000706, 4: 0.08000073, 5: 0.08000078, 6: 0.080000736, 7: 0.17999756, 8: 0.08000074, 9: 0.08000076}\n",
      "589 {0: 0.17272654, 1: 0.08181568, 2: 0.08182119, 3: 0.036364764, 4: 0.036364794, 5: 0.03636488, 6: 0.17272797, 7: 0.21818438, 8: 0.036364824, 9: 0.12726499}\n",
      "590 {0: 0.1999883, 1: 0.08889016, 2: 0.08889035, 3: 0.08889011, 4: 0.08889014, 5: 0.08889025, 6: 0.08889016, 7: 0.08889017, 8: 0.08889018, 9: 0.0888902}\n",
      "591 {0: 0.07272757, 1: 0.07272757, 2: 0.2545463, 3: 0.07272756, 4: 0.07272757, 5: 0.07272759, 6: 0.07272757, 7: 0.072727576, 8: 0.072727576, 9: 0.1636331}\n",
      "592 {0: 0.04444547, 1: 0.15555254, 2: 0.044445638, 3: 0.21111053, 4: 0.04444547, 5: 0.15555492, 6: 0.09999789, 7: 0.044445496, 8: 0.0444455, 9: 0.15555653}\n",
      "593 {0: 0.033334054, 1: 0.03333406, 2: 0.03333417, 3: 0.11666311, 4: 0.074999906, 5: 0.07499781, 6: 0.15833314, 7: 0.03333407, 8: 0.11666538, 9: 0.32500428}\n",
      "594 {0: 0.08000018, 1: 0.08000018, 2: 0.18000004, 3: 0.08000018, 4: 0.08000018, 5: 0.08000019, 6: 0.17999847, 7: 0.08000018, 8: 0.08000018, 9: 0.080000184}\n",
      "595 {0: 0.07272798, 1: 0.072727986, 2: 0.0727281, 3: 0.1636331, 4: 0.16363738, 5: 0.07272803, 6: 0.1636334, 7: 0.07272799, 8: 0.07272799, 9: 0.07272801}\n",
      "596 {0: 0.1217416, 1: 0.20869684, 2: 0.078261845, 3: 0.07825257, 4: 0.078261904, 5: 0.034784146, 6: 0.121739715, 7: 0.078259125, 8: 0.16521813, 9: 0.034784097}\n",
      "597 {0: 0.072727926, 1: 0.07272793, 2: 0.16363584, 3: 0.07272791, 4: 0.072727926, 5: 0.07272798, 6: 0.07272793, 7: 0.1636323, 8: 0.07272794, 9: 0.16363633}\n",
      "598 {0: 0.13845448, 1: 0.21538346, 2: 0.13846451, 3: 0.061539557, 4: 0.06153959, 5: 0.13845992, 6: 0.061539594, 7: 0.061539613, 8: 0.061539616, 9: 0.061539635}\n",
      "599 {0: 0.04000053, 1: 0.04000053, 2: 0.09000049, 3: 0.09000015, 4: 0.14000116, 5: 0.04000057, 6: 0.08999709, 7: 0.08999759, 8: 0.34000134, 9: 0.04000055}\n",
      "600 {0: 0.06666702, 1: 0.14999983, 2: 0.06666708, 3: 0.23333368, 4: 0.06666702, 5: 0.14999726, 6: 0.06666702, 7: 0.06666702, 8: 0.06666703, 9: 0.066667035}\n",
      "601 {0: 0.047060125, 1: 0.10586924, 2: 0.10588501, 3: 0.047060087, 4: 0.105883755, 5: 0.04706022, 6: 0.10588144, 7: 0.04706015, 8: 0.28236046, 9: 0.10587946}\n",
      "602 {0: 0.061539207, 1: 0.13846152, 2: 0.061539326, 3: 0.06153918, 4: 0.061539207, 5: 0.06153926, 6: 0.06153921, 7: 0.13846071, 8: 0.13845922, 9: 0.2153832}\n",
      "603 {0: 0.17999268, 1: 0.08000107, 2: 0.08000124, 3: 0.080001034, 4: 0.08000106, 5: 0.08000115, 6: 0.1799984, 7: 0.08000109, 8: 0.08000109, 9: 0.080001116}\n",
      "604 {0: 0.08000018, 1: 0.08000018, 2: 0.27999836, 3: 0.08000018, 4: 0.08000018, 5: 0.08000019, 6: 0.08000018, 7: 0.08000018, 8: 0.08000018, 9: 0.080000184}\n",
      "605 {0: 0.19999751, 1: 0.08888916, 2: 0.0888892, 3: 0.088889144, 4: 0.08888916, 5: 0.088889174, 6: 0.08888916, 7: 0.08888916, 8: 0.08888917, 9: 0.08888917}\n",
      "606 {0: 0.06666764, 1: 0.14999865, 2: 0.066667795, 3: 0.14999668, 4: 0.06666764, 5: 0.2333309, 6: 0.066667646, 7: 0.06666766, 8: 0.06666766, 9: 0.06666768}\n",
      "607 {0: 0.080000475, 1: 0.080000475, 2: 0.08000056, 3: 0.08000047, 4: 0.080000475, 5: 0.08000051, 6: 0.080000475, 7: 0.17999688, 8: 0.08000049, 9: 0.17999922}\n",
      "608 {0: 0.25454178, 1: 0.16363429, 2: 0.07272807, 3: 0.07272794, 4: 0.07272796, 5: 0.07272801, 6: 0.07272797, 7: 0.07272797, 8: 0.07272798, 9: 0.07272799}\n",
      "609 {0: 0.2000004, 1: 0.12857072, 2: 0.05714408, 3: 0.12857321, 4: 0.1285679, 5: 0.05714399, 6: 0.05714392, 7: 0.057143934, 8: 0.12856786, 9: 0.05714396}\n",
      "610 {0: 0.06666756, 1: 0.14999728, 2: 0.0666677, 3: 0.066667534, 4: 0.066667564, 5: 0.066667624, 6: 0.23333219, 7: 0.14999738, 8: 0.06666758, 9: 0.066667594}\n",
      "611 {0: 0.08888917, 1: 0.08888917, 2: 0.19999745, 3: 0.08888916, 4: 0.08888917, 5: 0.08888919, 6: 0.08888917, 7: 0.088889174, 8: 0.088889174, 9: 0.08888918}\n",
      "612 {0: 0.057143968, 1: 0.12856694, 2: 0.12857434, 3: 0.20000258, 4: 0.05714397, 5: 0.057144053, 6: 0.12856394, 7: 0.057143994, 8: 0.057143997, 9: 0.12857223}\n",
      "613 {0: 0.0800006, 1: 0.0800006, 2: 0.0800007, 3: 0.08000059, 4: 0.0800006, 5: 0.08000065, 6: 0.17999686, 7: 0.17999817, 8: 0.08000062, 9: 0.08000063}\n",
      "614 {0: 0.18823807, 1: 0.041171778, 2: 0.04117801, 3: 0.14412075, 4: 0.070589036, 5: 0.041174576, 6: 0.12941197, 7: 0.17353247, 8: 0.055877093, 9: 0.11470625}\n",
      "615 {0: 0.08888933, 1: 0.08888934, 2: 0.088889405, 3: 0.088889316, 4: 0.08888933, 5: 0.08888937, 6: 0.08888934, 7: 0.19999588, 8: 0.088889346, 9: 0.08888935}\n",
      "616 {0: 0.08888976, 1: 0.19999187, 2: 0.088889904, 3: 0.08888974, 4: 0.08888976, 5: 0.08888983, 6: 0.08888977, 7: 0.088889785, 8: 0.088889785, 9: 0.0888898}\n",
      "617 {0: 0.061539844, 1: 0.21537474, 2: 0.061540067, 3: 0.13845947, 4: 0.13846423, 5: 0.061539948, 6: 0.061539855, 7: 0.06153987, 8: 0.06153988, 9: 0.13846207}\n",
      "618 {0: 0.08888907, 1: 0.19999826, 2: 0.08888911, 3: 0.08888906, 4: 0.08888907, 5: 0.088889085, 6: 0.08888907, 7: 0.08888908, 8: 0.08888908, 9: 0.08888908}\n",
      "619 {0: 0.044445198, 1: 0.09999627, 2: 0.10000121, 3: 0.09999518, 4: 0.044445198, 5: 0.044445254, 6: 0.37778145, 7: 0.044445217, 8: 0.04444522, 9: 0.09999978}\n",
      "620 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "621 {0: 0.1384614, 1: 0.06153886, 2: 0.06153892, 3: 0.06153885, 4: 0.29230604, 5: 0.061538883, 6: 0.061538856, 7: 0.061538864, 8: 0.061538864, 9: 0.13846046}\n",
      "622 {0: 0.10768706, 1: 0.030770117, 2: 0.069230676, 3: 0.030770088, 4: 0.14615338, 5: 0.1461535, 6: 0.2615415, 7: 0.107692406, 8: 0.030770132, 9: 0.069231175}\n",
      "623 {0: 0.112492494, 1: 0.112500794, 2: 0.050001252, 3: 0.05000105, 4: 0.17500201, 5: 0.11249864, 6: 0.05000109, 7: 0.17500003, 8: 0.11250154, 9: 0.05000113}\n",
      "624 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "625 {0: 0.11999364, 1: 0.12000213, 2: 0.053334814, 3: 0.11999731, 4: 0.053334612, 5: 0.11999962, 6: 0.18666808, 7: 0.12000051, 8: 0.053334642, 9: 0.05333467}\n",
      "626 {0: 0.08888925, 1: 0.08888925, 2: 0.08888931, 3: 0.08888924, 4: 0.08888925, 5: 0.19999672, 6: 0.08888925, 7: 0.088889256, 8: 0.088889256, 9: 0.08888926}\n",
      "627 {0: 0.072727375, 1: 0.072727375, 2: 0.1636362, 3: 0.25454473, 4: 0.072727375, 5: 0.07272739, 6: 0.072727375, 7: 0.07272738, 8: 0.07272738, 9: 0.07272738}\n",
      "628 {0: 0.08888895, 1: 0.08888895, 2: 0.08888896, 3: 0.1999994, 4: 0.08888895, 5: 0.08888896, 6: 0.08888895, 7: 0.08888895, 8: 0.08888895, 9: 0.08888895}\n",
      "629 {0: 0.13845941, 1: 0.13846044, 2: 0.06153904, 3: 0.061538946, 4: 0.06153896, 5: 0.061538998, 6: 0.061538965, 7: 0.06153897, 8: 0.2923073, 9: 0.06153898}\n",
      "630 {0: 0.118750036, 1: 0.11874982, 2: 0.056251794, 3: 0.08750111, 4: 0.11874805, 5: 0.025000904, 6: 0.056245156, 7: 0.025000859, 8: 0.30625162, 9: 0.087500654}\n",
      "631 {0: 0.08888926, 1: 0.08888927, 2: 0.08888932, 3: 0.088889256, 4: 0.08888926, 5: 0.19999649, 6: 0.08888927, 7: 0.08888927, 8: 0.08888928, 9: 0.08888928}\n",
      "632 {0: 0.09999715, 1: 0.0444452, 2: 0.044445314, 3: 0.044445176, 4: 0.21111344, 5: 0.09999907, 6: 0.044445198, 7: 0.15555331, 8: 0.21111098, 9: 0.044445224}\n",
      "633 {0: 0.1285725, 1: 0.19999635, 2: 0.05714408, 3: 0.057143882, 4: 0.05714391, 5: 0.1285663, 6: 0.05714392, 7: 0.057143934, 8: 0.057143938, 9: 0.2000012}\n",
      "634 {0: 0.038095858, 1: 0.03809586, 2: 0.03809596, 3: 0.1333322, 4: 0.2285699, 5: 0.038095903, 6: 0.03809586, 7: 0.22857189, 8: 0.038095877, 9: 0.1809507}\n",
      "635 {0: 0.03528908, 1: 0.21176763, 2: 0.07451075, 3: 0.25098604, 4: 0.11372374, 5: 0.03529178, 6: 0.015686788, 7: 0.13333312, 8: 0.113724254, 9: 0.015686806}\n",
      "636 {0: 0.061539087, 1: 0.061539095, 2: 0.13846295, 3: 0.061539073, 4: 0.06153909, 5: 0.06153914, 6: 0.21538387, 7: 0.13845971, 8: 0.1384589, 9: 0.06153912}\n",
      "637 {0: 0.27200338, 1: 0.071997404, 2: 0.15200341, 3: 0.19199677, 4: 0.032000855, 5: 0.03200092, 6: 0.072000794, 7: 0.071996525, 8: 0.032000877, 9: 0.07199907}\n",
      "638 {0: 0.061538797, 1: 0.0615388, 2: 0.061538853, 3: 0.21538395, 4: 0.061538797, 5: 0.061538827, 6: 0.0615388, 7: 0.061538808, 8: 0.061538808, 9: 0.2923056}\n",
      "639 {0: 0.21110873, 1: 0.15555564, 2: 0.04444585, 3: 0.099993534, 4: 0.044445656, 5: 0.044445746, 6: 0.10000097, 7: 0.044445682, 8: 0.044445686, 9: 0.2111125}\n",
      "640 {0: 0.0444456, 1: 0.09999409, 2: 0.15555876, 3: 0.09999725, 4: 0.10000188, 5: 0.10000198, 6: 0.099999815, 7: 0.09999694, 8: 0.04444563, 9: 0.15555808}\n",
      "641 {0: 0.11525361, 1: 0.13220294, 2: 0.013559887, 3: 0.09830772, 4: 0.030508624, 5: 0.14915256, 6: 0.25085267, 7: 0.064403, 8: 0.13219915, 9: 0.013559831}\n",
      "642 {0: 0.088889286, 1: 0.088889286, 2: 0.088889346, 3: 0.08888927, 4: 0.088889286, 5: 0.088889316, 6: 0.088889286, 7: 0.19999635, 8: 0.08888929, 9: 0.0888893}\n",
      "643 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "644 {0: 0.08888913, 1: 0.08888913, 2: 0.08888917, 3: 0.08888912, 4: 0.08888913, 5: 0.088889144, 6: 0.08888913, 7: 0.19999781, 8: 0.08888914, 9: 0.08888914}\n",
      "645 {0: 0.08888927, 1: 0.08888928, 2: 0.08888934, 3: 0.08888926, 4: 0.08888927, 5: 0.0888893, 6: 0.19999638, 7: 0.088889286, 8: 0.088889286, 9: 0.08888929}\n",
      "646 {0: 0.08888915, 1: 0.08888916, 2: 0.0888892, 3: 0.088889144, 4: 0.08888915, 5: 0.19999756, 6: 0.08888916, 7: 0.08888916, 8: 0.08888917, 9: 0.08888917}\n",
      "647 {0: 0.053333927, 1: 0.12000037, 2: 0.11999831, 3: 0.12000033, 4: 0.053333927, 5: 0.053333968, 6: 0.18666705, 7: 0.053333938, 8: 0.05333394, 9: 0.18666425}\n",
      "648 {0: 0.061538942, 1: 0.061538946, 2: 0.06153902, 3: 0.13845667, 4: 0.061538942, 5: 0.06153898, 6: 0.061538946, 7: 0.061538953, 8: 0.061538957, 9: 0.36923167}\n",
      "649 {0: 0.15172404, 1: 0.1517255, 2: 0.013793921, 3: 0.04827391, 4: 0.08275639, 5: 0.09999828, 6: 0.15172896, 7: 0.11724136, 8: 0.117242485, 9: 0.06551513}\n",
      "650 {0: 0.053334154, 1: 0.120000936, 2: 0.11999901, 3: 0.18666738, 4: 0.053334154, 5: 0.120001085, 6: 0.11999698, 7: 0.05333417, 8: 0.053334173, 9: 0.11999796}\n",
      "651 {0: 0.1799991, 1: 0.08000034, 2: 0.08000039, 3: 0.080000326, 4: 0.08000034, 5: 0.08000036, 6: 0.17999811, 7: 0.08000035, 8: 0.08000035, 9: 0.08000035}\n",
      "652 {0: 0.027586626, 1: 0.09655052, 2: 0.062069673, 3: 0.096547015, 4: 0.027586626, 5: 0.37241933, 6: 0.13103424, 7: 0.027586633, 8: 0.027586635, 9: 0.13103268}\n",
      "653 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "654 {0: 0.0666669, 1: 0.0666669, 2: 0.23333171, 3: 0.23333305, 4: 0.0666669, 5: 0.066666916, 6: 0.0666669, 7: 0.0666669, 8: 0.0666669, 9: 0.06666691}\n",
      "655 {0: 0.0999995, 1: 0.044445258, 2: 0.10000194, 3: 0.15555689, 4: 0.04444525, 5: 0.21111071, 6: 0.10000071, 7: 0.09999831, 8: 0.099996194, 9: 0.044445287}\n",
      "656 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "657 {0: 0.17999277, 1: 0.08000077, 2: 0.08000089, 3: 0.18000083, 4: 0.080000766, 5: 0.080000825, 6: 0.08000077, 7: 0.08000078, 8: 0.08000078, 9: 0.0800008}\n",
      "658 {0: 0.066667475, 1: 0.066667475, 2: 0.15000196, 3: 0.066667445, 4: 0.066667475, 5: 0.15000106, 6: 0.14999534, 7: 0.14999677, 8: 0.06666749, 9: 0.066667505}\n",
      "659 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "660 {0: 0.028572578, 1: 0.06428397, 2: 0.028572759, 3: 0.13571224, 4: 0.38572797, 5: 0.028572664, 6: 0.09999044, 7: 0.064284906, 8: 0.099997565, 9: 0.06428489}\n",
      "661 {0: 0.08888896, 1: 0.088888966, 2: 0.08888897, 3: 0.08888896, 4: 0.19999929, 5: 0.088888966, 6: 0.08888896, 7: 0.088888966, 8: 0.088888966, 9: 0.088888966}\n",
      "662 {0: 0.08000116, 1: 0.080001175, 2: 0.080001354, 3: 0.08000113, 4: 0.17999025, 5: 0.08000125, 6: 0.08000117, 7: 0.18000014, 8: 0.08000119, 9: 0.08000121}\n",
      "663 {0: 0.22352824, 1: 0.16470622, 2: 0.047060158, 3: 0.047059946, 4: 0.047059976, 5: 0.10588095, 6: 0.16470468, 7: 0.04706, 8: 0.10587985, 9: 0.047060024}\n",
      "664 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "665 {0: 0.080000326, 1: 0.08000033, 2: 0.18000051, 3: 0.08000032, 4: 0.08000033, 5: 0.080000356, 6: 0.08000033, 7: 0.08000034, 8: 0.08000034, 9: 0.17999685}\n",
      "666 {0: 0.088889, 1: 0.088889, 2: 0.08888902, 3: 0.19999892, 4: 0.088889, 5: 0.08888901, 6: 0.088889, 7: 0.088889, 8: 0.088889, 9: 0.088889}\n",
      "667 {0: 0.08000065, 1: 0.27999392, 2: 0.08000076, 3: 0.08000063, 4: 0.08000065, 5: 0.0800007, 6: 0.080000654, 7: 0.08000066, 8: 0.08000067, 9: 0.080000676}\n",
      "668 {0: 0.039128106, 1: 0.16956532, 2: 0.10435068, 3: 0.14782886, 4: 0.19130613, 5: 0.017391965, 6: 0.039124846, 7: 0.104347356, 8: 0.10434763, 9: 0.08260911}\n",
      "669 {0: 0.06666785, 1: 0.31666824, 2: 0.06666803, 3: 0.06666781, 4: 0.06666785, 5: 0.06666794, 6: 0.066667855, 7: 0.06666787, 8: 0.06666788, 9: 0.14998868}\n",
      "670 {0: 0.088889465, 1: 0.088889465, 2: 0.08888956, 3: 0.19999458, 4: 0.088889465, 5: 0.08888951, 6: 0.088889465, 7: 0.08888947, 8: 0.08888948, 9: 0.08888949}\n",
      "671 {0: 0.08888905, 1: 0.08888905, 2: 0.08888907, 3: 0.08888904, 4: 0.08888905, 5: 0.19999854, 6: 0.08888905, 7: 0.08888905, 8: 0.088889055, 9: 0.088889055}\n",
      "672 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "673 {0: 0.088889934, 1: 0.08888995, 2: 0.08889011, 3: 0.08888991, 4: 0.19999024, 5: 0.08889002, 6: 0.08888995, 7: 0.088889964, 8: 0.08888997, 9: 0.08888999}\n",
      "674 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "675 {0: 0.08888926, 1: 0.08888926, 2: 0.08888932, 3: 0.08888925, 4: 0.19999653, 5: 0.088889286, 6: 0.08888926, 7: 0.08888927, 8: 0.08888927, 9: 0.08888928}\n",
      "676 {0: 0.07272754, 1: 0.07272754, 2: 0.16363607, 3: 0.07272753, 4: 0.07272754, 5: 0.25454363, 6: 0.07272754, 7: 0.07272754, 8: 0.07272754, 9: 0.072727546}\n",
      "677 {0: 0.02857211, 1: 0.028572114, 2: 0.064283915, 3: 0.06428571, 4: 0.02857211, 5: 0.09999867, 6: 0.09999832, 7: 0.17142609, 8: 0.028572127, 9: 0.38571888}\n",
      "678 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "679 {0: 0.16363297, 1: 0.0727277, 2: 0.16363718, 3: 0.16363594, 4: 0.072727695, 5: 0.072727725, 6: 0.0727277, 7: 0.0727277, 8: 0.0727277, 9: 0.07272772}\n",
      "680 {0: 0.04705973, 1: 0.105881624, 2: 0.047059875, 3: 0.2235275, 4: 0.1058828, 5: 0.16470537, 6: 0.047059737, 7: 0.04705975, 8: 0.16470385, 9: 0.04705977}\n",
      "681 {0: 0.25982094, 1: 0.119625844, 2: 0.016823016, 3: 0.14766286, 4: 0.13832174, 5: 0.04485748, 6: 0.035507437, 7: 0.06354873, 8: 0.11028082, 9: 0.06355115}\n",
      "682 {0: 0.08888917, 1: 0.08888917, 2: 0.19999745, 3: 0.08888916, 4: 0.08888917, 5: 0.08888919, 6: 0.08888917, 7: 0.088889174, 8: 0.088889174, 9: 0.08888918}\n",
      "683 {0: 0.066667266, 1: 0.06666728, 2: 0.06666737, 3: 0.06666725, 4: 0.06666727, 5: 0.14999746, 6: 0.06666727, 7: 0.31666422, 8: 0.06666729, 9: 0.066667296}\n",
      "684 {0: 0.07272853, 1: 0.07272854, 2: 0.07272873, 3: 0.0727285, 4: 0.07272853, 5: 0.1636357, 6: 0.07272854, 7: 0.16363664, 8: 0.07272856, 9: 0.1636277}\n",
      "685 {0: 0.12856676, 1: 0.12857248, 2: 0.057143852, 3: 0.12857272, 4: 0.057143714, 5: 0.057143778, 6: 0.057143718, 7: 0.057143733, 8: 0.057143737, 9: 0.27142555}\n",
      "686 {0: 0.088889115, 1: 0.08888912, 2: 0.08888915, 3: 0.08888911, 4: 0.088889115, 5: 0.08888914, 6: 0.19999789, 7: 0.08888912, 8: 0.08888912, 9: 0.08888913}\n",
      "687 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "688 {0: 0.08888919, 1: 0.08888919, 2: 0.08888924, 3: 0.08888918, 4: 0.08888919, 5: 0.08888921, 6: 0.08888919, 7: 0.0888892, 8: 0.19999722, 9: 0.088889204}\n",
      "689 {0: 0.08000038, 1: 0.080000386, 2: 0.080000445, 3: 0.08000037, 4: 0.08000038, 5: 0.08000041, 6: 0.080000386, 7: 0.17999755, 8: 0.080000386, 9: 0.1799993}\n",
      "690 {0: 0.08000035, 1: 0.080000356, 2: 0.08000041, 3: 0.18000008, 4: 0.08000035, 5: 0.08000038, 6: 0.080000356, 7: 0.080000356, 8: 0.080000356, 9: 0.17999704}\n",
      "691 {0: 0.16361128, 1: 0.072729655, 2: 0.16364016, 3: 0.07272957, 4: 0.07272964, 5: 0.16364095, 6: 0.072729655, 7: 0.072729684, 8: 0.0727297, 9: 0.072729744}\n",
      "692 {0: 0.08888893, 1: 0.08888893, 2: 0.19999962, 3: 0.08888893, 4: 0.08888893, 5: 0.08888893, 6: 0.08888893, 7: 0.08888893, 8: 0.08888893, 9: 0.08888893}\n",
      "693 {0: 0.088889465, 1: 0.088889465, 2: 0.088889554, 3: 0.1999947, 4: 0.088889465, 5: 0.08888951, 6: 0.088889465, 7: 0.08888947, 8: 0.08888947, 9: 0.08888949}\n",
      "694 {0: 0.08888923, 1: 0.08888923, 2: 0.08888928, 3: 0.08888921, 4: 0.08888923, 5: 0.08888925, 6: 0.08888923, 7: 0.088889234, 8: 0.088889234, 9: 0.19999693}\n",
      "695 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "696 {0: 0.08888954, 1: 0.19999395, 2: 0.08888964, 3: 0.08888952, 4: 0.08888954, 5: 0.088889584, 6: 0.08888955, 7: 0.088889554, 8: 0.088889554, 9: 0.08888956}\n",
      "697 {0: 0.07272824, 1: 0.07272825, 2: 0.0727284, 3: 0.07272822, 4: 0.16363645, 5: 0.16363226, 6: 0.07272825, 7: 0.16363335, 8: 0.07272826, 9: 0.07272828}\n",
      "698 {0: 0.08888945, 1: 0.19999477, 2: 0.08888955, 3: 0.08888944, 4: 0.08888945, 5: 0.0888895, 6: 0.08888946, 7: 0.088889465, 8: 0.088889465, 9: 0.08888948}\n",
      "699 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "700 {0: 0.22307993, 1: 0.069227085, 2: 0.03077015, 3: 0.22307883, 4: 0.06923054, 5: 0.030770084, 6: 0.03077003, 7: 0.107688874, 8: 0.1461535, 9: 0.069231}\n",
      "701 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "702 {0: 0.08888893, 1: 0.08888893, 2: 0.19999962, 3: 0.08888893, 4: 0.08888893, 5: 0.08888893, 6: 0.08888893, 7: 0.08888893, 8: 0.08888893, 9: 0.08888893}\n",
      "703 {0: 0.072727375, 1: 0.072727375, 2: 0.34545356, 3: 0.072727375, 4: 0.072727375, 5: 0.07272739, 6: 0.072727375, 7: 0.07272738, 8: 0.07272738, 9: 0.07272738}\n",
      "704 {0: 0.08888908, 1: 0.088889085, 2: 0.19999816, 3: 0.08888908, 4: 0.08888908, 5: 0.0888891, 6: 0.088889085, 7: 0.088889085, 8: 0.088889085, 9: 0.08888909}\n",
      "705 {0: 0.08000019, 1: 0.08000019, 2: 0.08000022, 3: 0.08000019, 4: 0.17999956, 5: 0.08000021, 6: 0.08000019, 7: 0.0800002, 8: 0.0800002, 9: 0.17999883}\n",
      "706 {0: 0.25128675, 1: 0.097436145, 2: 0.04615422, 3: 0.16786511, 4: 0.12307775, 5: 0.02051344, 6: 0.07179331, 7: 0.07179121, 8: 0.10393119, 9: 0.046150908}\n",
      "707 {0: 0.14999051, 1: 0.23333494, 2: 0.06666793, 3: 0.06666773, 4: 0.06666776, 5: 0.06666783, 6: 0.14999992, 7: 0.06666777, 8: 0.06666779, 9: 0.0666678}\n",
      "708 {0: 0.08000035, 1: 0.080000356, 2: 0.08000041, 3: 0.08000034, 4: 0.08000035, 5: 0.08000038, 6: 0.080000356, 7: 0.1799983, 8: 0.080000356, 9: 0.17999883}\n",
      "709 {0: 0.057143483, 1: 0.057143487, 2: 0.057143584, 3: 0.05714347, 4: 0.1285722, 5: 0.12856874, 6: 0.057143487, 7: 0.0571435, 8: 0.19999997, 9: 0.19999808}\n",
      "710 {0: 0.08888897, 1: 0.08888897, 2: 0.08888899, 3: 0.08888897, 4: 0.08888897, 5: 0.19999917, 6: 0.08888897, 7: 0.08888897, 8: 0.08888897, 9: 0.08888897}\n",
      "711 {0: 0.08888897, 1: 0.08888897, 2: 0.08888899, 3: 0.08888897, 4: 0.08888897, 5: 0.08888898, 6: 0.08888897, 7: 0.19999915, 8: 0.08888897, 9: 0.08888897}\n",
      "712 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "713 {0: 0.053334013, 1: 0.05333402, 2: 0.053334124, 3: 0.12000054, 4: 0.11999784, 5: 0.053334065, 6: 0.05333402, 7: 0.053334028, 8: 0.2533312, 9: 0.18666616}\n",
      "714 {0: 0.0800005, 1: 0.27999535, 2: 0.08000057, 3: 0.08000048, 4: 0.0800005, 5: 0.080000535, 6: 0.0800005, 7: 0.080000505, 8: 0.08000051, 9: 0.08000051}\n",
      "715 {0: 0.05714344, 1: 0.05714345, 2: 0.1285727, 3: 0.057143427, 4: 0.2714277, 5: 0.12856984, 6: 0.05714345, 7: 0.057143454, 8: 0.12856908, 9: 0.05714347}\n",
      "716 {0: 0.08888925, 1: 0.19999665, 2: 0.08888931, 3: 0.08888924, 4: 0.08888925, 5: 0.08888928, 6: 0.08888925, 7: 0.088889256, 8: 0.088889256, 9: 0.08888926}\n",
      "717 {0: 0.080000594, 1: 0.080000594, 2: 0.08000068, 3: 0.08000057, 4: 0.080000594, 5: 0.08000064, 6: 0.080000594, 7: 0.17999788, 8: 0.1799972, 9: 0.08000062}\n",
      "718 {0: 0.08181646, 1: 0.036364373, 2: 0.03636448, 3: 0.17272641, 4: 0.30909583, 5: 0.08181095, 6: 0.03636437, 7: 0.03636438, 8: 0.17272836, 9: 0.036364395}\n",
      "719 {0: 0.12666814, 1: 0.15999986, 2: 0.059997838, 3: 0.12666713, 4: 0.026667269, 5: 0.05999873, 6: 0.12666667, 7: 0.02666728, 8: 0.16000037, 9: 0.12666672}\n",
      "720 {0: 0.19999851, 1: 0.0571435, 2: 0.12857285, 3: 0.057143476, 4: 0.05714349, 5: 0.05714354, 6: 0.0571435, 7: 0.1999982, 8: 0.05714351, 9: 0.12856945}\n",
      "721 {0: 0.088889115, 1: 0.08888912, 2: 0.08888915, 3: 0.08888911, 4: 0.088889115, 5: 0.08888914, 6: 0.08888912, 7: 0.08888912, 8: 0.08888912, 9: 0.19999786}\n",
      "722 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "723 {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}\n",
      "724 {0: 0.07272774, 1: 0.07272774, 2: 0.072727814, 3: 0.072727725, 4: 0.16363284, 5: 0.25454512, 6: 0.07272774, 7: 0.07272775, 8: 0.07272775, 9: 0.07272776}\n",
      "725 {0: 0.07272787, 1: 0.072727874, 2: 0.07272796, 3: 0.16363357, 4: 0.07272787, 5: 0.07272791, 6: 0.072727874, 7: 0.2545433, 8: 0.07272788, 9: 0.07272789}\n",
      "726 {0: 0.080000065, 1: 0.080000065, 2: 0.08000008, 3: 0.080000065, 4: 0.2799994, 5: 0.08000007, 6: 0.080000065, 7: 0.080000065, 8: 0.08000007, 9: 0.08000007}\n",
      "727 {0: 0.1384566, 1: 0.06153919, 2: 0.061539304, 3: 0.13846253, 4: 0.061539188, 5: 0.061539244, 6: 0.06153919, 7: 0.0615392, 8: 0.061539207, 9: 0.29230636}\n",
      "728 {0: 0.12857023, 1: 0.12856895, 2: 0.20000175, 3: 0.12857218, 4: 0.057143506, 5: 0.05714355, 6: 0.12856929, 7: 0.057143517, 8: 0.05714352, 9: 0.057143528}\n",
      "729 {0: 0.08888911, 1: 0.08888911, 2: 0.08888914, 3: 0.0888891, 4: 0.08888911, 5: 0.19999796, 6: 0.08888911, 7: 0.088889115, 8: 0.088889115, 9: 0.08888912}\n",
      "730 {0: 0.099998705, 1: 0.044445064, 2: 0.100000575, 3: 0.32222328, 4: 0.15555364, 5: 0.099998415, 6: 0.044445064, 7: 0.044445068, 8: 0.04444507, 9: 0.044445083}\n",
      "731 {0: 0.030769523, 1: 0.030769529, 2: 0.030769572, 3: 0.18461503, 4: 0.030769525, 5: 0.10769127, 6: 0.30000016, 7: 0.22307628, 8: 0.030769533, 9: 0.030769536}\n",
      "732 {0: 0.026667278, 1: 0.026667286, 2: 0.06000049, 3: 0.15999822, 4: 0.12666734, 5: 0.059998646, 6: 0.1266647, 7: 0.15999968, 8: 0.22666904, 9: 0.026667306}\n",
      "733 {0: 0.08888896, 1: 0.088888966, 2: 0.08888897, 3: 0.08888896, 4: 0.19999929, 5: 0.088888966, 6: 0.08888896, 7: 0.088888966, 8: 0.088888966, 9: 0.088888966}\n",
      "734 {0: 0.08888903, 1: 0.08888904, 2: 0.088889055, 3: 0.08888903, 4: 0.08888903, 5: 0.08888904, 6: 0.08888904, 7: 0.08888904, 8: 0.08888904, 9: 0.1999987}\n",
      "735 {0: 0.06666709, 1: 0.06666709, 2: 0.15000018, 3: 0.06666707, 4: 0.06666709, 5: 0.23333086, 6: 0.14999932, 7: 0.06666709, 8: 0.066667095, 9: 0.0666671}\n"
     ]
    }
   ],
   "source": [
    "topic_proportions, doc_topics_list = test_eta('auto', dictionary, ntopics=10)\n",
    "\n",
    "for doc_topics in doc_topics_list:\n",
    "    print(doc_topics['document'], doc_topics['probabilities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "fb84338c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     document   topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
      "0           0  0.080000  0.080000  0.080001  0.080000  0.080000  0.080001   \n",
      "1           1  0.057144  0.128571  0.128573  0.199997  0.057144  0.057144   \n",
      "2           2  0.053334  0.053334  0.053334  0.120001  0.053334  0.053334   \n",
      "3           3  0.088889  0.088889  0.199997  0.088889  0.088889  0.088889   \n",
      "4           4  0.074997  0.200000  0.075000  0.158338  0.033334  0.158334   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "731       731  0.030770  0.030770  0.030770  0.184615  0.030770  0.107691   \n",
      "732       732  0.026667  0.026667  0.060000  0.159998  0.126667  0.059999   \n",
      "733       733  0.088889  0.088889  0.088889  0.088889  0.199999  0.088889   \n",
      "734       734  0.088889  0.088889  0.088889  0.088889  0.088889  0.088889   \n",
      "735       735  0.066667  0.066667  0.150000  0.066667  0.066667  0.233331   \n",
      "\n",
      "      topic_7   topic_8   topic_9  topic_10  \n",
      "0    0.080000  0.080000  0.279996  0.080001  \n",
      "1    0.057144  0.057144  0.199997  0.057144  \n",
      "2    0.053334  0.253331  0.253331  0.053334  \n",
      "3    0.088889  0.088889  0.088889  0.088889  \n",
      "4    0.033334  0.158332  0.074997  0.033334  \n",
      "..        ...       ...       ...       ...  \n",
      "731  0.300000  0.223076  0.030770  0.030770  \n",
      "732  0.126665  0.160000  0.226669  0.026667  \n",
      "733  0.088889  0.088889  0.088889  0.088889  \n",
      "734  0.088889  0.088889  0.088889  0.199999  \n",
      "735  0.149999  0.066667  0.066667  0.066667  \n",
      "\n",
      "[736 rows x 11 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 736 entries, 0 to 735\n",
      "Data columns (total 11 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   document  736 non-null    int64  \n",
      " 1   topic_1   736 non-null    float32\n",
      " 2   topic_2   736 non-null    float32\n",
      " 3   topic_3   736 non-null    float32\n",
      " 4   topic_4   736 non-null    float32\n",
      " 5   topic_5   736 non-null    float32\n",
      " 6   topic_6   736 non-null    float32\n",
      " 7   topic_7   736 non-null    float32\n",
      " 8   topic_8   736 non-null    float32\n",
      " 9   topic_9   736 non-null    float32\n",
      " 10  topic_10  736 non-null    float32\n",
      "dtypes: float32(10), int64(1)\n",
      "memory usage: 34.6 KB\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for i, doc_topics in enumerate(doc_topics_list):\n",
    "    row = {'document': i}\n",
    "    for topic, prob in doc_topics['probabilities'].items():\n",
    "        row[f'topic_{topic+1}'] = prob\n",
    "    data.append(row)\n",
    "\n",
    "df_reg = pd.DataFrame(data)\n",
    "print(df_reg)\n",
    "df_reg.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "fe52f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg.to_csv('df_reg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "fde387f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0: How about his signing into law of \"right-to-try\" legislation, allowing gravely ill patients access to experimental drugs?\n",
      "Topic proportions: [0.08000048249959946, 0.08000048995018005, 0.08000056445598602, 0.08000047504901886, 0.08000048249959946, 0.08000051975250244, 0.08000048995018005, 0.08000049740076065, 0.27999550104141235, 0.08000050485134125]\n",
      "Document 1: I haven't seen this posted yet - but by far his most important accomplishment has been keeping America out of any new foreign conflicts. [According to this article](https://www.theelders.org/news/only-us-president-who-didnt-wage-war), the only other president who managed to keep the US out of new foreign conflicts was Jimmy Carter. Thus if this holds through the rest of his term, Trump will be only the second to manage that.\n",
      "\n",
      "For all the talk about Trump being an idiot, and dangerous, and in over his head, I'd submit that keeping the US out of other people's wars is not an easy thing to do. I think he deserves an enormous amount of respect for having pulled this off.\n",
      "Topic proportions: [0.057143595069646835, 0.12857137620449066, 0.1285729706287384, 0.1999967247247696, 0.057143595069646835, 0.05714365094900131, 0.057143598794937134, 0.05714361369609833, 0.19999726116657257, 0.05714362859725952]\n",
      "Document 2: Doubling the standard deduction on taxes is absolutely a net positive thing for most people in the country. Previously people would rarely get above the 12k line unless they used their mortgage interest deduction, so itemizing unfairly balanced towards home owners (of which I am one and previously benefited). But this hurt less wealthy people who already were behind because they weren’t earning home equity. \n",
      "\n",
      "Letting more people have a doubled standard deduction was an absolute step in the right direction and made our tax code partially less regressive.\n",
      "Topic proportions: [0.05333389341831207, 0.05333390086889267, 0.053333986550569534, 0.12000056356191635, 0.05333389714360237, 0.053333938121795654, 0.05333390086889267, 0.25333061814308167, 0.25333136320114136, 0.05333392322063446]\n",
      "Document 3: [deleted]\n",
      "Topic proportions: [0.08888916671276093, 0.08888916671276093, 0.1999974548816681, 0.08888915926218033, 0.08888916671276093, 0.08888918906450272, 0.08888916671276093, 0.08888917416334152, 0.08888917416334152, 0.08888918161392212]\n",
      "Document 4: I can't believe I'm doing this, but they say you should argue in favour of things you disagree with sometimes..... That said, this isn't easy.\n",
      "\n",
      "1- He's donated his entire presidential salary to a variety of causes every year since his inauguration- VAs, education services and plenty more.\n",
      "\n",
      "2- He convinced the Mexican government to modernise its labour laws as part of a trade treaty. Mexicans can now unionise properly! Could be argued that this doesn't affect the US, but I think that a happier and more well-protected workforce in a trade partner country will have benefits.\n",
      "\n",
      "3- He started positive reforms to the prison system with the First Step act.\n",
      "\n",
      "4- he killed (not personally) Abu Bakr al-Baghdadi.\n",
      "\n",
      "Honourable mention: SPACE FORCE. Worth it for the name alone, and he gave the rest of the world a good laugh.\n",
      "\n",
      "Of course, none of this makes up for completely trashing the (ahem) formerly positive international reputation of the US, but there you go.\n",
      "\n",
      "**Edit:** added an 'ahem'.\n",
      "\n",
      "**Second edit:** If you're going to add to the five hundred comments regarding #1, I know. I really do. Read them all first.\n",
      "Topic proportions: [0.07499685138463974, 0.2000000774860382, 0.07499969750642776, 0.1583375185728073, 0.033334359526634216, 0.15833449363708496, 0.03333436697721481, 0.15833160281181335, 0.07499665021896362, 0.0333344042301178]\n",
      "Document 5: He lowered unemployment at record lows by the year of 2019. A very high percentage of American's say they are better off financially since he has been president, according to official studies. There is factually more job openings than there are unemployed. And wages have gone up.\n",
      "\n",
      "African-American unemployment has recently achieved the lowest rate ever recorded. Hispanic-American unemployment is at the lowest rate ever recorded. Asian-American unemployment recently achieved the lowest rate ever recorded. Women’s unemployment recently reached the lowest rate in 65 years. Youth unemployment has recently hit the lowest rate in nearly half a century. Lowest unemployment rate ever recorded for Americans without a high school diploma. Under the Administration, veterans’ unemployment recently reached its lowest rate in nearly 20 years. He has got NATO allies to cough up more money for our collective security. Allies have increased defense spending by $130 billion since 2016. And the White House reports almost twice as many allies are meeting their commitment to spend 2% of gross domestic product on defense today than before Trump arrived.\n",
      "\n",
      "He stood with the people of Hong Kong. He warned China not to use violence to suppress pro-democracy protests and signed the Hong Kong Human Rights and Democracy Act. Hong Kong people marched with American flags and sang our national anthem in gratitude.\n",
      "\n",
      "His tariff threats forced Mexico to crack down on illegal immigration. Mexico is for the first time in recent history enforcing its own immigration laws — sending thousands of National Guard forces to its southern border to stop caravans of Central American migrants. Plus, Congress is poised to approve the U.S.-Mexico-Canada free-trade agreement, which would not have been possible without the threat of tariffs.\n",
      "\n",
      "Economic growth last quarter hit 4.2 percent.\n",
      "\n",
      "Median household income has hit highest level ever recorded.\n",
      "\n",
      "He worked toward and enforced the FDA to approve more affordable generic drugs than ever before in history. And because of that, many drug companies are freezing or reversing planned price increases.\n",
      "\n",
      "He reformed the Medicare program to stop hospitals from overcharging low-income seniors on their drugs—saving seniors hundreds of millions of dollars this year alone.\n",
      "\n",
      "Trump signed a law ending the gag orders on Pharmacists that prevented them from sharing money-saving information.\n",
      "\n",
      "Secured $6 billion in NEW funding to fight the opioid epidemic.\n",
      "\n",
      "They have reduced high-dose opioid prescriptions by 16 percent during my first year in office.\n",
      "\n",
      "He signed a bill this year allowing some drug imports from Canada so that prescription prices would go down.\n",
      "\n",
      "Trump signed an executive order this year that forces all healthcare providers to disclose the cost of their services so that Americans can comparison shop and know how much less providers charge insurance companies. When signing that bill he said no American should be blindsided by bills for medical services they never agreed to in advance. Hospitals will now be required to post their standard charges for services, which include the discounted price a hospital is willing to accept\n",
      "\n",
      "Signed VA Choice Act and VA Accountability Act, expanded VA telehealth services, walk-in-clinics, and same-day urgent primary and mental health care.\n",
      "\n",
      "Trump recently signed 3 bills to benefit Native people. One gives compensation to the Spokane tribe for loss of their lands in the mid-1900's, one funds Native language programs, and the third gives federal recognition to the Little Shell Tribe of Chippewa Indians in Montana.\n",
      "\n",
      "He signed a law to make cruelty to animals a federal felony so that animal abusers face tougher consequences.\n",
      "\n",
      "Violent crime has fallen every year he’s been in office after rising during the 2 years before he was elected.\n",
      "\n",
      "He signed a bill making CBD and Hemp legal.\n",
      "\n",
      "Trump’s EPA gave $100 million to fix the water infrastructure problem in Flint, Michigan.\n",
      "\n",
      "Under Trump’s leadership, in 2018 the U.S. surpassed Russia and Saudi Arabia to become the world’s largest producer of crude oil.\n",
      "\n",
      "He signed the “Allow States and Victims to Fight Online Sex Trafficking Act” (FOSTA), which includes the “Stop Enabling Sex Traffickers Act” (SESTA) which both give law enforcement and victims new tools to fight sex trafficking.\n",
      "\n",
      "He also signed a bill to require airports to provide spaces for breastfeeding Moms.\n",
      "\n",
      "Trump signed the biggest wilderness protection & conservation bill in a decade and designated 375,000 acres as protected land. Even though everyone freaked out when he dropped us from programs, it wasn't about the programs doing good, it was about how the programs misused their funding. Well there you go. Now we have better implementations. But the news won't tell you that.\n",
      "\n",
      "Trump signed the Save our Seas Act which funds $10 million per year to clean tons of plastic & garbage from the ocean.\n",
      "\n",
      "The First Step Act’s reforms addressed inequities in sentencing laws that disproportionately harmed Black Americans and reformed mandatory minimums that created unfair outcomes.\n",
      "\n",
      "The First Step Act expanded judicial discretion in sentencing of non-violent crimes.\n",
      "\n",
      "Over 90% of those benefiting from the retroactive sentencing reductions in the First Step Act are Black Americans.\n",
      "\n",
      "The First Step Act provides rehabilitative programs to inmates, helping them successfully rejoin society and not return to crime.\n",
      "\n",
      "Trump increased funding for Historically Black Colleges and Universities (HBCU's) by more than 14%.\n",
      "\n",
      "Trump signed legislation forgiving Hurricane Katrina debt that threatened HBCU's.\n",
      "\n",
      "He signed funding legislation in September 2018 that increased funding for school choice by $42 million.\n",
      "\n",
      "The tax cuts signed into law by Trump promote school choice by allowing families to use 529 college savings plans for elementary and secondary education.\n",
      "\n",
      "Signed legislation to improve the National Suicide Hotline.\n",
      "\n",
      "Signed the most comprehensive childhood cancer legislation ever into law, which will advance childhood cancer research and improve treatments.\n",
      "\n",
      "The Tax Cuts and Jobs Act signed into law by Trump doubled the maximum amount of the child tax credit available to parents and lifted the income limits so more people could claim it.\n",
      "\n",
      "In 2018, Trump signed into law a $2.4 billion funding increase for the Child Care and Development Fund, providing a total of $8.1 billion to States to fund child care for low-income families.\n",
      "\n",
      "The Child and Dependent Care Tax Credit (CDCTC) signed into law by Trump provides a tax credit equal to 20-35% of child care expenses, $3,000 per child & $6,000 per family + Flexible Spending Accounts (FSA's) allow you to set aside up to $5,000 in pre-tax $ to use for child care.\n",
      "\n",
      "In 2019 he signed the Autism Collaboration, Accountability, Research, Education and Support Act (CARES) into law which allocates $1.8 billion in funding over the next five years to help people with autism spectrum disorder and to help their families.\n",
      "\n",
      "In 2019 he signed into law two funding packages providing nearly $19 million in new funding for Lupus specific research and education programs, as well an additional $41.7 billion in funding for the National Institutes of Health (NIH), the most Lupus funding EVER.\n",
      "\n",
      "(I appreciate all the opinions and fact checking guys, it’s cool to see.)\n",
      "Topic proportions: [0.11428486555814743, 0.01904851384460926, 0.019048647955060005, 0.06666252762079239, 0.06666504591703415, 0.04285642132163048, 0.18571873009204865, 0.37619221210479736, 0.0904744565486908, 0.019048545509576797]\n",
      "Document 6: He fully funded the Land and Water Conservation Fund in perpetuity, as well as funded nearly all of the necessary backlog repair work for the national parks. Look up the Great American Outdoors Act. I have more, but I believe this disproves your claim.\n",
      "\n",
      "Edit: Some people seem to have misunderstood what this CMV is about. I am NOT arguing that Trump's environmental record is good, I am simply pointing out one positive thing that he has done as president.\n",
      "Topic proportions: [0.12857010960578918, 0.1285698413848877, 0.05714486539363861, 0.12856841087341309, 0.12857137620449066, 0.05714472383260727, 0.12857380509376526, 0.12856754660606384, 0.057144638150930405, 0.05714466795325279]\n",
      "Document 7: He did make a difference when it comes to defence spending in Europe, which is now slowly climbing upwards for the first time since 1991. Now, whether that is beneficial or not is subjective, but I do think it qualifies as him successfully achieving a political goal. Regards\n",
      "Topic proportions: [0.08888903260231018, 0.08888904005289078, 0.08888905495405197, 0.08888903260231018, 0.08888903260231018, 0.08888904005289078, 0.08888904005289078, 0.08888904005289078, 0.08888904005289078, 0.19999870657920837]\n",
      "Document 8: [deleted]\n",
      "Topic proportions: [0.08888916671276093, 0.08888916671276093, 0.1999974548816681, 0.08888915926218033, 0.08888916671276093, 0.08888918906450272, 0.08888916671276093, 0.08888917416334152, 0.08888917416334152, 0.08888918161392212]\n",
      "Document 9: The one huge lasting positive impact on the USA that Donald Trump has had that I see is that his presidency has given truth to the thing parents tell their kids, namely that they can do anything and be anything they want- even president of the United States.\n",
      "\n",
      "I frankly usually say this as a joke, but you know, there really is some serious truth to it.\n",
      "\n",
      "We need new thoughts in America. We need new perspectives and we need young people to get involved. That doesn't happen if they don't believe they can make a difference, and being president certainly allows them to make a big difference.\n",
      "\n",
      "Now, after Trump, when they look around and see someone who doesn't fit the traditional political mold, who (IMO at least) isn't qualified for the job, it lowers the bar. That's a bad thing generally, but maybe in this case it makes some kid say \"hey, maybe I DO have a chance, I don't necessarily need to be a Harvard grad or a war hero or something really tough like that to be president\", and maybe that kid turns out to be a fantastic president.\n",
      "\n",
      "Giving up before a fight even begins because you think you have no chance is a horrible thing. Trump shows people that truly ANYONE could be president, so they'll be more willing to start that journey. And, even if they ultimately fail, they may do some good in the process anyway.\n",
      "\n",
      "Trump lowering the bar means that I'm no longer lying to my kids when I say they really can be president if they want, and that's a lasting positive.\n",
      "Topic proportions: [0.04000096395611763, 0.08999581634998322, 0.09000219404697418, 0.1899975836277008, 0.04000096395611763, 0.1399986445903778, 0.04000096768140793, 0.09000124782323837, 0.090001180768013, 0.19000038504600525]\n"
     ]
    }
   ],
   "source": [
    "# Visualize topic to see if assignation is consistent\n",
    "for i in range(10):\n",
    "    print(f\"Document {i}: {df.body[i]}\")\n",
    "    topic_props = df_reg.iloc[i, 1:].tolist()\n",
    "    print(f\"Topic proportions: {topic_props}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3030e4",
   "metadata": {},
   "source": [
    "NGRAMES: tax_cut, positive_change, human_trafficking,court_system, middle_east"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e7de36",
   "metadata": {},
   "source": [
    "##### Try LDA using priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "183f365d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -4.28\n",
      "Topic 0: ['obama', 'war', 'military', 'federal', 'iran', 'state', 'killing', 'democrat']\n",
      "Topic 1: ['right', 'government', 'policy', 'peace', 'vote', 'action', 'anti', 'nation']\n",
      "Topic 2: ['job', 'tax', 'economy', 'politics', 'presidency', 'business', 'pay', 'employment']\n",
      "Topic 3: ['country', 'china', 'impact', 'US', 'change', 'republican', 'power', 'political']\n",
      "Topic 4: ['american', 'administration', 'accomplishment', 'work', 'signed', 'racist', 'korea', 'bill']\n",
      "0: [(0, 0.13339068), (1, 0.13340044), (2, 0.29977468), (3, 0.13337661), (4, 0.30005756)]\n",
      "1: [(0, 0.1801595), (1, 0.080051996), (2, 0.17985356), (3, 0.2798353), (4, 0.28009963)]\n",
      "2: [(0, 0.072747946), (1, 0.082578264), (2, 0.6082912), (3, 0.16363402), (4, 0.07274856)]\n",
      "3: [(0, 0.16002986), (1, 0.16003324), (2, 0.35988414), (3, 0.16002281), (4, 0.16002992)]\n",
      "4: [(0, 0.24000439), (1, 0.24003349), (2, 0.09836267), (3, 0.19017732), (4, 0.23142211)]\n",
      "5: [(0, 0.11950696), (1, 0.0734529), (2, 0.38002276), (3, 0.11582601), (4, 0.31119138)]\n",
      "6: [(0, 0.28000858), (1, 0.17994799), (2, 0.17977923), (3, 0.08005413), (4, 0.28021008)]\n",
      "7: [(0, 0.16001572), (1, 0.16001886), (2, 0.16001788), (3, 0.35993007), (4, 0.16001745)]\n",
      "8: [(0, 0.1600299), (1, 0.16003327), (2, 0.35988408), (3, 0.16002283), (4, 0.16002996)]\n",
      "9: [(0, 0.2532661), (1, 0.1122951), (2, 0.14083813), (3, 0.44356298), (4, 0.05003767)]\n",
      "10: [(0, 0.13335124), (1, 0.13335516), (2, 0.4665942), (3, 0.13334827), (4, 0.13335119)]\n",
      "11: [(0, 0.07792799), (1, 0.03481608), (2, 0.16070563), (3, 0.5317675), (4, 0.1947828)]\n",
      "12: [(0, 0.36501274), (1, 0.052634653), (2, 0.19405536), (3, 0.1589939), (4, 0.22930337)]\n",
      "13: [(0, 0.13336916), (1, 0.13337411), (2, 0.13337456), (3, 0.29993924), (4, 0.29994288)]\n",
      "14: [(0, 0.061560836), (1, 0.21547021), (2, 0.17588438), (3, 0.4855227), (4, 0.06156191)]\n",
      "15: [(0, 0.42174602), (1, 0.07552281), (2, 0.24333194), (3, 0.075663), (4, 0.18373623)]\n",
      "16: [(0, 0.100044265), (1, 0.35003456), (2, 0.22495672), (3, 0.122627795), (4, 0.20233667)]\n",
      "17: [(0, 0.22825544), (1, 0.13328366), (2, 0.46347597), (3, 0.13307098), (4, 0.04191398)]\n",
      "18: [(0, 0.44628072), (1, 0.12590326), (2, 0.09710769), (3, 0.16523945), (4, 0.16546892)]\n",
      "19: [(0, 0.31928623), (1, 0.40301478), (2, 0.05276239), (3, 0.11251747), (4, 0.11241912)]\n",
      "20: [(0, 0.23736425), (1, 0.048047297), (2, 0.0875369), (3, 0.32954046), (4, 0.29751107)]\n",
      "21: [(0, 0.28499907), (1, 0.028602352), (2, 0.13549274), (3, 0.028591894), (4, 0.5223139)]\n",
      "22: [(0, 0.3999445), (1, 0.11431488), (2, 0.11431453), (3, 0.11430537), (4, 0.2571207)]\n",
      "23: [(0, 0.49698514), (1, 0.16452515), (2, 0.04709147), (3, 0.18581453), (4, 0.10558367)]\n",
      "24: [(0, 0.12992322), (1, 0.42708388), (2, 0.048350833), (3, 0.34607735), (4, 0.04856469)]\n",
      "25: [(0, 0.33767173), (1, 0.118624605), (2, 0.056079485), (3, 0.11864937), (4, 0.36897478)]\n",
      "26: [(0, 0.09465078), (1, 0.41054744), (2, 0.09486656), (3, 0.14727864), (4, 0.2526566)]\n",
      "27: [(0, 0.20720929), (1, 0.03698036), (2, 0.097852945), (3, 0.57509303), (4, 0.08286433)]\n",
      "28: [(0, 0.35999906), (1, 0.16000023), (2, 0.16000023), (3, 0.16000023), (4, 0.16000023)]\n",
      "29: [(0, 0.34552816), (1, 0.16337189), (2, 0.2546468), (3, 0.16368072), (4, 0.07277241)]\n",
      "30: [(0, 0.12592264), (1, 0.40828243), (2, 0.039869715), (3, 0.21272694), (4, 0.21319829)]\n",
      "31: [(0, 0.2665346), (1, 0.117264085), (2, 0.31939903), (3, 0.2422662), (4, 0.054536115)]\n",
      "32: [(0, 0.38037893), (1, 0.2582713), (2, 0.13549265), (3, 0.08005155), (4, 0.14580555)]\n",
      "33: [(0, 0.100039735), (1, 0.34965664), (2, 0.35023323), (3, 0.1000307), (4, 0.100039735)]\n",
      "34: [(0, 0.39973137), (1, 0.11435009), (2, 0.114350826), (3, 0.11432914), (4, 0.25723857)]\n",
      "35: [(0, 0.16005519), (1, 0.35980672), (2, 0.16005467), (3, 0.16003472), (4, 0.16004874)]\n",
      "36: [(0, 0.35220066), (1, 0.16000563), (2, 0.16778462), (3, 0.160004), (4, 0.16000511)]\n",
      "37: [(0, 0.1600063), (1, 0.16000731), (2, 0.16000724), (3, 0.35997254), (4, 0.1600066)]\n",
      "38: [(0, 0.053366944), (1, 0.11977814), (2, 0.51387596), (3, 0.19315292), (4, 0.119826026)]\n",
      "39: [(0, 0.13617267), (1, 0.11160093), (2, 0.20178114), (3, 0.084880285), (4, 0.46556497)]\n",
      "40: [(0, 0.1600299), (1, 0.16003326), (2, 0.35988408), (3, 0.16002283), (4, 0.16002993)]\n",
      "41: [(0, 0.27273458), (1, 0.15808639), (2, 0.16574432), (3, 0.2750254), (4, 0.12840933)]\n",
      "42: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "43: [(0, 0.05213704), (1, 0.55878603), (2, 0.059634395), (3, 0.10586462), (4, 0.22357789)]\n",
      "44: [(0, 0.35218266), (1, 0.25146133), (2, 0.22990455), (3, 0.10847566), (4, 0.05797584)]\n",
      "45: [(0, 0.10274692), (1, 0.22346354), (2, 0.059769906), (3, 0.5669377), (4, 0.047081973)]\n",
      "46: [(0, 0.1600006), (1, 0.1600006), (2, 0.3599975), (3, 0.1600006), (4, 0.1600006)]\n",
      "47: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "48: [(0, 0.13336763), (1, 0.29984665), (2, 0.30005765), (3, 0.13336009), (4, 0.13336802)]\n",
      "49: [(0, 0.29987252), (1, 0.13338189), (2, 0.13338086), (3, 0.29998758), (4, 0.1333771)]\n",
      "50: [(0, 0.35999906), (1, 0.16000025), (2, 0.16000025), (3, 0.16000025), (4, 0.16000025)]\n",
      "51: [(0, 0.29983994), (1, 0.13338126), (2, 0.13338035), (3, 0.3000236), (4, 0.13337487)]\n",
      "52: [(0, 0.22491798), (1, 0.3499456), (2, 0.10010361), (3, 0.10006995), (4, 0.22496288)]\n",
      "53: [(0, 0.22499242), (1, 0.22504847), (2, 0.100041956), (3, 0.11103659), (4, 0.33888057)]\n",
      "54: [(0, 0.16002652), (1, 0.1600315), (2, 0.16003127), (3, 0.16002092), (4, 0.35988984)]\n",
      "55: [(0, 0.07345717), (1, 0.254044), (2, 0.07277774), (3, 0.2542851), (4, 0.345436)]\n",
      "56: [(0, 0.2999439), (1, 0.13341735), (2, 0.167439), (3, 0.2657917), (4, 0.13340804)]\n",
      "57: [(0, 0.35991952), (1, 0.16002268), (2, 0.16002245), (3, 0.16001546), (4, 0.16001992)]\n",
      "58: [(0, 0.1600233), (1, 0.16002595), (2, 0.35990953), (3, 0.16001739), (4, 0.16002384)]\n",
      "59: [(0, 0.29990003), (1, 0.13336568), (2, 0.1333661), (3, 0.30000606), (4, 0.13336216)]\n",
      "60: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "61: [(0, 0.1333729), (1, 0.29988846), (2, 0.29999918), (3, 0.1333646), (4, 0.13337488)]\n",
      "62: [(0, 0.13336882), (1, 0.29995662), (2, 0.29994425), (3, 0.13336103), (4, 0.13336924)]\n",
      "63: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "64: [(0, 0.46658966), (1, 0.13335504), (2, 0.13335481), (3, 0.13334812), (4, 0.13335238)]\n",
      "65: [(0, 0.13345557), (1, 0.13342783), (2, 0.13345934), (3, 0.2994789), (4, 0.30017838)]\n",
      "66: [(0, 0.11150836), (1, 0.4635366), (2, 0.100050904), (3, 0.10003418), (4, 0.22486997)]\n",
      "67: [(0, 0.1600244), (1, 0.35990205), (2, 0.1600301), (3, 0.1600191), (4, 0.16002436)]\n",
      "68: [(0, 0.29992118), (1, 0.13335843), (2, 0.13335863), (3, 0.30000606), (4, 0.1333557)]\n",
      "69: [(0, 0.1333639), (1, 0.13336828), (2, 0.29987198), (3, 0.30003187), (4, 0.13336396)]\n",
      "70: [(0, 0.352177), (1, 0.16000564), (2, 0.16780825), (3, 0.160004), (4, 0.16000511)]\n",
      "71: [(0, 0.114322886), (1, 0.2570519), (2, 0.11432666), (3, 0.39997327), (4, 0.11432532)]\n",
      "72: [(0, 0.13334534), (1, 0.2517588), (2, 0.133347), (3, 0.34820333), (4, 0.1333455)]\n",
      "73: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "74: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "75: [(0, 0.16000028), (1, 0.16000028), (2, 0.35999885), (3, 0.16000028), (4, 0.16000028)]\n",
      "76: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "77: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "78: [(0, 0.28002015), (1, 0.1796021), (2, 0.18021005), (3, 0.08004817), (4, 0.2801195)]\n",
      "79: [(0, 0.6193154), (1, 0.091586776), (2, 0.1387754), (3, 0.043735225), (4, 0.10658723)]\n",
      "80: [(0, 0.11433846), (1, 0.25714874), (2, 0.11434485), (3, 0.25724658), (4, 0.25692138)]\n",
      "81: [(0, 0.6415384), (1, 0.07339169), (2, 0.029663574), (3, 0.18941106), (4, 0.06599529)]\n",
      "82: [(0, 0.29995382), (1, 0.13337389), (2, 0.29994172), (3, 0.13336073), (4, 0.13336983)]\n",
      "83: [(0, 0.16002984), (1, 0.16003321), (2, 0.35988426), (3, 0.16002281), (4, 0.1600299)]\n",
      "84: [(0, 0.16007473), (1, 0.16006735), (2, 0.16007161), (3, 0.16004527), (4, 0.3597411)]\n",
      "85: [(0, 0.12304077), (1, 0.1383095), (2, 0.07687215), (3, 0.21529438), (4, 0.4464832)]\n",
      "86: [(0, 0.061544105), (1, 0.46411985), (2, 0.21442838), (3, 0.045299184), (4, 0.21460849)]\n",
      "87: [(0, 0.17220032), (1, 0.038142394), (2, 0.39862213), (3, 0.30921626), (4, 0.081818886)]\n",
      "88: [(0, 0.19851658), (1, 0.48593646), (2, 0.05871623), (3, 0.12821107), (4, 0.12861966)]\n",
      "89: [(0, 0.16002218), (1, 0.35991192), (2, 0.16002528), (3, 0.16001749), (4, 0.16002312)]\n",
      "90: [(0, 0.13334551), (1, 0.133347), (2, 0.29997176), (3, 0.29999033), (4, 0.13334543)]\n",
      "91: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "92: [(0, 0.105619654), (1, 0.16457222), (2, 0.28245106), (3, 0.28260267), (4, 0.16475438)]\n",
      "93: [(0, 0.19989362), (1, 0.19989468), (2, 0.14074147), (3, 0.37053093), (4, 0.08893924)]\n",
      "94: [(0, 0.16002984), (1, 0.1600332), (2, 0.35988423), (3, 0.16002281), (4, 0.1600299)]\n",
      "95: [(0, 0.20861682), (1, 0.07822173), (2, 0.12161245), (3, 0.42628714), (4, 0.16526182)]\n",
      "96: [(0, 0.1497592), (1, 0.15004486), (2, 0.25737053), (3, 0.2437476), (4, 0.19907784)]\n",
      "97: [(0, 0.120500505), (1, 0.3033114), (2, 0.16793725), (3, 0.28747013), (4, 0.12078072)]\n",
      "98: [(0, 0.16002986), (1, 0.16003321), (2, 0.35988417), (3, 0.16002281), (4, 0.1600299)]\n",
      "99: [(0, 0.35219592), (1, 0.16000563), (2, 0.16778935), (3, 0.160004), (4, 0.16000511)]\n",
      "100: [(0, 0.16002986), (1, 0.16003323), (2, 0.35988417), (3, 0.16002281), (4, 0.1600299)]\n",
      "101: [(0, 0.16008197), (1, 0.16010779), (2, 0.35965487), (3, 0.16006969), (4, 0.16008574)]\n",
      "102: [(0, 0.22509329), (1, 0.10003), (2, 0.47482902), (3, 0.10002015), (4, 0.100027524)]\n",
      "103: [(0, 0.35984576), (1, 0.16004103), (2, 0.16004448), (3, 0.16002975), (4, 0.16003898)]\n",
      "104: [(0, 0.114323385), (1, 0.11432901), (2, 0.2570412), (3, 0.3287705), (4, 0.18553591)]\n",
      "105: [(0, 0.22488426), (1, 0.10004615), (2, 0.100046374), (3, 0.43373358), (4, 0.14128967)]\n",
      "106: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "107: [(0, 0.16002336), (1, 0.16002853), (2, 0.16002686), (3, 0.16001783), (4, 0.35990337)]\n",
      "108: [(0, 0.2247946), (1, 0.47506952), (2, 0.100053206), (3, 0.100035444), (4, 0.10004721)]\n",
      "109: [(0, 0.13336577), (1, 0.29996774), (2, 0.29994005), (3, 0.13335907), (4, 0.1333674)]\n",
      "110: [(0, 0.2587464), (1, 0.43209916), (2, 0.072755374), (3, 0.16364624), (4, 0.072752796)]\n",
      "111: [(0, 0.11430286), (1, 0.114305295), (2, 0.39994338), (3, 0.114299245), (4, 0.25714922)]\n",
      "112: [(0, 0.10772333), (1, 0.31086975), (2, 0.31118956), (3, 0.08892718), (4, 0.18129016)]\n",
      "113: [(0, 0.16003492), (1, 0.16003744), (2, 0.35986653), (3, 0.16002597), (4, 0.1600351)]\n",
      "114: [(0, 0.13337411), (1, 0.13338135), (2, 0.29998162), (3, 0.133365), (4, 0.29989788)]\n",
      "115: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "116: [(0, 0.1600096), (1, 0.16001067), (2, 0.1600109), (3, 0.35995921), (4, 0.16000965)]\n",
      "117: [(0, 0.16002986), (1, 0.16003323), (2, 0.3598842), (3, 0.16002281), (4, 0.1600299)]\n",
      "118: [(0, 0.13336457), (1, 0.2850649), (2, 0.2998776), (3, 0.14832759), (4, 0.13336536)]\n",
      "119: [(0, 0.08116251), (1, 0.07274795), (2, 0.116324455), (3, 0.29334134), (4, 0.43642375)]\n",
      "120: [(0, 0.1143143), (1, 0.114319876), (2, 0.25717887), (3, 0.39987147), (4, 0.1143155)]\n",
      "121: [(0, 0.42234623), (1, 0.08892907), (2, 0.1998599), (3, 0.08891538), (4, 0.19994938)]\n",
      "122: [(0, 0.10003133), (1, 0.22500362), (2, 0.10003656), (3, 0.4748971), (4, 0.1000314)]\n",
      "123: [(0, 0.08892535), (1, 0.19982053), (2, 0.08893023), (3, 0.53339636), (4, 0.08892755)]\n",
      "124: [(0, 0.07274186), (1, 0.07274392), (2, 0.09063403), (3, 0.6790017), (4, 0.08487851)]\n",
      "125: [(0, 0.2635611), (1, 0.08154911), (2, 0.08164943), (3, 0.17275354), (4, 0.40048683)]\n",
      "126: [(0, 0.114298366), (1, 0.11430005), (2, 0.11430052), (3, 0.114297874), (4, 0.54280317)]\n",
      "127: [(0, 0.13343859), (1, 0.13344982), (2, 0.13345167), (3, 0.1334153), (4, 0.46624464)]\n",
      "128: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "129: [(0, 0.11433305), (1, 0.25721142), (2, 0.11434066), (3, 0.2570111), (4, 0.25710383)]\n",
      "130: [(0, 0.14991976), (1, 0.06670907), (2, 0.31672454), (3, 0.15004197), (4, 0.3166046)]\n",
      "131: [(0, 0.15176894), (1, 0.4817061), (2, 0.14990374), (3, 0.1499167), (4, 0.06670447)]\n",
      "132: [(0, 0.39954513), (1, 0.25734463), (2, 0.11438416), (3, 0.11435027), (4, 0.114375815)]\n",
      "133: [(0, 0.2998116), (1, 0.13337603), (2, 0.2807264), (3, 0.15271243), (4, 0.1333735)]\n",
      "134: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "135: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "136: [(0, 0.08002142), (1, 0.080024), (2, 0.38006204), (3, 0.08001648), (4, 0.37987605)]\n",
      "137: [(0, 0.16003239), (1, 0.16003552), (2, 0.359875), (3, 0.16002527), (4, 0.16003183)]\n",
      "138: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "139: [(0, 0.13336895), (1, 0.13337237), (2, 0.2998467), (3, 0.30004346), (4, 0.13336846)]\n",
      "140: [(0, 0.2997399), (1, 0.1333951), (2, 0.1333933), (3, 0.13337426), (4, 0.3000974)]\n",
      "141: [(0, 0.07279017), (1, 0.16367559), (2, 0.16306281), (3, 0.5276791), (4, 0.07279234)]\n",
      "142: [(0, 0.2247305), (1, 0.100056894), (2, 0.2249973), (3, 0.35016578), (4, 0.100049525)]\n",
      "143: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "144: [(0, 0.30001533), (1, 0.13333869), (2, 0.13333894), (3, 0.13333713), (4, 0.2999699)]\n",
      "145: [(0, 0.11432418), (1, 0.3998344), (2, 0.25720358), (3, 0.11431462), (4, 0.11432316)]\n",
      "146: [(0, 0.16000682), (1, 0.16000745), (2, 0.16000788), (3, 0.3599709), (4, 0.16000691)]\n",
      "147: [(0, 0.029640969), (1, 0.03142697), (2, 0.8379549), (3, 0.030940682), (4, 0.070036486)]\n",
      "148: [(0, 0.16003494), (1, 0.16003744), (2, 0.35986656), (3, 0.16002597), (4, 0.16003507)]\n",
      "149: [(0, 0.3598519), (1, 0.16004032), (2, 0.16004038), (3, 0.16002825), (4, 0.16003914)]\n",
      "150: [(0, 0.10005125), (1, 0.22499813), (2, 0.10006002), (3, 0.22495179), (4, 0.3499388)]\n",
      "151: [(0, 0.25449184), (1, 0.2544743), (2, 0.2545753), (3, 0.07276835), (4, 0.16369022)]\n",
      "152: [(0, 0.31101248), (1, 0.4093469), (2, 0.08890953), (3, 0.1018246), (4, 0.0889065)]\n",
      "153: [(0, 0.11431961), (1, 0.11432775), (2, 0.54271924), (3, 0.11431269), (4, 0.11432067)]\n",
      "154: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "155: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "156: [(0, 0.19977961), (1, 0.08897134), (2, 0.0889703), (3, 0.42223728), (4, 0.20004143)]\n",
      "157: [(0, 0.0727604), (1, 0.16349751), (2, 0.07276539), (3, 0.6182143), (4, 0.072762325)]\n",
      "158: [(0, 0.1600057), (1, 0.1600062), (2, 0.16000651), (3, 0.1600043), (4, 0.35997733)]\n",
      "159: [(0, 0.08077654), (1, 0.092293546), (2, 0.12693249), (3, 0.44648805), (4, 0.2535094)]\n",
      "160: [(0, 0.14024426), (1, 0.35563517), (2, 0.057775937), (3, 0.24316874), (4, 0.2031759)]\n",
      "161: [(0, 0.13335659), (1, 0.29989725), (2, 0.13335983), (3, 0.1333515), (4, 0.30003485)]\n",
      "162: [(0, 0.16343434), (1, 0.072756976), (2, 0.072759), (3, 0.6182954), (4, 0.07275431)]\n",
      "163: [(0, 0.1600206), (1, 0.16002522), (2, 0.35991615), (3, 0.16001646), (4, 0.16002157)]\n",
      "164: [(0, 0.08892213), (1, 0.19994874), (2, 0.27813148), (3, 0.34407547), (4, 0.08892213)]\n",
      "165: [(0, 0.11431713), (1, 0.25721142), (2, 0.2569917), (3, 0.25716242), (4, 0.114317335)]\n",
      "166: [(0, 0.114307664), (1, 0.25714377), (2, 0.11430999), (3, 0.25714663), (4, 0.25709197)]\n",
      "167: [(0, 0.116454914), (1, 0.25638494), (2, 0.11459145), (3, 0.11449162), (4, 0.39807707)]\n",
      "168: [(0, 0.1600279), (1, 0.1600309), (2, 0.16003905), (3, 0.1600212), (4, 0.35988095)]\n",
      "169: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "170: [(0, 0.08891554), (1, 0.3111279), (2, 0.088919245), (3, 0.088910185), (4, 0.42212716)]\n",
      "171: [(0, 0.47478935), (1, 0.100048065), (2, 0.21065556), (3, 0.114461996), (4, 0.100045055)]\n",
      "172: [(0, 0.13335638), (1, 0.30003637), (2, 0.2998994), (3, 0.13335098), (4, 0.13335691)]\n",
      "173: [(0, 0.10003394), (1, 0.22493646), (2, 0.10004208), (3, 0.10002666), (4, 0.47496086)]\n",
      "174: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "175: [(0, 0.16000913), (1, 0.16001031), (2, 0.3368709), (3, 0.18310036), (4, 0.1600093)]\n",
      "176: [(0, 0.11429833), (1, 0.11429925), (2, 0.11430008), (3, 0.5428045), (4, 0.11429786)]\n",
      "177: [(0, 0.057163298), (1, 0.057165746), (2, 0.3427896), (3, 0.48571783), (4, 0.057163518)]\n",
      "178: [(0, 0.13334946), (1, 0.13334824), (2, 0.13334976), (3, 0.13334422), (4, 0.46660835)]\n",
      "179: [(0, 0.13343237), (1, 0.29998), (2, 0.13344914), (3, 0.13340929), (4, 0.29972914)]\n",
      "180: [(0, 0.3499672), (1, 0.2248956), (2, 0.100053735), (3, 0.22503677), (4, 0.100046694)]\n",
      "181: [(0, 0.10004218), (1, 0.22495122), (2, 0.100048386), (3, 0.47491586), (4, 0.100042336)]\n",
      "182: [(0, 0.13333854), (1, 0.13333894), (2, 0.13333921), (3, 0.30000997), (4, 0.29997334)]\n",
      "183: [(0, 0.22643831), (1, 0.20000476), (2, 0.042144053), (3, 0.38404837), (4, 0.14736453)]\n",
      "184: [(0, 0.3196736), (1, 0.19135007), (2, 0.08893816), (3, 0.0889214), (4, 0.3111168)]\n",
      "185: [(0, 0.1600434), (1, 0.35982653), (2, 0.1600492), (3, 0.16003433), (4, 0.16004656)]\n",
      "186: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "187: [(0, 0.16002442), (1, 0.3599019), (2, 0.16003014), (3, 0.16001911), (4, 0.16002437)]\n",
      "188: [(0, 0.23801392), (1, 0.44006106), (2, 0.09201822), (3, 0.04001544), (4, 0.18989134)]\n",
      "189: [(0, 0.13341981), (1, 0.30000106), (2, 0.2997571), (3, 0.13339974), (4, 0.13342234)]\n",
      "190: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "191: [(0, 0.47505012), (1, 0.100006886), (2, 0.10000704), (3, 0.22492968), (4, 0.10000626)]\n",
      "192: [(0, 0.11430692), (1, 0.3999361), (2, 0.11431013), (3, 0.25713912), (4, 0.11430771)]\n",
      "193: [(0, 0.533305), (1, 0.08894242), (2, 0.19988944), (3, 0.08892564), (4, 0.0889375)]\n",
      "194: [(0, 0.4665996), (1, 0.1333517), (2, 0.13335264), (3, 0.13334616), (4, 0.13334991)]\n",
      "195: [(0, 0.072749734), (1, 0.2545782), (2, 0.07275366), (3, 0.16362202), (4, 0.4362964)]\n",
      "196: [(0, 0.13334845), (1, 0.13335024), (2, 0.1333503), (3, 0.4666021), (4, 0.13334896)]\n",
      "197: [(0, 0.3000076), (1, 0.29988894), (2, 0.13337262), (3, 0.13336015), (4, 0.1333707)]\n",
      "198: [(0, 0.2383546), (1, 0.114333056), (2, 0.27598366), (3, 0.25700077), (4, 0.11432789)]\n",
      "199: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "200: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "201: [(0, 0.16001123), (1, 0.16001363), (2, 0.16001262), (3, 0.16000898), (4, 0.35995352)]\n",
      "202: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "203: [(0, 0.25703084), (1, 0.11433217), (2, 0.11433223), (3, 0.25721267), (4, 0.25709212)]\n",
      "204: [(0, 0.2976205), (1, 0.2250567), (2, 0.10331039), (3, 0.14918917), (4, 0.22482322)]\n",
      "205: [(0, 0.15787944), (1, 0.11432589), (2, 0.49916196), (3, 0.11431184), (4, 0.114320874)]\n",
      "206: [(0, 0.08893278), (1, 0.42235935), (2, 0.20001358), (3, 0.08892278), (4, 0.19977152)]\n",
      "207: [(0, 0.49179947), (1, 0.28991726), (2, 0.07276624), (3, 0.07275428), (4, 0.07276278)]\n",
      "208: [(0, 0.16003627), (1, 0.35980755), (2, 0.16004111), (3, 0.16003117), (4, 0.16008395)]\n",
      "209: [(0, 0.13337429), (1, 0.299921), (2, 0.13338147), (3, 0.29994687), (4, 0.13337633)]\n",
      "210: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "211: [(0, 0.088931516), (1, 0.19986877), (2, 0.20006874), (3, 0.08892182), (4, 0.4222091)]\n",
      "212: [(0, 0.13335814), (1, 0.13336104), (2, 0.46657005), (3, 0.13335238), (4, 0.13335834)]\n",
      "213: [(0, 0.11432802), (1, 0.3998648), (2, 0.1143345), (3, 0.11431908), (4, 0.25715363)]\n",
      "214: [(0, 0.25349692), (1, 0.3113137), (2, 0.053372916), (3, 0.1952636), (4, 0.18655288)]\n",
      "215: [(0, 0.29062575), (1, 0.11242875), (2, 0.12176063), (3, 0.36283827), (4, 0.11234659)]\n",
      "216: [(0, 0.13336952), (1, 0.29987293), (2, 0.13337536), (3, 0.13336137), (4, 0.30002084)]\n",
      "217: [(0, 0.3625325), (1, 0.2374453), (2, 0.1752386), (3, 0.17474048), (4, 0.05004313)]\n",
      "218: [(0, 0.13336973), (1, 0.13337858), (2, 0.13337523), (3, 0.30004388), (4, 0.29983258)]\n",
      "219: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "220: [(0, 0.16002804), (1, 0.16003212), (2, 0.16003723), (3, 0.35987392), (4, 0.16002871)]\n",
      "221: [(0, 0.31112292), (1, 0.31119677), (2, 0.088951975), (3, 0.19978267), (4, 0.08894568)]\n",
      "222: [(0, 0.35999906), (1, 0.16000023), (2, 0.16000023), (3, 0.16000023), (4, 0.16000023)]\n",
      "223: [(0, 0.05717651), (1, 0.057179064), (2, 0.057179406), (3, 0.7001446), (4, 0.12832043)]\n",
      "224: [(0, 0.4665842), (1, 0.13335602), (2, 0.13335676), (3, 0.13334885), (4, 0.13335417)]\n",
      "225: [(0, 0.16001451), (1, 0.16001636), (2, 0.1600157), (3, 0.1600119), (4, 0.35994157)]\n",
      "226: [(0, 0.19971152), (1, 0.19990107), (2, 0.18730725), (3, 0.1017786), (4, 0.31130156)]\n",
      "227: [(0, 0.35999906), (1, 0.16000025), (2, 0.16000025), (3, 0.16000025), (4, 0.16000025)]\n",
      "228: [(0, 0.088927574), (1, 0.20005186), (2, 0.42226824), (3, 0.088919), (4, 0.19983333)]\n",
      "229: [(0, 0.114340864), (1, 0.25730202), (2, 0.25711566), (3, 0.25690114), (4, 0.11434035)]\n",
      "230: [(0, 0.16675945), (1, 0.08002981), (2, 0.1931994), (3, 0.47998443), (4, 0.08002688)]\n",
      "231: [(0, 0.061557095), (1, 0.07783429), (2, 0.061560474), (3, 0.66067773), (4, 0.13837044)]\n",
      "232: [(0, 0.2796993), (1, 0.28002706), (2, 0.18000776), (3, 0.080056585), (4, 0.1802093)]\n",
      "233: [(0, 0.35999906), (1, 0.16000025), (2, 0.16000025), (3, 0.16000025), (4, 0.16000025)]\n",
      "234: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "235: [(0, 0.2569084), (1, 0.114352554), (2, 0.2573722), (3, 0.25702393), (4, 0.114342935)]\n",
      "236: [(0, 0.42226765), (1, 0.08892609), (2, 0.08892709), (3, 0.3109574), (4, 0.088921785)]\n",
      "237: [(0, 0.063807525), (1, 0.12962484), (2, 0.41320783), (3, 0.057157766), (4, 0.33620203)]\n",
      "238: [(0, 0.16002788), (1, 0.16003087), (2, 0.16003904), (3, 0.16002119), (4, 0.35988104)]\n",
      "239: [(0, 0.08003332), (1, 0.37992766), (2, 0.08003784), (3, 0.09389432), (4, 0.36610684)]\n",
      "240: [(0, 0.088910446), (1, 0.08891286), (2, 0.08891271), (3, 0.6443538), (4, 0.088910215)]\n",
      "241: [(0, 0.22468619), (1, 0.10102561), (2, 0.34939614), (3, 0.22484167), (4, 0.10005037)]\n",
      "242: [(0, 0.1333634), (1, 0.46654624), (2, 0.13336784), (3, 0.13335848), (4, 0.13336408)]\n",
      "243: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "244: [(0, 0.46651796), (1, 0.13337448), (2, 0.13337578), (3, 0.13336149), (4, 0.13337028)]\n",
      "245: [(0, 0.16000584), (1, 0.35997707), (2, 0.16000658), (3, 0.1600046), (4, 0.16000587)]\n",
      "246: [(0, 0.1334125), (1, 0.30008468), (2, 0.13341247), (3, 0.13338386), (4, 0.29970655)]\n",
      "247: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "248: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "249: [(0, 0.1600225), (1, 0.16002643), (2, 0.3599099), (3, 0.16001804), (4, 0.16002311)]\n",
      "250: [(0, 0.13782759), (1, 0.13838638), (2, 0.061614513), (3, 0.5236949), (4, 0.13847665)]\n",
      "251: [(0, 0.19460179), (1, 0.08003798), (2, 0.0800379), (3, 0.56528634), (4, 0.08003596)]\n",
      "252: [(0, 0.32100493), (1, 0.19988504), (2, 0.35153103), (3, 0.070402), (4, 0.05717697)]\n",
      "253: [(0, 0.10004372), (1, 0.35005707), (2, 0.10004947), (3, 0.1000337), (4, 0.34981605)]\n",
      "254: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "255: [(0, 0.08892063), (1, 0.42233604), (2, 0.20003854), (3, 0.19978349), (4, 0.08892127)]\n",
      "256: [(0, 0.29982024), (1, 0.13337821), (2, 0.13337806), (3, 0.30004957), (4, 0.1333739)]\n",
      "257: [(0, 0.1333946), (1, 0.29978612), (2, 0.30005533), (3, 0.13337442), (4, 0.13338953)]\n",
      "258: [(0, 0.47853392), (1, 0.062324993), (2, 0.13055868), (3, 0.0571627), (4, 0.27141967)]\n",
      "259: [(0, 0.25161234), (1, 0.11430941), (2, 0.11987305), (3, 0.39989826), (4, 0.11430691)]\n",
      "260: [(0, 0.16002701), (1, 0.16002378), (2, 0.1600271), (3, 0.16001771), (4, 0.35990438)]\n",
      "261: [(0, 0.050028432), (1, 0.29840592), (2, 0.11233562), (3, 0.05154205), (4, 0.487688)]\n",
      "262: [(0, 0.2569437), (1, 0.11433234), (2, 0.11433417), (3, 0.114318214), (4, 0.40007156)]\n",
      "263: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "264: [(0, 0.25451022), (1, 0.34554663), (2, 0.25440264), (3, 0.07276395), (4, 0.07277654)]\n",
      "265: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "266: [(0, 0.3453643), (1, 0.21113202), (2, 0.16544539), (3, 0.15556876), (4, 0.12248957)]\n",
      "267: [(0, 0.114325576), (1, 0.25687796), (2, 0.40015075), (3, 0.11431778), (4, 0.11432796)]\n",
      "268: [(0, 0.16002625), (1, 0.16002949), (2, 0.35989618), (3, 0.16002074), (4, 0.16002731)]\n",
      "269: [(0, 0.114301205), (1, 0.11430358), (2, 0.25720313), (3, 0.11429802), (4, 0.39989406)]\n",
      "270: [(0, 0.256998), (1, 0.11438457), (2, 0.11438677), (3, 0.39985275), (4, 0.114377946)]\n",
      "271: [(0, 0.08891799), (1, 0.19981119), (2, 0.088923834), (3, 0.08891185), (4, 0.53343517)]\n",
      "272: [(0, 0.11434357), (1, 0.21662322), (2, 0.15464666), (3, 0.25724232), (4, 0.25714424)]\n",
      "273: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "274: [(0, 0.11432047), (1, 0.11432499), (2, 0.1143267), (3, 0.39995113), (4, 0.2570767)]\n",
      "275: [(0, 0.10003159), (1, 0.3498738), (2, 0.10003653), (3, 0.10002549), (4, 0.3500326)]\n",
      "276: [(0, 0.16266148), (1, 0.35733724), (2, 0.16000043), (3, 0.16000043), (4, 0.16000043)]\n",
      "277: [(0, 0.31120518), (1, 0.0889067), (2, 0.31098753), (3, 0.088901445), (4, 0.1999992)]\n",
      "278: [(0, 0.13338412), (1, 0.2999406), (2, 0.13339281), (3, 0.2998978), (4, 0.1333847)]\n",
      "279: [(0, 0.23312739), (1, 0.2335305), (2, 0.14991678), (3, 0.15001786), (4, 0.23340744)]\n",
      "280: [(0, 0.47496912), (1, 0.100050256), (2, 0.2249025), (3, 0.10003395), (4, 0.100044206)]\n",
      "281: [(0, 0.31114626), (1, 0.088977784), (2, 0.10120892), (3, 0.19987659), (4, 0.2987904)]\n",
      "282: [(0, 0.10002819), (1, 0.10003177), (2, 0.10003278), (3, 0.47490886), (4, 0.22499844)]\n",
      "283: [(0, 0.16001192), (1, 0.16001356), (2, 0.16001374), (3, 0.16000958), (4, 0.35995123)]\n",
      "284: [(0, 0.11430871), (1, 0.25710076), (2, 0.3999734), (3, 0.114304304), (4, 0.11431282)]\n",
      "285: [(0, 0.16000026), (1, 0.1613418), (2, 0.35865742), (3, 0.16000026), (4, 0.16000026)]\n",
      "286: [(0, 0.06157961), (1, 0.29232034), (2, 0.13841341), (3, 0.21545647), (4, 0.2922302)]\n",
      "287: [(0, 0.2567061), (1, 0.120378196), (2, 0.39413145), (3, 0.11437738), (4, 0.11440688)]\n",
      "288: [(0, 0.10003399), (1, 0.22487356), (2, 0.32641667), (3, 0.24864127), (4, 0.100034505)]\n",
      "289: [(0, 0.08002179), (1, 0.08040823), (2, 0.27959195), (3, 0.38007003), (4, 0.17990804)]\n",
      "290: [(0, 0.13334408), (1, 0.13334638), (2, 0.13334543), (3, 0.13334194), (4, 0.46662217)]\n",
      "291: [(0, 0.22807418), (1, 0.3468303), (2, 0.100032985), (3, 0.22503264), (4, 0.10002991)]\n",
      "292: [(0, 0.45691743), (1, 0.14162152), (2, 0.11390495), (3, 0.2375218), (4, 0.050034314)]\n",
      "293: [(0, 0.16002178), (1, 0.16002397), (2, 0.35991532), (3, 0.16001655), (4, 0.1600224)]\n",
      "294: [(0, 0.08005845), (1, 0.28011122), (2, 0.18000482), (3, 0.17985605), (4, 0.27996942)]\n",
      "295: [(0, 0.29993424), (1, 0.13342482), (2, 0.13341624), (3, 0.13339211), (4, 0.2998326)]\n",
      "296: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "297: [(0, 0.13339494), (1, 0.2999237), (2, 0.13340418), (3, 0.13337982), (4, 0.29989737)]\n",
      "298: [(0, 0.16001365), (1, 0.3599459), (2, 0.16001569), (3, 0.16001062), (4, 0.16001418)]\n",
      "299: [(0, 0.160035), (1, 0.16003777), (2, 0.16004494), (3, 0.35984644), (4, 0.16003586)]\n",
      "300: [(0, 0.16001801), (1, 0.16002013), (2, 0.16002145), (3, 0.16001365), (4, 0.3599268)]\n",
      "301: [(0, 0.16002944), (1, 0.35988164), (2, 0.1600343), (3, 0.16002367), (4, 0.1600309)]\n",
      "302: [(0, 0.23334852), (1, 0.15004183), (2, 0.14981306), (3, 0.16531128), (4, 0.30148533)]\n",
      "303: [(0, 0.35052317), (1, 0.100103304), (2, 0.22413656), (3, 0.10007191), (4, 0.22516507)]\n",
      "304: [(0, 0.1600119), (1, 0.16001353), (2, 0.16001381), (3, 0.16000952), (4, 0.3599512)]\n",
      "305: [(0, 0.10003834), (1, 0.22509903), (2, 0.2248738), (3, 0.3499499), (4, 0.10003893)]\n",
      "306: [(0, 0.16000584), (1, 0.35997707), (2, 0.16000658), (3, 0.1600046), (4, 0.16000587)]\n",
      "307: [(0, 0.16000958), (1, 0.16001064), (2, 0.16001087), (3, 0.35995921), (4, 0.16000964)]\n",
      "308: [(0, 0.114320695), (1, 0.11432529), (2, 0.11432564), (3, 0.25692979), (4, 0.4000986)]\n",
      "309: [(0, 0.16003287), (1, 0.35986924), (2, 0.16003707), (3, 0.16002591), (4, 0.16003491)]\n",
      "310: [(0, 0.19948065), (1, 0.31103972), (2, 0.3064659), (3, 0.09405812), (4, 0.088955626)]\n",
      "311: [(0, 0.16341922), (1, 0.07276016), (2, 0.6183149), (3, 0.072749466), (4, 0.0727563)]\n",
      "312: [(0, 0.64865047), (1, 0.05675397), (2, 0.054797433), (3, 0.18644883), (4, 0.053349275)]\n",
      "313: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "314: [(0, 0.2998698), (1, 0.13336621), (2, 0.1333661), (3, 0.13335495), (4, 0.30004293)]\n",
      "315: [(0, 0.16001157), (1, 0.16001315), (2, 0.16001344), (3, 0.16000907), (4, 0.35995275)]\n",
      "316: [(0, 0.3598914), (1, 0.16003123), (2, 0.1600308), (3, 0.1600198), (4, 0.16002677)]\n",
      "317: [(0, 0.3421856), (1, 0.23700652), (2, 0.13208906), (3, 0.21595238), (4, 0.07276645)]\n",
      "318: [(0, 0.35999906), (1, 0.16000023), (2, 0.16000023), (3, 0.16000023), (4, 0.16000023)]\n",
      "319: [(0, 0.29986754), (1, 0.1333688), (2, 0.3000428), (3, 0.13335694), (4, 0.13336386)]\n",
      "320: [(0, 0.35985488), (1, 0.16004089), (2, 0.16004089), (3, 0.16002792), (4, 0.16003543)]\n",
      "321: [(0, 0.321995), (1, 0.024262555), (2, 0.20607595), (3, 0.22326326), (4, 0.22440319)]\n",
      "322: [(0, 0.59988344), (1, 0.061561517), (2, 0.061561476), (3, 0.061554167), (4, 0.21543942)]\n",
      "323: [(0, 0.2778848), (1, 0.13335784), (2, 0.32205117), (3, 0.13335031), (4, 0.13335583)]\n",
      "324: [(0, 0.2544681), (1, 0.07278248), (2, 0.25466183), (3, 0.16346726), (4, 0.25462028)]\n",
      "325: [(0, 0.5482727), (1, 0.04213661), (2, 0.042654693), (3, 0.21982141), (4, 0.1471146)]\n",
      "326: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "327: [(0, 0.64408237), (1, 0.08898951), (2, 0.088990614), (3, 0.088958), (4, 0.08897956)]\n",
      "328: [(0, 0.17979723), (1, 0.080059275), (2, 0.17988262), (3, 0.38018727), (4, 0.18007357)]\n",
      "329: [(0, 0.114307046), (1, 0.11431027), (2, 0.25702155), (3, 0.40004754), (4, 0.1143136)]\n",
      "330: [(0, 0.3598518), (1, 0.16004033), (2, 0.1600404), (3, 0.16002828), (4, 0.16003917)]\n",
      "331: [(0, 0.10002833), (1, 0.100032605), (2, 0.10003272), (3, 0.5998791), (4, 0.10002728)]\n",
      "332: [(0, 0.3599446), (1, 0.1600157), (2, 0.1600156), (3, 0.16001049), (4, 0.16001356)]\n",
      "333: [(0, 0.1799349), (1, 0.080095254), (2, 0.2797338), (3, 0.20639677), (4, 0.25383925)]\n",
      "334: [(0, 0.1513284), (1, 0.094513245), (2, 0.04448916), (3, 0.1470365), (4, 0.56263274)]\n",
      "335: [(0, 0.2997443), (1, 0.13338411), (2, 0.13339013), (3, 0.13336977), (4, 0.30011168)]\n",
      "336: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "337: [(0, 0.133343), (1, 0.13334407), (2, 0.13334414), (3, 0.46662596), (4, 0.13334285)]\n",
      "338: [(0, 0.16000958), (1, 0.16001064), (2, 0.16001087), (3, 0.35995921), (4, 0.16000964)]\n",
      "339: [(0, 0.16002981), (1, 0.16003212), (2, 0.16003343), (3, 0.35987604), (4, 0.16002862)]\n",
      "340: [(0, 0.25704432), (1, 0.11434534), (2, 0.114346124), (3, 0.25704047), (4, 0.25722373)]\n",
      "341: [(0, 0.47886917), (1, 0.0800259), (2, 0.081163816), (3, 0.080018036), (4, 0.2799231)]\n",
      "342: [(0, 0.11430028), (1, 0.25710988), (2, 0.11430205), (3, 0.11429674), (4, 0.39999107)]\n",
      "343: [(0, 0.1497692), (1, 0.06669486), (2, 0.15009768), (3, 0.48340288), (4, 0.15003538)]\n",
      "344: [(0, 0.16003424), (1, 0.16004242), (2, 0.16003834), (3, 0.16002613), (4, 0.3598588)]\n",
      "345: [(0, 0.29991934), (1, 0.13338464), (2, 0.13338435), (3, 0.29993337), (4, 0.13337831)]\n",
      "346: [(0, 0.46653017), (1, 0.1333717), (2, 0.13337202), (3, 0.13335891), (4, 0.13336723)]\n",
      "347: [(0, 0.11431255), (1, 0.114316456), (2, 0.40007758), (3, 0.114307605), (4, 0.25698584)]\n",
      "348: [(0, 0.07275631), (1, 0.07276141), (2, 0.07275946), (3, 0.4364434), (4, 0.3452794)]\n",
      "349: [(0, 0.17486072), (1, 0.050049014), (2, 0.117534496), (3, 0.54537255), (4, 0.11218323)]\n",
      "350: [(0, 0.26848313), (1, 0.16321193), (2, 0.15762042), (3, 0.07094662), (4, 0.33973786)]\n",
      "351: [(0, 0.31102857), (1, 0.19993404), (2, 0.08895456), (3, 0.19993073), (4, 0.20015207)]\n",
      "352: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "353: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "354: [(0, 0.16001259), (1, 0.1600143), (2, 0.1600141), (3, 0.16000944), (4, 0.35994962)]\n",
      "355: [(0, 0.0889639), (1, 0.19997829), (2, 0.08899102), (3, 0.19931448), (4, 0.42275232)]\n",
      "356: [(0, 0.0889475), (1, 0.1998773), (2, 0.16023731), (3, 0.23971157), (4, 0.31122628)]\n",
      "357: [(0, 0.35985085), (1, 0.16004312), (2, 0.16004224), (3, 0.16002832), (4, 0.16003545)]\n",
      "358: [(0, 0.08892394), (1, 0.08892855), (2, 0.39899263), (3, 0.3342304), (4, 0.088924445)]\n",
      "359: [(0, 0.12834902), (1, 0.556373), (2, 0.19992316), (3, 0.058176834), (4, 0.057177942)]\n",
      "360: [(0, 0.22489408), (1, 0.35020986), (2, 0.1000712), (3, 0.22476147), (4, 0.100063406)]\n",
      "361: [(0, 0.16002573), (1, 0.16003028), (2, 0.35989767), (3, 0.16002), (4, 0.1600263)]\n",
      "362: [(0, 0.11430663), (1, 0.11430885), (2, 0.11430897), (3, 0.114301614), (4, 0.5427739)]\n",
      "363: [(0, 0.6175732), (1, 0.066701055), (2, 0.06670178), (3, 0.14978173), (4, 0.099242225)]\n",
      "364: [(0, 0.11434254), (1, 0.25705102), (2, 0.11435207), (3, 0.32868138), (4, 0.18557294)]\n",
      "365: [(0, 0.1600318), (1, 0.3598731), (2, 0.16003811), (3, 0.16002469), (4, 0.16003235)]\n",
      "366: [(0, 0.13460502), (1, 0.4653468), (2, 0.13335194), (3, 0.13334619), (4, 0.13335004)]\n",
      "367: [(0, 0.22491436), (1, 0.22486588), (2, 0.22499195), (3, 0.100052595), (4, 0.22517522)]\n",
      "368: [(0, 0.14877617), (1, 0.15536329), (2, 0.16223975), (3, 0.43356684), (4, 0.10005392)]\n",
      "369: [(0, 0.25938705), (1, 0.11198682), (2, 0.24479422), (3, 0.19203751), (4, 0.19179441)]\n",
      "370: [(0, 0.29994395), (1, 0.13339965), (2, 0.1334011), (3, 0.13338132), (4, 0.299874)]\n",
      "371: [(0, 0.19958594), (1, 0.20021258), (2, 0.0889677), (3, 0.31109846), (4, 0.20013534)]\n",
      "372: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "373: [(0, 0.1600506), (1, 0.3033326), (2, 0.21652222), (3, 0.16004199), (4, 0.16005257)]\n",
      "374: [(0, 0.1143539), (1, 0.2571498), (2, 0.1282966), (3, 0.3858433), (4, 0.11435644)]\n",
      "375: [(0, 0.35999906), (1, 0.16000025), (2, 0.16000025), (3, 0.16000025), (4, 0.16000025)]\n",
      "376: [(0, 0.16000527), (1, 0.1600059), (2, 0.16000599), (3, 0.35997763), (4, 0.16000524)]\n",
      "377: [(0, 0.31106254), (1, 0.18675189), (2, 0.18373683), (3, 0.118663274), (4, 0.1997855)]\n",
      "378: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "379: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "380: [(0, 0.1143537), (1, 0.5425862), (2, 0.11436491), (3, 0.11433965), (4, 0.11435557)]\n",
      "381: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "382: [(0, 0.1747595), (1, 0.61294466), (2, 0.112233534), (3, 0.05002687), (4, 0.050035458)]\n",
      "383: [(0, 0.16000026), (1, 0.16134223), (2, 0.358657), (3, 0.16000026), (4, 0.16000026)]\n",
      "384: [(0, 0.16002716), (1, 0.35989395), (2, 0.16003132), (3, 0.16002096), (4, 0.16002661)]\n",
      "385: [(0, 0.11433491), (1, 0.39978182), (2, 0.11434108), (3, 0.25720605), (4, 0.11433616)]\n",
      "386: [(0, 0.25703493), (1, 0.11432012), (2, 0.11431992), (3, 0.1143123), (4, 0.40001273)]\n",
      "387: [(0, 0.16000584), (1, 0.35997707), (2, 0.16000658), (3, 0.1600046), (4, 0.16000587)]\n",
      "388: [(0, 0.2570133), (1, 0.1869614), (2, 0.09542143), (3, 0.3380623), (4, 0.122541584)]\n",
      "389: [(0, 0.35999906), (1, 0.16000023), (2, 0.16000023), (3, 0.16000023), (4, 0.16000023)]\n",
      "390: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "391: [(0, 0.3500003), (1, 0.100064054), (2, 0.100060284), (3, 0.100042835), (4, 0.34983253)]\n",
      "392: [(0, 0.16002806), (1, 0.16003215), (2, 0.16003726), (3, 0.35987383), (4, 0.16002873)]\n",
      "393: [(0, 0.2999309), (1, 0.13336067), (2, 0.13336124), (3, 0.2999892), (4, 0.13335799)]\n",
      "394: [(0, 0.15946361), (1, 0.21191593), (2, 0.25718722), (3, 0.25707564), (4, 0.11435766)]\n",
      "395: [(0, 0.080030106), (1, 0.08003407), (2, 0.17983304), (3, 0.58007246), (4, 0.08003037)]\n",
      "396: [(0, 0.13336533), (1, 0.46653703), (2, 0.13337304), (3, 0.1333582), (4, 0.13336639)]\n",
      "397: [(0, 0.042126518), (1, 0.09877446), (2, 0.20624973), (3, 0.55816174), (4, 0.09468759)]\n",
      "398: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "399: [(0, 0.1999753), (1, 0.20001234), (2, 0.08898695), (3, 0.31110016), (4, 0.19992529)]\n",
      "400: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "401: [(0, 0.13335942), (1, 0.13336211), (2, 0.13336149), (3, 0.46655846), (4, 0.13335855)]\n",
      "402: [(0, 0.25441638), (1, 0.112094834), (2, 0.0727754), (3, 0.3970444), (4, 0.16366902)]\n",
      "403: [(0, 0.16001257), (1, 0.1600143), (2, 0.1600141), (3, 0.16000944), (4, 0.35994962)]\n",
      "404: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "405: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "406: [(0, 0.29999155), (1, 0.13338713), (2, 0.1333841), (3, 0.13336757), (4, 0.29986966)]\n",
      "407: [(0, 0.29358685), (1, 0.13337338), (2, 0.13986966), (3, 0.29980057), (4, 0.13336955)]\n",
      "408: [(0, 0.16000932), (1, 0.16001049), (2, 0.35996348), (3, 0.1600074), (4, 0.16000926)]\n",
      "409: [(0, 0.180109), (1, 0.3799367), (2, 0.08003579), (3, 0.27988714), (4, 0.080031335)]\n",
      "410: [(0, 0.32218933), (1, 0.26667437), (2, 0.044488728), (3, 0.21117483), (4, 0.15547273)]\n",
      "411: [(0, 0.13842703), (1, 0.25702372), (2, 0.2571631), (3, 0.114317596), (4, 0.23306857)]\n",
      "412: [(0, 0.089240484), (1, 0.3706676), (2, 0.08004399), (3, 0.18003722), (4, 0.28001073)]\n",
      "413: [(0, 0.36905694), (1, 0.10591048), (2, 0.10571198), (3, 0.22346519), (4, 0.19585535)]\n",
      "414: [(0, 0.16003734), (1, 0.16004124), (2, 0.16003908), (3, 0.16002716), (4, 0.35985518)]\n",
      "415: [(0, 0.13338096), (1, 0.46648207), (2, 0.13338736), (3, 0.13336922), (4, 0.13338034)]\n",
      "416: [(0, 0.08895564), (1, 0.42197543), (2, 0.16031092), (3, 0.23980105), (4, 0.08895692)]\n",
      "417: [(0, 0.35990435), (1, 0.16002727), (2, 0.16002625), (3, 0.16001832), (4, 0.16002387)]\n",
      "418: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "419: [(0, 0.25708562), (1, 0.11434278), (2, 0.11434277), (3, 0.114324845), (4, 0.39990395)]\n",
      "420: [(0, 0.23320521), (1, 0.15016635), (2, 0.066732645), (3, 0.23304088), (4, 0.31685492)]\n",
      "421: [(0, 0.13334405), (1, 0.13334548), (2, 0.30003926), (3, 0.13334143), (4, 0.29992974)]\n",
      "422: [(0, 0.25164157), (1, 0.11432115), (2, 0.11989216), (3, 0.3998277), (4, 0.1143174)]\n",
      "423: [(0, 0.13336036), (1, 0.4665587), (2, 0.13336398), (3, 0.13335614), (4, 0.13336086)]\n",
      "424: [(0, 0.13335687), (1, 0.13335967), (2, 0.1333619), (3, 0.13335155), (4, 0.46657)]\n",
      "425: [(0, 0.32537547), (1, 0.11433058), (2, 0.11432934), (3, 0.25718674), (4, 0.18877788)]\n",
      "426: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "427: [(0, 0.17999174), (1, 0.08006166), (2, 0.28027567), (3, 0.17972447), (4, 0.27994648)]\n",
      "428: [(0, 0.13335381), (1, 0.13335702), (2, 0.13335699), (3, 0.4665781), (4, 0.1333541)]\n",
      "429: [(0, 0.16002229), (1, 0.16002472), (2, 0.35991302), (3, 0.16001783), (4, 0.16002212)]\n",
      "430: [(0, 0.3531561), (1, 0.16108234), (2, 0.11432223), (3, 0.11431211), (4, 0.25712723)]\n",
      "431: [(0, 0.22067942), (1, 0.46535513), (2, 0.12866923), (3, 0.12810488), (4, 0.057191335)]\n",
      "432: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "433: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "434: [(0, 0.114302695), (1, 0.11430503), (2, 0.114305146), (3, 0.3999836), (4, 0.2571035)]\n",
      "435: [(0, 0.080037944), (1, 0.123299465), (2, 0.08004284), (3, 0.43667337), (4, 0.27994645)]\n",
      "436: [(0, 0.0906093), (1, 0.08003173), (2, 0.08003237), (3, 0.29084015), (4, 0.45848647)]\n",
      "437: [(0, 0.22480097), (1, 0.10004835), (2, 0.100051135), (3, 0.47505516), (4, 0.100044414)]\n",
      "438: [(0, 0.29996052), (1, 0.29993832), (2, 0.1333724), (3, 0.1333604), (4, 0.13336831)]\n",
      "439: [(0, 0.14961328), (1, 0.23327012), (2, 0.4816786), (3, 0.06873005), (4, 0.066707954)]\n",
      "440: [(0, 0.5211384), (1, 0.21533063), (2, 0.1403951), (3, 0.06156367), (4, 0.061572112)]\n",
      "441: [(0, 0.2571864), (1, 0.2571419), (2, 0.11430077), (3, 0.11429603), (4, 0.25707492)]\n",
      "442: [(0, 0.17989306), (1, 0.18001531), (2, 0.17997158), (3, 0.17985027), (4, 0.2802697)]\n",
      "443: [(0, 0.08893401), (1, 0.31110275), (2, 0.20011702), (3, 0.19981194), (4, 0.2000343)]\n",
      "444: [(0, 0.08892204), (1, 0.19985221), (2, 0.08892588), (3, 0.53337806), (4, 0.08892178)]\n",
      "445: [(0, 0.13339277), (1, 0.13339993), (2, 0.13339779), (3, 0.29992425), (4, 0.2998853)]\n",
      "446: [(0, 0.19997287), (1, 0.31121594), (2, 0.20003137), (3, 0.08894164), (4, 0.19983816)]\n",
      "447: [(0, 0.13333838), (1, 0.13333899), (2, 0.13333906), (3, 0.46664524), (4, 0.13333835)]\n",
      "448: [(0, 0.13339601), (1, 0.14644594), (2, 0.28714642), (3, 0.13337146), (4, 0.29964018)]\n",
      "449: [(0, 0.1600145), (1, 0.16001634), (2, 0.16001569), (3, 0.16001189), (4, 0.35994157)]\n",
      "450: [(0, 0.35990357), (1, 0.1600269), (2, 0.16002756), (3, 0.16001813), (4, 0.16002388)]\n",
      "451: [(0, 0.47499287), (1, 0.10005152), (2, 0.100053266), (3, 0.22485648), (4, 0.10004587)]\n",
      "452: [(0, 0.08002297), (1, 0.080026455), (2, 0.080026284), (3, 0.664544), (4, 0.09538037)]\n",
      "453: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "454: [(0, 0.16001365), (1, 0.3599459), (2, 0.1600157), (3, 0.16001062), (4, 0.16001418)]\n",
      "455: [(0, 0.48535046), (1, 0.17200114), (2, 0.17737484), (3, 0.052966386), (4, 0.11230715)]\n",
      "456: [(0, 0.27127364), (1, 0.33926326), (2, 0.05717153), (3, 0.060859893), (4, 0.27143165)]\n",
      "457: [(0, 0.18626323), (1, 0.45253497), (2, 0.05338033), (3, 0.054457944), (4, 0.25336352)]\n",
      "458: [(0, 0.13335061), (1, 0.13335218), (2, 0.13335279), (3, 0.4665942), (4, 0.13335018)]\n",
      "459: [(0, 0.13338386), (1, 0.13339159), (2, 0.29990035), (3, 0.13337258), (4, 0.29995164)]\n",
      "460: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "461: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "462: [(0, 0.3196962), (1, 0.03811224), (2, 0.09312367), (3, 0.51095766), (4, 0.038110312)]\n",
      "463: [(0, 0.095352195), (1, 0.19988374), (2, 0.08892357), (3, 0.52692103), (4, 0.08891944)]\n",
      "464: [(0, 0.18016252), (1, 0.3801062), (2, 0.17990783), (3, 0.08003676), (4, 0.17978665)]\n",
      "465: [(0, 0.13336757), (1, 0.1333749), (2, 0.13337187), (3, 0.3000364), (4, 0.29984924)]\n",
      "466: [(0, 0.1600218), (1, 0.16002397), (2, 0.3599153), (3, 0.16001657), (4, 0.16002242)]\n",
      "467: [(0, 0.13336925), (1, 0.29994124), (2, 0.29995608), (3, 0.13336203), (4, 0.1333714)]\n",
      "468: [(0, 0.16000704), (1, 0.16000782), (2, 0.35997227), (3, 0.16000544), (4, 0.1600074)]\n",
      "469: [(0, 0.16001642), (1, 0.1600199), (2, 0.16002028), (3, 0.16001362), (4, 0.3599298)]\n",
      "470: [(0, 0.4800944), (1, 0.08004081), (2, 0.080038704), (3, 0.18004033), (4, 0.17978576)]\n",
      "471: [(0, 0.16002797), (1, 0.1600297), (2, 0.1600297), (3, 0.16002093), (4, 0.35989177)]\n",
      "472: [(0, 0.06669154), (1, 0.06669443), (2, 0.14979291), (3, 0.06668571), (4, 0.6501354)]\n",
      "473: [(0, 0.25438133), (1, 0.07276626), (2, 0.072767265), (3, 0.43643674), (4, 0.16364838)]\n",
      "474: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "475: [(0, 0.34415588), (1, 0.3453148), (2, 0.074120924), (3, 0.16364129), (4, 0.072767094)]\n",
      "476: [(0, 0.22470544), (1, 0.100071296), (2, 0.35011357), (3, 0.22504678), (4, 0.10006288)]\n",
      "477: [(0, 0.13338515), (1, 0.1345086), (2, 0.2990842), (3, 0.29963642), (4, 0.1333856)]\n",
      "478: [(0, 0.25702077), (1, 0.114333324), (2, 0.11433428), (3, 0.32879198), (4, 0.18551967)]\n",
      "479: [(0, 0.16002294), (1, 0.16002634), (2, 0.3599083), (3, 0.16001815), (4, 0.16002427)]\n",
      "480: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "481: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "482: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "483: [(0, 0.16000932), (1, 0.16001049), (2, 0.35996348), (3, 0.1600074), (4, 0.16000926)]\n",
      "484: [(0, 0.29986286), (1, 0.13341117), (2, 0.13341223), (3, 0.13338432), (4, 0.2999294)]\n",
      "485: [(0, 0.13336596), (1, 0.13336876), (2, 0.1333708), (3, 0.46652815), (4, 0.13336635)]\n",
      "486: [(0, 0.16000958), (1, 0.16001064), (2, 0.16001087), (3, 0.35995921), (4, 0.16000964)]\n",
      "487: [(0, 0.13335252), (1, 0.13335596), (2, 0.13337405), (3, 0.13334821), (4, 0.4665693)]\n",
      "488: [(0, 0.16002463), (1, 0.16002756), (2, 0.16002992), (3, 0.16001907), (4, 0.35989884)]\n",
      "489: [(0, 0.16001193), (1, 0.16001356), (2, 0.16001382), (3, 0.16000955), (4, 0.3599512)]\n",
      "490: [(0, 0.16000932), (1, 0.16001049), (2, 0.35996348), (3, 0.1600074), (4, 0.16000926)]\n",
      "491: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "492: [(0, 0.16002227), (1, 0.1600247), (2, 0.35991305), (3, 0.16001782), (4, 0.16002211)]\n",
      "493: [(0, 0.06670696), (1, 0.14984661), (2, 0.31670865), (3, 0.40002927), (4, 0.066708475)]\n",
      "494: [(0, 0.05337152), (1, 0.19235241), (2, 0.5147197), (3, 0.053360607), (4, 0.18619575)]\n",
      "495: [(0, 0.114299454), (1, 0.114301756), (2, 0.11430134), (3, 0.39999977), (4, 0.25709766)]\n",
      "496: [(0, 0.15786657), (1, 0.11432061), (2, 0.49918637), (3, 0.11430935), (4, 0.11431709)]\n",
      "497: [(0, 0.22466339), (1, 0.1001141), (2, 0.22509065), (3, 0.35003927), (4, 0.10009258)]\n",
      "498: [(0, 0.10346688), (1, 0.40070373), (2, 0.1405269), (3, 0.25199813), (4, 0.10330437)]\n",
      "499: [(0, 0.37371257), (1, 0.18005364), (2, 0.08637586), (3, 0.27980882), (4, 0.08004913)]\n",
      "500: [(0, 0.16003342), (1, 0.16003738), (2, 0.16004075), (3, 0.35985303), (4, 0.16003543)]\n",
      "501: [(0, 0.22589357), (1, 0.0889337), (2, 0.1999895), (3, 0.3962537), (4, 0.08892953)]\n",
      "502: [(0, 0.11962324), (1, 0.11988869), (2, 0.25357196), (3, 0.38679287), (4, 0.12012325)]\n",
      "503: [(0, 0.29984257), (1, 0.13337107), (2, 0.1333709), (3, 0.30004853), (4, 0.1333669)]\n",
      "504: [(0, 0.08893841), (1, 0.08894721), (2, 0.1999307), (3, 0.19977269), (4, 0.42241105)]\n",
      "505: [(0, 0.16004182), (1, 0.1600504), (2, 0.16004886), (3, 0.35981238), (4, 0.16004656)]\n",
      "506: [(0, 0.049919836), (1, 0.453742), (2, 0.05255969), (3, 0.27922487), (4, 0.16455355)]\n",
      "507: [(0, 0.38951156), (1, 0.115212485), (2, 0.26667356), (3, 0.114299566), (4, 0.11430282)]\n",
      "508: [(0, 0.35989794), (1, 0.16002809), (2, 0.16002835), (3, 0.16002078), (4, 0.16002488)]\n",
      "509: [(0, 0.105758496), (1, 0.22349136), (2, 0.2825825), (3, 0.22356652), (4, 0.16460113)]\n",
      "510: [(0, 0.25433096), (1, 0.07275972), (2, 0.16375344), (3, 0.34547162), (4, 0.16368423)]\n",
      "511: [(0, 0.16003738), (1, 0.16004129), (2, 0.16003911), (3, 0.16002719), (4, 0.35985506)]\n",
      "512: [(0, 0.1600387), (1, 0.35984862), (2, 0.1600437), (3, 0.16002934), (4, 0.16003965)]\n",
      "513: [(0, 0.4000172), (1, 0.11431726), (2, 0.25704503), (3, 0.11430679), (4, 0.11431371)]\n",
      "514: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "515: [(0, 0.067178205), (1, 0.5668946), (2, 0.06672405), (3, 0.14964065), (4, 0.14956257)]\n",
      "516: [(0, 0.13341473), (1, 0.2998165), (2, 0.13342834), (3, 0.29992503), (4, 0.13341543)]\n",
      "517: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "518: [(0, 0.16002007), (1, 0.16002287), (2, 0.1600229), (3, 0.35991356), (4, 0.16002056)]\n",
      "519: [(0, 0.25692913), (1, 0.114429586), (2, 0.2573311), (3, 0.11438298), (4, 0.25692722)]\n",
      "520: [(0, 0.13336012), (1, 0.13336404), (2, 0.1333689), (3, 0.46654618), (4, 0.13336077)]\n",
      "521: [(0, 0.09755221), (1, 0.3111317), (2, 0.08892301), (3, 0.41347435), (4, 0.08891876)]\n",
      "522: [(0, 0.16266559), (1, 0.35733312), (2, 0.16000043), (3, 0.16000043), (4, 0.16000043)]\n",
      "523: [(0, 0.13335994), (1, 0.13336283), (2, 0.2999225), (3, 0.29999492), (4, 0.13335983)]\n",
      "524: [(0, 0.16002984), (1, 0.1600332), (2, 0.3598842), (3, 0.16002281), (4, 0.1600299)]\n",
      "525: [(0, 0.19999312), (1, 0.08900328), (2, 0.08900262), (3, 0.19984736), (4, 0.42215368)]\n",
      "526: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "527: [(0, 0.1998497), (1, 0.08895328), (2, 0.20002414), (3, 0.42222694), (4, 0.08894594)]\n",
      "528: [(0, 0.31099194), (1, 0.08893569), (2, 0.19991224), (3, 0.3112271), (4, 0.08893304)]\n",
      "529: [(0, 0.10001986), (1, 0.10002212), (2, 0.1000221), (3, 0.10552696), (4, 0.594409)]\n",
      "530: [(0, 0.17983979), (1, 0.17974411), (2, 0.28058195), (3, 0.17989077), (4, 0.17994338)]\n",
      "531: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "532: [(0, 0.32012585), (1, 0.16003427), (2, 0.1997849), (3, 0.16002308), (4, 0.16003194)]\n",
      "533: [(0, 0.094479725), (1, 0.14718726), (2, 0.39257917), (3, 0.27103892), (4, 0.09471496)]\n",
      "534: [(0, 0.16002797), (1, 0.1600297), (2, 0.16002968), (3, 0.16002093), (4, 0.35989174)]\n",
      "535: [(0, 0.19979286), (1, 0.20014848), (2, 0.19984081), (3, 0.08892697), (4, 0.31129086)]\n",
      "536: [(0, 0.06367438), (1, 0.047070663), (2, 0.78994584), (3, 0.052239615), (4, 0.04706949)]\n",
      "537: [(0, 0.25690663), (1, 0.11436096), (2, 0.11435925), (3, 0.2570489), (4, 0.25732425)]\n",
      "538: [(0, 0.16000026), (1, 0.16134202), (2, 0.35865718), (3, 0.16000026), (4, 0.16000026)]\n",
      "539: [(0, 0.1333763), (1, 0.13338187), (2, 0.29998735), (3, 0.2998782), (4, 0.13337629)]\n",
      "540: [(0, 0.10003351), (1, 0.2249723), (2, 0.10003737), (3, 0.47492263), (4, 0.10003419)]\n",
      "541: [(0, 0.16000682), (1, 0.16000745), (2, 0.1600079), (3, 0.3599709), (4, 0.16000691)]\n",
      "542: [(0, 0.17827709), (1, 0.13338183), (2, 0.2999926), (3, 0.2549703), (4, 0.13337821)]\n",
      "543: [(0, 0.16350445), (1, 0.07279634), (2, 0.3458059), (3, 0.34510234), (4, 0.072790995)]\n",
      "544: [(0, 0.16001259), (1, 0.1600143), (2, 0.1600141), (3, 0.16000944), (4, 0.35994962)]\n",
      "545: [(0, 0.2457943), (1, 0.23133652), (2, 0.17833027), (3, 0.25889105), (4, 0.085647844)]\n",
      "546: [(0, 0.16000932), (1, 0.16001049), (2, 0.35996348), (3, 0.1600074), (4, 0.16000926)]\n",
      "547: [(0, 0.35999906), (1, 0.16000025), (2, 0.16000025), (3, 0.16000025), (4, 0.16000025)]\n",
      "548: [(0, 0.16002427), (1, 0.16002731), (2, 0.16002753), (3, 0.16001852), (4, 0.35990238)]\n",
      "549: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "550: [(0, 0.13336402), (1, 0.29999825), (2, 0.29991534), (3, 0.13335827), (4, 0.13336414)]\n",
      "551: [(0, 0.35984078), (1, 0.1600447), (2, 0.16004433), (3, 0.16003004), (4, 0.16004016)]\n",
      "552: [(0, 0.11538396), (1, 0.39889768), (2, 0.1143111), (3, 0.11430272), (4, 0.25710455)]\n",
      "553: [(0, 0.081539296), (1, 0.296734), (2, 0.1727399), (3, 0.41259837), (4, 0.03638843)]\n",
      "554: [(0, 0.05791019), (1, 0.315402), (2, 0.07691608), (3, 0.29641917), (4, 0.25335255)]\n",
      "555: [(0, 0.16004327), (1, 0.16004333), (2, 0.16004437), (3, 0.16003142), (4, 0.35983762)]\n",
      "556: [(0, 0.2250303), (1, 0.1888235), (2, 0.22503433), (3, 0.26110238), (4, 0.10000947)]\n",
      "557: [(0, 0.13338315), (1, 0.29998216), (2, 0.2998796), (3, 0.1333723), (4, 0.13338278)]\n",
      "558: [(0, 0.29986086), (1, 0.29997176), (2, 0.13339715), (3, 0.133379), (4, 0.13339122)]\n",
      "559: [(0, 0.16002943), (1, 0.35988173), (2, 0.16003428), (3, 0.16002367), (4, 0.16003089)]\n",
      "560: [(0, 0.16000527), (1, 0.1600059), (2, 0.16000599), (3, 0.35997763), (4, 0.16000524)]\n",
      "561: [(0, 0.06668804), (1, 0.23329912), (2, 0.06669171), (3, 0.48329863), (4, 0.15002248)]\n",
      "562: [(0, 0.16000028), (1, 0.16000028), (2, 0.35999885), (3, 0.16000028), (4, 0.16000028)]\n",
      "563: [(0, 0.1600145), (1, 0.16001633), (2, 0.16001569), (3, 0.16001189), (4, 0.3599416)]\n",
      "564: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "565: [(0, 0.16003695), (1, 0.16004005), (2, 0.16003907), (3, 0.3598462), (4, 0.16003779)]\n",
      "566: [(0, 0.04707971), (1, 0.6355427), (2, 0.04708256), (3, 0.105779625), (4, 0.16451548)]\n",
      "567: [(0, 0.35987666), (1, 0.1600345), (2, 0.16003415), (3, 0.16002406), (4, 0.16003059)]\n",
      "568: [(0, 0.13334998), (1, 0.13335225), (2, 0.13335487), (3, 0.46659258), (4, 0.13335036)]\n",
      "569: [(0, 0.11431501), (1, 0.11431824), (2, 0.25699958), (3, 0.11430813), (4, 0.40005904)]\n",
      "570: [(0, 0.06669405), (1, 0.38115704), (2, 0.066696644), (3, 0.4187591), (4, 0.06669317)]\n",
      "571: [(0, 0.29986155), (1, 0.2999918), (2, 0.13338938), (3, 0.1333724), (4, 0.13338485)]\n",
      "572: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "573: [(0, 0.06158188), (1, 0.19035825), (2, 0.21540293), (3, 0.31731683), (4, 0.21534012)]\n",
      "574: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "575: [(0, 0.13337697), (1, 0.2999164), (2, 0.2999617), (3, 0.13336638), (4, 0.13337857)]\n",
      "576: [(0, 0.29982588), (1, 0.13337652), (2, 0.1333761), (3, 0.3000507), (4, 0.1333708)]\n",
      "577: [(0, 0.16000958), (1, 0.16001064), (2, 0.16001087), (3, 0.35995921), (4, 0.16000964)]\n",
      "578: [(0, 0.2569518), (1, 0.1143544), (2, 0.11435796), (3, 0.2572228), (4, 0.257113)]\n",
      "579: [(0, 0.17453453), (1, 0.118362345), (2, 0.48227862), (3, 0.17478889), (4, 0.05003564)]\n",
      "580: [(0, 0.3969527), (1, 0.23337615), (2, 0.14362629), (3, 0.076025054), (4, 0.15001984)]\n",
      "581: [(0, 0.2516637), (1, 0.2570185), (2, 0.11989265), (3, 0.25710112), (4, 0.11432402)]\n",
      "582: [(0, 0.11432539), (1, 0.40000194), (2, 0.2570303), (3, 0.11431613), (4, 0.11432623)]\n",
      "583: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "584: [(0, 0.16002974), (1, 0.35987818), (2, 0.16003473), (3, 0.16002499), (4, 0.16003233)]\n",
      "585: [(0, 0.18650061), (1, 0.16348605), (2, 0.25467965), (3, 0.16358556), (4, 0.23174813)]\n",
      "586: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "587: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "588: [(0, 0.299899), (1, 0.13339183), (2, 0.13339242), (3, 0.13337344), (4, 0.2999433)]\n",
      "589: [(0, 0.117328316), (1, 0.19329692), (2, 0.322476), (3, 0.10004635), (4, 0.26685238)]\n",
      "590: [(0, 0.3596337), (1, 0.16009788), (2, 0.16010328), (3, 0.1600724), (4, 0.16009268)]\n",
      "591: [(0, 0.1143005), (1, 0.114302196), (2, 0.11430233), (3, 0.5427943), (4, 0.11430072)]\n",
      "592: [(0, 0.19983906), (1, 0.2714656), (2, 0.05718787), (3, 0.4143243), (4, 0.057183124)]\n",
      "593: [(0, 0.13976866), (1, 0.14071205), (2, 0.1894257), (3, 0.3900679), (4, 0.14002563)]\n",
      "594: [(0, 0.13334897), (1, 0.13335085), (2, 0.13335145), (3, 0.29998893), (4, 0.29995987)]\n",
      "595: [(0, 0.3488625), (1, 0.16186762), (2, 0.11794757), (3, 0.25698614), (4, 0.11433615)]\n",
      "596: [(0, 0.053203963), (1, 0.25274485), (2, 0.40402597), (3, 0.0936839), (4, 0.19634129)]\n",
      "597: [(0, 0.11432621), (1, 0.114334606), (2, 0.25711277), (3, 0.39989784), (4, 0.11432857)]\n",
      "598: [(0, 0.31090245), (1, 0.311143), (2, 0.08894494), (3, 0.20007147), (4, 0.08893819)]\n",
      "599: [(0, 0.13741009), (1, 0.050206076), (2, 0.40011394), (3, 0.112461425), (4, 0.29980844)]\n",
      "600: [(0, 0.10001643), (1, 0.22501314), (2, 0.10001851), (3, 0.47493547), (4, 0.100016415)]\n",
      "601: [(0, 0.06157478), (1, 0.21487299), (2, 0.36965108), (3, 0.1385575), (4, 0.21534365)]\n",
      "602: [(0, 0.08894525), (1, 0.19997978), (2, 0.19996595), (3, 0.19978744), (4, 0.31132162)]\n",
      "603: [(0, 0.29980958), (1, 0.13342144), (2, 0.13342611), (3, 0.2999266), (4, 0.13341627)]\n",
      "604: [(0, 0.30009866), (1, 0.13336731), (2, 0.13336992), (3, 0.13335663), (4, 0.2998075)]\n",
      "605: [(0, 0.35992026), (1, 0.16002274), (2, 0.16002245), (3, 0.16001529), (4, 0.16001928)]\n",
      "606: [(0, 0.100043535), (1, 0.10003962), (2, 0.10004669), (3, 0.5998338), (4, 0.10003639)]\n",
      "607: [(0, 0.13335693), (1, 0.13336107), (2, 0.13336089), (3, 0.14799342), (4, 0.45192772)]\n",
      "608: [(0, 0.3998207), (1, 0.25724298), (2, 0.114316806), (3, 0.11430678), (4, 0.11431275)]\n",
      "609: [(0, 0.17983073), (1, 0.28023216), (2, 0.17985946), (3, 0.2800201), (4, 0.08005757)]\n",
      "610: [(0, 0.10003423), (1, 0.47502586), (2, 0.10003857), (3, 0.10002609), (4, 0.22487521)]\n",
      "611: [(0, 0.16002989), (1, 0.16003324), (2, 0.35988414), (3, 0.16002281), (4, 0.16002992)]\n",
      "612: [(0, 0.08003624), (1, 0.17980185), (2, 0.08004267), (3, 0.56128514), (4, 0.09883413)]\n",
      "613: [(0, 0.1333791), (1, 0.13338597), (2, 0.30001628), (3, 0.29983756), (4, 0.13338108)]\n",
      "614: [(0, 0.23555645), (1, 0.17914277), (2, 0.16964325), (3, 0.26254824), (4, 0.15310927)]\n",
      "615: [(0, 0.35985225), (1, 0.16004047), (2, 0.16004062), (3, 0.16002852), (4, 0.16003814)]\n",
      "616: [(0, 0.16006431), (1, 0.3597559), (2, 0.16007039), (3, 0.16004397), (4, 0.16006543)]\n",
      "617: [(0, 0.19599345), (1, 0.19999759), (2, 0.0933495), (3, 0.3112784), (4, 0.19938104)]\n",
      "618: [(0, 0.16266187), (1, 0.35733685), (2, 0.16000043), (3, 0.16000043), (4, 0.16000043)]\n",
      "619: [(0, 0.057168756), (1, 0.16181138), (2, 0.12858272), (3, 0.5867175), (4, 0.06571967)]\n",
      "620: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "621: [(0, 0.088901006), (1, 0.08890247), (2, 0.088902086), (3, 0.09282062), (4, 0.64047384)]\n",
      "622: [(0, 0.2617654), (1, 0.17265017), (2, 0.10622197), (3, 0.28672722), (4, 0.17263521)]\n",
      "623: [(0, 0.23292379), (1, 0.06670473), (2, 0.40028125), (3, 0.06669226), (4, 0.23339793)]\n",
      "624: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "625: [(0, 0.25436687), (1, 0.25474334), (2, 0.07278683), (3, 0.16340286), (4, 0.25470012)]\n",
      "626: [(0, 0.35985914), (1, 0.16003942), (2, 0.16003913), (3, 0.16002727), (4, 0.16003507)]\n",
      "627: [(0, 0.11429151), (1, 0.11429222), (2, 0.114292406), (3, 0.5428322), (4, 0.11429166)]\n",
      "628: [(0, 0.16000527), (1, 0.1600059), (2, 0.16000599), (3, 0.35997763), (4, 0.16000524)]\n",
      "629: [(0, 0.20502222), (1, 0.19500248), (2, 0.31114614), (3, 0.08891211), (4, 0.19991706)]\n",
      "630: [(0, 0.17145091), (1, 0.28789717), (2, 0.23395236), (3, 0.13571168), (4, 0.17098783)]\n",
      "631: [(0, 0.3598433), (1, 0.16004433), (2, 0.16004334), (3, 0.160031), (4, 0.16003798)]\n",
      "632: [(0, 0.0858598), (1, 0.05716019), (2, 0.6715836), (3, 0.12823798), (4, 0.05715846)]\n",
      "633: [(0, 0.08558852), (1, 0.27996746), (2, 0.08006383), (3, 0.17977186), (4, 0.37460834)]\n",
      "634: [(0, 0.28235275), (1, 0.10581788), (2, 0.2112674), (3, 0.17704585), (4, 0.2235161)]\n",
      "635: [(0, 0.18699127), (1, 0.2119795), (2, 0.13884893), (3, 0.27505258), (4, 0.1871277)]\n",
      "636: [(0, 0.088916086), (1, 0.08892015), (2, 0.3110482), (3, 0.20000896), (4, 0.31110662)]\n",
      "637: [(0, 0.3240241), (1, 0.31805936), (2, 0.08550056), (3, 0.1866623), (4, 0.08575372)]\n",
      "638: [(0, 0.08890431), (1, 0.08890683), (2, 0.088906676), (3, 0.64437664), (4, 0.08890553)]\n",
      "639: [(0, 0.19977117), (1, 0.20003118), (2, 0.27182955), (3, 0.12807624), (4, 0.20029186)]\n",
      "640: [(0, 0.12834945), (1, 0.12829106), (2, 0.05719341), (3, 0.34301394), (4, 0.3431521)]\n",
      "641: [(0, 0.18101355), (1, 0.16043533), (2, 0.14132206), (3, 0.14175525), (4, 0.37547383)]\n",
      "642: [(0, 0.1600272), (1, 0.16003025), (2, 0.35989267), (3, 0.16002057), (4, 0.16002929)]\n",
      "643: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "644: [(0, 0.16001861), (1, 0.16002151), (2, 0.16002177), (3, 0.16001473), (4, 0.35992336)]\n",
      "645: [(0, 0.16002625), (1, 0.16002947), (2, 0.35989627), (3, 0.16002072), (4, 0.1600273)]\n",
      "646: [(0, 0.16002242), (1, 0.35990864), (2, 0.16002695), (3, 0.16001794), (4, 0.16002402)]\n",
      "647: [(0, 0.25436315), (1, 0.25458223), (2, 0.072762154), (3, 0.34553367), (4, 0.07275881)]\n",
      "648: [(0, 0.08890905), (1, 0.08891129), (2, 0.08891272), (3, 0.623456), (4, 0.10981095)]\n",
      "649: [(0, 0.2504319), (1, 0.1973927), (2, 0.19131076), (3, 0.07310612), (4, 0.28775856)]\n",
      "650: [(0, 0.21734685), (1, 0.3174015), (2, 0.07276409), (3, 0.31972748), (4, 0.072760075)]\n",
      "651: [(0, 0.13335526), (1, 0.30000558), (2, 0.13335945), (3, 0.13335097), (4, 0.29992872)]\n",
      "652: [(0, 0.07173144), (1, 0.12996432), (2, 0.07185486), (3, 0.53455937), (4, 0.19189003)]\n",
      "653: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "654: [(0, 0.100022495), (1, 0.10002561), (2, 0.22482482), (3, 0.4751046), (4, 0.10002247)]\n",
      "655: [(0, 0.19982688), (1, 0.16790652), (2, 0.12850705), (3, 0.4465781), (4, 0.057181455)]\n",
      "656: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "657: [(0, 0.29974908), (1, 0.13339087), (2, 0.13339213), (3, 0.3000837), (4, 0.13338423)]\n",
      "658: [(0, 0.100048445), (1, 0.22485784), (2, 0.22492103), (3, 0.3501253), (4, 0.10004739)]\n",
      "659: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "660: [(0, 0.3616335), (1, 0.036368947), (2, 0.1604293), (3, 0.19977386), (4, 0.24179436)]\n",
      "661: [(0, 0.1600057), (1, 0.1600062), (2, 0.16000651), (3, 0.1600043), (4, 0.35997733)]\n",
      "662: [(0, 0.13337648), (1, 0.13338229), (2, 0.13338344), (3, 0.13336651), (4, 0.46649128)]\n",
      "663: [(0, 0.3273424), (1, 0.334406), (2, 0.13830991), (3, 0.061575945), (4, 0.13836579)]\n",
      "664: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "665: [(0, 0.13335973), (1, 0.29989827), (2, 0.1333628), (3, 0.3000187), (4, 0.1333605)]\n",
      "666: [(0, 0.16000958), (1, 0.16001064), (2, 0.16001087), (3, 0.35995921), (4, 0.16000964)]\n",
      "667: [(0, 0.13337338), (1, 0.4665094), (2, 0.13337922), (3, 0.13336429), (4, 0.13337366)]\n",
      "668: [(0, 0.16231535), (1, 0.23654285), (2, 0.2773468), (3, 0.13815355), (4, 0.18564148)]\n",
      "669: [(0, 0.100068115), (1, 0.47546148), (2, 0.22434196), (3, 0.10005718), (4, 0.10007127)]\n",
      "670: [(0, 0.16004942), (1, 0.16006115), (2, 0.16005453), (3, 0.35978863), (4, 0.1600463)]\n",
      "671: [(0, 0.16001365), (1, 0.3599459), (2, 0.16001569), (3, 0.16001062), (4, 0.16001418)]\n",
      "672: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "673: [(0, 0.16007158), (1, 0.16008085), (2, 0.16008303), (3, 0.16005462), (4, 0.35970995)]\n",
      "674: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "675: [(0, 0.16002801), (1, 0.16002972), (2, 0.1600297), (3, 0.16002095), (4, 0.35989165)]\n",
      "676: [(0, 0.25702757), (1, 0.25714794), (2, 0.25719076), (3, 0.114313446), (4, 0.114320256)]\n",
      "677: [(0, 0.26682696), (1, 0.13301578), (2, 0.28356788), (3, 0.19997124), (4, 0.11661813)]\n",
      "678: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "679: [(0, 0.114308394), (1, 0.11431084), (2, 0.25704393), (3, 0.40002847), (4, 0.11430834)]\n",
      "680: [(0, 0.1383562), (1, 0.13834825), (2, 0.29261497), (3, 0.29221758), (4, 0.13846304)]\n",
      "681: [(0, 0.3137247), (1, 0.17296223), (2, 0.15181789), (3, 0.15941255), (4, 0.20208259)]\n",
      "682: [(0, 0.16002986), (1, 0.16003321), (2, 0.35988417), (3, 0.16002281), (4, 0.16002992)]\n",
      "683: [(0, 0.22485583), (1, 0.10005566), (2, 0.10915421), (3, 0.10003973), (4, 0.4658946)]\n",
      "684: [(0, 0.114356734), (1, 0.1143662), (2, 0.11436738), (3, 0.25695467), (4, 0.39995503)]\n",
      "685: [(0, 0.0800323), (1, 0.18006289), (2, 0.08003594), (3, 0.18003443), (4, 0.47983447)]\n",
      "686: [(0, 0.16001642), (1, 0.1600199), (2, 0.16002028), (3, 0.16001362), (4, 0.35992977)]\n",
      "687: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "688: [(0, 0.16002329), (1, 0.16002654), (2, 0.16002786), (3, 0.16001807), (4, 0.3599042)]\n",
      "689: [(0, 0.13335994), (1, 0.1333633), (2, 0.29993206), (3, 0.13335371), (4, 0.29999098)]\n",
      "690: [(0, 0.13335845), (1, 0.29989377), (2, 0.13336122), (3, 0.30002746), (4, 0.1333591)]\n",
      "691: [(0, 0.11438224), (1, 0.2573751), (2, 0.3995127), (3, 0.11435236), (4, 0.1143776)]\n",
      "692: [(0, 0.16000682), (1, 0.16000745), (2, 0.16000788), (3, 0.3599709), (4, 0.16000691)]\n",
      "693: [(0, 0.35985088), (1, 0.16004117), (2, 0.16004239), (3, 0.16002794), (4, 0.16003759)]\n",
      "694: [(0, 0.16003597), (1, 0.16004206), (2, 0.16004154), (3, 0.35984394), (4, 0.16003646)]\n",
      "695: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "696: [(0, 0.16004339), (1, 0.35982665), (2, 0.16004917), (3, 0.16003431), (4, 0.16004652)]\n",
      "697: [(0, 0.39981505), (1, 0.11433948), (2, 0.11433843), (3, 0.11432414), (4, 0.2571829)]\n",
      "698: [(0, 0.16003585), (1, 0.35985678), (2, 0.16004238), (3, 0.1600279), (4, 0.16003706)]\n",
      "699: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "700: [(0, 0.19609074), (1, 0.1270634), (2, 0.081916705), (3, 0.24877565), (4, 0.34615353)]\n",
      "701: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "702: [(0, 0.16000682), (1, 0.16000745), (2, 0.1600079), (3, 0.3599709), (4, 0.16000693)]\n",
      "703: [(0, 0.11431132), (1, 0.11431681), (2, 0.25699872), (3, 0.40006173), (4, 0.11431138)]\n",
      "704: [(0, 0.16002251), (1, 0.16002645), (2, 0.3599099), (3, 0.16001804), (4, 0.16002312)]\n",
      "705: [(0, 0.13334176), (1, 0.13334277), (2, 0.13334301), (3, 0.13333997), (4, 0.46663246)]\n",
      "706: [(0, 0.2521099), (1, 0.2064218), (2, 0.091876216), (3, 0.3428647), (4, 0.10672743)]\n",
      "707: [(0, 0.22444291), (1, 0.35036397), (2, 0.10007415), (3, 0.10005), (4, 0.22506899)]\n",
      "708: [(0, 0.13336441), (1, 0.13336778), (2, 0.13336822), (3, 0.29992613), (4, 0.29997346)]\n",
      "709: [(0, 0.26299205), (1, 0.17984322), (2, 0.39711133), (3, 0.08002306), (4, 0.08003036)]\n",
      "710: [(0, 0.1600116), (1, 0.16001327), (2, 0.16001314), (3, 0.35995007), (4, 0.1600119)]\n",
      "711: [(0, 0.16000915), (1, 0.16001032), (2, 0.33677354), (3, 0.18319772), (4, 0.16000931)]\n",
      "712: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "713: [(0, 0.07276286), (1, 0.25446296), (2, 0.072767146), (3, 0.25457406), (4, 0.3454329)]\n",
      "714: [(0, 0.13335355), (1, 0.46658656), (2, 0.13335711), (3, 0.13334899), (4, 0.13335383)]\n",
      "715: [(0, 0.08002632), (1, 0.08002912), (2, 0.17988333), (3, 0.18000062), (4, 0.48006058)]\n",
      "716: [(0, 0.16002338), (1, 0.35990766), (2, 0.16002676), (3, 0.16001841), (4, 0.16002378)]\n",
      "717: [(0, 0.13335824), (1, 0.13336113), (2, 0.13336512), (3, 0.13335231), (4, 0.46656317)]\n",
      "718: [(0, 0.40364033), (1, 0.044472955), (2, 0.27549815), (3, 0.17657709), (4, 0.099811465)]\n",
      "719: [(0, 0.031747222), (1, 0.06854985), (2, 0.10758657), (3, 0.22884503), (4, 0.5632713)]\n",
      "720: [(0, 0.27993292), (1, 0.08003235), (2, 0.36692238), (3, 0.19308378), (4, 0.08002856)]\n",
      "721: [(0, 0.16001955), (1, 0.35992557), (2, 0.16002153), (3, 0.1600144), (4, 0.16001898)]\n",
      "722: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "723: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "724: [(0, 0.114322975), (1, 0.1143265), (2, 0.114327185), (3, 0.39997166), (4, 0.25705168)]\n",
      "725: [(0, 0.11432513), (1, 0.11432967), (2, 0.4000392), (3, 0.25698125), (4, 0.11432474)]\n",
      "726: [(0, 0.45904985), (1, 0.13333876), (2, 0.14093594), (3, 0.13333718), (4, 0.13333824)]\n",
      "727: [(0, 0.31108192), (1, 0.08893962), (2, 0.08893916), (3, 0.20007172), (4, 0.31096762)]\n",
      "728: [(0, 0.08001573), (1, 0.28002846), (2, 0.08001829), (3, 0.38008785), (4, 0.17984964)]\n",
      "729: [(0, 0.16002886), (1, 0.16003358), (2, 0.16003494), (3, 0.35987356), (4, 0.16002902)]\n",
      "730: [(0, 0.19986437), (1, 0.12854332), (2, 0.05717503), (3, 0.48588884), (4, 0.12852846)]\n",
      "731: [(0, 0.03781725), (1, 0.041037973), (2, 0.10586463), (3, 0.73368084), (4, 0.08159928)]\n",
      "732: [(0, 0.042807974), (1, 0.030844236), (2, 0.4684646), (3, 0.27340198), (4, 0.1844812)]\n",
      "733: [(0, 0.1600057), (1, 0.1600062), (2, 0.16000651), (3, 0.1600043), (4, 0.35997733)]\n",
      "734: [(0, 0.16001192), (1, 0.16001356), (2, 0.16001374), (3, 0.16000958), (4, 0.3599512)]\n",
      "735: [(0, 0.3498976), (1, 0.100039065), (2, 0.22505398), (3, 0.22497432), (4, 0.100035004)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<gensim.models.ldamodel.LdaModel at 0x14c0fd460>,\n",
       " [{'document': 0,\n",
       "   'probabilities': {0: 0.13339068,\n",
       "    1: 0.13340044,\n",
       "    2: 0.29977468,\n",
       "    3: 0.13337661,\n",
       "    4: 0.30005756}},\n",
       "  {'document': 1,\n",
       "   'probabilities': {0: 0.1801595,\n",
       "    1: 0.080051996,\n",
       "    2: 0.17985356,\n",
       "    3: 0.2798353,\n",
       "    4: 0.28009963}},\n",
       "  {'document': 2,\n",
       "   'probabilities': {0: 0.072747946,\n",
       "    1: 0.082578264,\n",
       "    2: 0.6082912,\n",
       "    3: 0.16363402,\n",
       "    4: 0.07274856}},\n",
       "  {'document': 3,\n",
       "   'probabilities': {0: 0.16002986,\n",
       "    1: 0.16003324,\n",
       "    2: 0.35988414,\n",
       "    3: 0.16002281,\n",
       "    4: 0.16002992}},\n",
       "  {'document': 4,\n",
       "   'probabilities': {0: 0.24000439,\n",
       "    1: 0.24003349,\n",
       "    2: 0.09836267,\n",
       "    3: 0.19017732,\n",
       "    4: 0.23142211}},\n",
       "  {'document': 5,\n",
       "   'probabilities': {0: 0.11950696,\n",
       "    1: 0.0734529,\n",
       "    2: 0.38002276,\n",
       "    3: 0.11582601,\n",
       "    4: 0.31119138}},\n",
       "  {'document': 6,\n",
       "   'probabilities': {0: 0.28000858,\n",
       "    1: 0.17994799,\n",
       "    2: 0.17977923,\n",
       "    3: 0.08005413,\n",
       "    4: 0.28021008}},\n",
       "  {'document': 7,\n",
       "   'probabilities': {0: 0.16001572,\n",
       "    1: 0.16001886,\n",
       "    2: 0.16001788,\n",
       "    3: 0.35993007,\n",
       "    4: 0.16001745}},\n",
       "  {'document': 8,\n",
       "   'probabilities': {0: 0.1600299,\n",
       "    1: 0.16003327,\n",
       "    2: 0.35988408,\n",
       "    3: 0.16002283,\n",
       "    4: 0.16002996}},\n",
       "  {'document': 9,\n",
       "   'probabilities': {0: 0.2532661,\n",
       "    1: 0.1122951,\n",
       "    2: 0.14083813,\n",
       "    3: 0.44356298,\n",
       "    4: 0.05003767}},\n",
       "  {'document': 10,\n",
       "   'probabilities': {0: 0.13335124,\n",
       "    1: 0.13335516,\n",
       "    2: 0.4665942,\n",
       "    3: 0.13334827,\n",
       "    4: 0.13335119}},\n",
       "  {'document': 11,\n",
       "   'probabilities': {0: 0.07792799,\n",
       "    1: 0.03481608,\n",
       "    2: 0.16070563,\n",
       "    3: 0.5317675,\n",
       "    4: 0.1947828}},\n",
       "  {'document': 12,\n",
       "   'probabilities': {0: 0.36501274,\n",
       "    1: 0.052634653,\n",
       "    2: 0.19405536,\n",
       "    3: 0.1589939,\n",
       "    4: 0.22930337}},\n",
       "  {'document': 13,\n",
       "   'probabilities': {0: 0.13336916,\n",
       "    1: 0.13337411,\n",
       "    2: 0.13337456,\n",
       "    3: 0.29993924,\n",
       "    4: 0.29994288}},\n",
       "  {'document': 14,\n",
       "   'probabilities': {0: 0.061560836,\n",
       "    1: 0.21547021,\n",
       "    2: 0.17588438,\n",
       "    3: 0.4855227,\n",
       "    4: 0.06156191}},\n",
       "  {'document': 15,\n",
       "   'probabilities': {0: 0.42174602,\n",
       "    1: 0.07552281,\n",
       "    2: 0.24333194,\n",
       "    3: 0.075663,\n",
       "    4: 0.18373623}},\n",
       "  {'document': 16,\n",
       "   'probabilities': {0: 0.100044265,\n",
       "    1: 0.35003456,\n",
       "    2: 0.22495672,\n",
       "    3: 0.122627795,\n",
       "    4: 0.20233667}},\n",
       "  {'document': 17,\n",
       "   'probabilities': {0: 0.22825544,\n",
       "    1: 0.13328366,\n",
       "    2: 0.46347597,\n",
       "    3: 0.13307098,\n",
       "    4: 0.04191398}},\n",
       "  {'document': 18,\n",
       "   'probabilities': {0: 0.44628072,\n",
       "    1: 0.12590326,\n",
       "    2: 0.09710769,\n",
       "    3: 0.16523945,\n",
       "    4: 0.16546892}},\n",
       "  {'document': 19,\n",
       "   'probabilities': {0: 0.31928623,\n",
       "    1: 0.40301478,\n",
       "    2: 0.05276239,\n",
       "    3: 0.11251747,\n",
       "    4: 0.11241912}},\n",
       "  {'document': 20,\n",
       "   'probabilities': {0: 0.23736425,\n",
       "    1: 0.048047297,\n",
       "    2: 0.0875369,\n",
       "    3: 0.32954046,\n",
       "    4: 0.29751107}},\n",
       "  {'document': 21,\n",
       "   'probabilities': {0: 0.28499907,\n",
       "    1: 0.028602352,\n",
       "    2: 0.13549274,\n",
       "    3: 0.028591894,\n",
       "    4: 0.5223139}},\n",
       "  {'document': 22,\n",
       "   'probabilities': {0: 0.3999445,\n",
       "    1: 0.11431488,\n",
       "    2: 0.11431453,\n",
       "    3: 0.11430537,\n",
       "    4: 0.2571207}},\n",
       "  {'document': 23,\n",
       "   'probabilities': {0: 0.49698514,\n",
       "    1: 0.16452515,\n",
       "    2: 0.04709147,\n",
       "    3: 0.18581453,\n",
       "    4: 0.10558367}},\n",
       "  {'document': 24,\n",
       "   'probabilities': {0: 0.12992322,\n",
       "    1: 0.42708388,\n",
       "    2: 0.048350833,\n",
       "    3: 0.34607735,\n",
       "    4: 0.04856469}},\n",
       "  {'document': 25,\n",
       "   'probabilities': {0: 0.33767173,\n",
       "    1: 0.118624605,\n",
       "    2: 0.056079485,\n",
       "    3: 0.11864937,\n",
       "    4: 0.36897478}},\n",
       "  {'document': 26,\n",
       "   'probabilities': {0: 0.09465078,\n",
       "    1: 0.41054744,\n",
       "    2: 0.09486656,\n",
       "    3: 0.14727864,\n",
       "    4: 0.2526566}},\n",
       "  {'document': 27,\n",
       "   'probabilities': {0: 0.20720929,\n",
       "    1: 0.03698036,\n",
       "    2: 0.097852945,\n",
       "    3: 0.57509303,\n",
       "    4: 0.08286433}},\n",
       "  {'document': 28,\n",
       "   'probabilities': {0: 0.35999906,\n",
       "    1: 0.16000023,\n",
       "    2: 0.16000023,\n",
       "    3: 0.16000023,\n",
       "    4: 0.16000023}},\n",
       "  {'document': 29,\n",
       "   'probabilities': {0: 0.34552816,\n",
       "    1: 0.16337189,\n",
       "    2: 0.2546468,\n",
       "    3: 0.16368072,\n",
       "    4: 0.07277241}},\n",
       "  {'document': 30,\n",
       "   'probabilities': {0: 0.12592264,\n",
       "    1: 0.40828243,\n",
       "    2: 0.039869715,\n",
       "    3: 0.21272694,\n",
       "    4: 0.21319829}},\n",
       "  {'document': 31,\n",
       "   'probabilities': {0: 0.2665346,\n",
       "    1: 0.117264085,\n",
       "    2: 0.31939903,\n",
       "    3: 0.2422662,\n",
       "    4: 0.054536115}},\n",
       "  {'document': 32,\n",
       "   'probabilities': {0: 0.38037893,\n",
       "    1: 0.2582713,\n",
       "    2: 0.13549265,\n",
       "    3: 0.08005155,\n",
       "    4: 0.14580555}},\n",
       "  {'document': 33,\n",
       "   'probabilities': {0: 0.100039735,\n",
       "    1: 0.34965664,\n",
       "    2: 0.35023323,\n",
       "    3: 0.1000307,\n",
       "    4: 0.100039735}},\n",
       "  {'document': 34,\n",
       "   'probabilities': {0: 0.39973137,\n",
       "    1: 0.11435009,\n",
       "    2: 0.114350826,\n",
       "    3: 0.11432914,\n",
       "    4: 0.25723857}},\n",
       "  {'document': 35,\n",
       "   'probabilities': {0: 0.16005519,\n",
       "    1: 0.35980672,\n",
       "    2: 0.16005467,\n",
       "    3: 0.16003472,\n",
       "    4: 0.16004874}},\n",
       "  {'document': 36,\n",
       "   'probabilities': {0: 0.35220066,\n",
       "    1: 0.16000563,\n",
       "    2: 0.16778462,\n",
       "    3: 0.160004,\n",
       "    4: 0.16000511}},\n",
       "  {'document': 37,\n",
       "   'probabilities': {0: 0.1600063,\n",
       "    1: 0.16000731,\n",
       "    2: 0.16000724,\n",
       "    3: 0.35997254,\n",
       "    4: 0.1600066}},\n",
       "  {'document': 38,\n",
       "   'probabilities': {0: 0.053366944,\n",
       "    1: 0.11977814,\n",
       "    2: 0.51387596,\n",
       "    3: 0.19315292,\n",
       "    4: 0.119826026}},\n",
       "  {'document': 39,\n",
       "   'probabilities': {0: 0.13617267,\n",
       "    1: 0.11160093,\n",
       "    2: 0.20178114,\n",
       "    3: 0.084880285,\n",
       "    4: 0.46556497}},\n",
       "  {'document': 40,\n",
       "   'probabilities': {0: 0.1600299,\n",
       "    1: 0.16003326,\n",
       "    2: 0.35988408,\n",
       "    3: 0.16002283,\n",
       "    4: 0.16002993}},\n",
       "  {'document': 41,\n",
       "   'probabilities': {0: 0.27273458,\n",
       "    1: 0.15808639,\n",
       "    2: 0.16574432,\n",
       "    3: 0.2750254,\n",
       "    4: 0.12840933}},\n",
       "  {'document': 42, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 43,\n",
       "   'probabilities': {0: 0.05213704,\n",
       "    1: 0.55878603,\n",
       "    2: 0.059634395,\n",
       "    3: 0.10586462,\n",
       "    4: 0.22357789}},\n",
       "  {'document': 44,\n",
       "   'probabilities': {0: 0.35218266,\n",
       "    1: 0.25146133,\n",
       "    2: 0.22990455,\n",
       "    3: 0.10847566,\n",
       "    4: 0.05797584}},\n",
       "  {'document': 45,\n",
       "   'probabilities': {0: 0.10274692,\n",
       "    1: 0.22346354,\n",
       "    2: 0.059769906,\n",
       "    3: 0.5669377,\n",
       "    4: 0.047081973}},\n",
       "  {'document': 46,\n",
       "   'probabilities': {0: 0.1600006,\n",
       "    1: 0.1600006,\n",
       "    2: 0.3599975,\n",
       "    3: 0.1600006,\n",
       "    4: 0.1600006}},\n",
       "  {'document': 47, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 48,\n",
       "   'probabilities': {0: 0.13336763,\n",
       "    1: 0.29984665,\n",
       "    2: 0.30005765,\n",
       "    3: 0.13336009,\n",
       "    4: 0.13336802}},\n",
       "  {'document': 49,\n",
       "   'probabilities': {0: 0.29987252,\n",
       "    1: 0.13338189,\n",
       "    2: 0.13338086,\n",
       "    3: 0.29998758,\n",
       "    4: 0.1333771}},\n",
       "  {'document': 50,\n",
       "   'probabilities': {0: 0.35999906,\n",
       "    1: 0.16000025,\n",
       "    2: 0.16000025,\n",
       "    3: 0.16000025,\n",
       "    4: 0.16000025}},\n",
       "  {'document': 51,\n",
       "   'probabilities': {0: 0.29983994,\n",
       "    1: 0.13338126,\n",
       "    2: 0.13338035,\n",
       "    3: 0.3000236,\n",
       "    4: 0.13337487}},\n",
       "  {'document': 52,\n",
       "   'probabilities': {0: 0.22491798,\n",
       "    1: 0.3499456,\n",
       "    2: 0.10010361,\n",
       "    3: 0.10006995,\n",
       "    4: 0.22496288}},\n",
       "  {'document': 53,\n",
       "   'probabilities': {0: 0.22499242,\n",
       "    1: 0.22504847,\n",
       "    2: 0.100041956,\n",
       "    3: 0.11103659,\n",
       "    4: 0.33888057}},\n",
       "  {'document': 54,\n",
       "   'probabilities': {0: 0.16002652,\n",
       "    1: 0.1600315,\n",
       "    2: 0.16003127,\n",
       "    3: 0.16002092,\n",
       "    4: 0.35988984}},\n",
       "  {'document': 55,\n",
       "   'probabilities': {0: 0.07345717,\n",
       "    1: 0.254044,\n",
       "    2: 0.07277774,\n",
       "    3: 0.2542851,\n",
       "    4: 0.345436}},\n",
       "  {'document': 56,\n",
       "   'probabilities': {0: 0.2999439,\n",
       "    1: 0.13341735,\n",
       "    2: 0.167439,\n",
       "    3: 0.2657917,\n",
       "    4: 0.13340804}},\n",
       "  {'document': 57,\n",
       "   'probabilities': {0: 0.35991952,\n",
       "    1: 0.16002268,\n",
       "    2: 0.16002245,\n",
       "    3: 0.16001546,\n",
       "    4: 0.16001992}},\n",
       "  {'document': 58,\n",
       "   'probabilities': {0: 0.1600233,\n",
       "    1: 0.16002595,\n",
       "    2: 0.35990953,\n",
       "    3: 0.16001739,\n",
       "    4: 0.16002384}},\n",
       "  {'document': 59,\n",
       "   'probabilities': {0: 0.29990003,\n",
       "    1: 0.13336568,\n",
       "    2: 0.1333661,\n",
       "    3: 0.30000606,\n",
       "    4: 0.13336216}},\n",
       "  {'document': 60, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 61,\n",
       "   'probabilities': {0: 0.1333729,\n",
       "    1: 0.29988846,\n",
       "    2: 0.29999918,\n",
       "    3: 0.1333646,\n",
       "    4: 0.13337488}},\n",
       "  {'document': 62,\n",
       "   'probabilities': {0: 0.13336882,\n",
       "    1: 0.29995662,\n",
       "    2: 0.29994425,\n",
       "    3: 0.13336103,\n",
       "    4: 0.13336924}},\n",
       "  {'document': 63, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 64,\n",
       "   'probabilities': {0: 0.46658966,\n",
       "    1: 0.13335504,\n",
       "    2: 0.13335481,\n",
       "    3: 0.13334812,\n",
       "    4: 0.13335238}},\n",
       "  {'document': 65,\n",
       "   'probabilities': {0: 0.13345557,\n",
       "    1: 0.13342783,\n",
       "    2: 0.13345934,\n",
       "    3: 0.2994789,\n",
       "    4: 0.30017838}},\n",
       "  {'document': 66,\n",
       "   'probabilities': {0: 0.11150836,\n",
       "    1: 0.4635366,\n",
       "    2: 0.100050904,\n",
       "    3: 0.10003418,\n",
       "    4: 0.22486997}},\n",
       "  {'document': 67,\n",
       "   'probabilities': {0: 0.1600244,\n",
       "    1: 0.35990205,\n",
       "    2: 0.1600301,\n",
       "    3: 0.1600191,\n",
       "    4: 0.16002436}},\n",
       "  {'document': 68,\n",
       "   'probabilities': {0: 0.29992118,\n",
       "    1: 0.13335843,\n",
       "    2: 0.13335863,\n",
       "    3: 0.30000606,\n",
       "    4: 0.1333557}},\n",
       "  {'document': 69,\n",
       "   'probabilities': {0: 0.1333639,\n",
       "    1: 0.13336828,\n",
       "    2: 0.29987198,\n",
       "    3: 0.30003187,\n",
       "    4: 0.13336396}},\n",
       "  {'document': 70,\n",
       "   'probabilities': {0: 0.352177,\n",
       "    1: 0.16000564,\n",
       "    2: 0.16780825,\n",
       "    3: 0.160004,\n",
       "    4: 0.16000511}},\n",
       "  {'document': 71,\n",
       "   'probabilities': {0: 0.114322886,\n",
       "    1: 0.2570519,\n",
       "    2: 0.11432666,\n",
       "    3: 0.39997327,\n",
       "    4: 0.11432532}},\n",
       "  {'document': 72,\n",
       "   'probabilities': {0: 0.13334534,\n",
       "    1: 0.2517588,\n",
       "    2: 0.133347,\n",
       "    3: 0.34820333,\n",
       "    4: 0.1333455}},\n",
       "  {'document': 73, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 74, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 75,\n",
       "   'probabilities': {0: 0.16000028,\n",
       "    1: 0.16000028,\n",
       "    2: 0.35999885,\n",
       "    3: 0.16000028,\n",
       "    4: 0.16000028}},\n",
       "  {'document': 76, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 77, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 78,\n",
       "   'probabilities': {0: 0.28002015,\n",
       "    1: 0.1796021,\n",
       "    2: 0.18021005,\n",
       "    3: 0.08004817,\n",
       "    4: 0.2801195}},\n",
       "  {'document': 79,\n",
       "   'probabilities': {0: 0.6193154,\n",
       "    1: 0.091586776,\n",
       "    2: 0.1387754,\n",
       "    3: 0.043735225,\n",
       "    4: 0.10658723}},\n",
       "  {'document': 80,\n",
       "   'probabilities': {0: 0.11433846,\n",
       "    1: 0.25714874,\n",
       "    2: 0.11434485,\n",
       "    3: 0.25724658,\n",
       "    4: 0.25692138}},\n",
       "  {'document': 81,\n",
       "   'probabilities': {0: 0.6415384,\n",
       "    1: 0.07339169,\n",
       "    2: 0.029663574,\n",
       "    3: 0.18941106,\n",
       "    4: 0.06599529}},\n",
       "  {'document': 82,\n",
       "   'probabilities': {0: 0.29995382,\n",
       "    1: 0.13337389,\n",
       "    2: 0.29994172,\n",
       "    3: 0.13336073,\n",
       "    4: 0.13336983}},\n",
       "  {'document': 83,\n",
       "   'probabilities': {0: 0.16002984,\n",
       "    1: 0.16003321,\n",
       "    2: 0.35988426,\n",
       "    3: 0.16002281,\n",
       "    4: 0.1600299}},\n",
       "  {'document': 84,\n",
       "   'probabilities': {0: 0.16007473,\n",
       "    1: 0.16006735,\n",
       "    2: 0.16007161,\n",
       "    3: 0.16004527,\n",
       "    4: 0.3597411}},\n",
       "  {'document': 85,\n",
       "   'probabilities': {0: 0.12304077,\n",
       "    1: 0.1383095,\n",
       "    2: 0.07687215,\n",
       "    3: 0.21529438,\n",
       "    4: 0.4464832}},\n",
       "  {'document': 86,\n",
       "   'probabilities': {0: 0.061544105,\n",
       "    1: 0.46411985,\n",
       "    2: 0.21442838,\n",
       "    3: 0.045299184,\n",
       "    4: 0.21460849}},\n",
       "  {'document': 87,\n",
       "   'probabilities': {0: 0.17220032,\n",
       "    1: 0.038142394,\n",
       "    2: 0.39862213,\n",
       "    3: 0.30921626,\n",
       "    4: 0.081818886}},\n",
       "  {'document': 88,\n",
       "   'probabilities': {0: 0.19851658,\n",
       "    1: 0.48593646,\n",
       "    2: 0.05871623,\n",
       "    3: 0.12821107,\n",
       "    4: 0.12861966}},\n",
       "  {'document': 89,\n",
       "   'probabilities': {0: 0.16002218,\n",
       "    1: 0.35991192,\n",
       "    2: 0.16002528,\n",
       "    3: 0.16001749,\n",
       "    4: 0.16002312}},\n",
       "  {'document': 90,\n",
       "   'probabilities': {0: 0.13334551,\n",
       "    1: 0.133347,\n",
       "    2: 0.29997176,\n",
       "    3: 0.29999033,\n",
       "    4: 0.13334543}},\n",
       "  {'document': 91, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 92,\n",
       "   'probabilities': {0: 0.105619654,\n",
       "    1: 0.16457222,\n",
       "    2: 0.28245106,\n",
       "    3: 0.28260267,\n",
       "    4: 0.16475438}},\n",
       "  {'document': 93,\n",
       "   'probabilities': {0: 0.19989362,\n",
       "    1: 0.19989468,\n",
       "    2: 0.14074147,\n",
       "    3: 0.37053093,\n",
       "    4: 0.08893924}},\n",
       "  {'document': 94,\n",
       "   'probabilities': {0: 0.16002984,\n",
       "    1: 0.1600332,\n",
       "    2: 0.35988423,\n",
       "    3: 0.16002281,\n",
       "    4: 0.1600299}},\n",
       "  {'document': 95,\n",
       "   'probabilities': {0: 0.20861682,\n",
       "    1: 0.07822173,\n",
       "    2: 0.12161245,\n",
       "    3: 0.42628714,\n",
       "    4: 0.16526182}},\n",
       "  {'document': 96,\n",
       "   'probabilities': {0: 0.1497592,\n",
       "    1: 0.15004486,\n",
       "    2: 0.25737053,\n",
       "    3: 0.2437476,\n",
       "    4: 0.19907784}},\n",
       "  {'document': 97,\n",
       "   'probabilities': {0: 0.120500505,\n",
       "    1: 0.3033114,\n",
       "    2: 0.16793725,\n",
       "    3: 0.28747013,\n",
       "    4: 0.12078072}},\n",
       "  {'document': 98,\n",
       "   'probabilities': {0: 0.16002986,\n",
       "    1: 0.16003321,\n",
       "    2: 0.35988417,\n",
       "    3: 0.16002281,\n",
       "    4: 0.1600299}},\n",
       "  {'document': 99,\n",
       "   'probabilities': {0: 0.35219592,\n",
       "    1: 0.16000563,\n",
       "    2: 0.16778935,\n",
       "    3: 0.160004,\n",
       "    4: 0.16000511}},\n",
       "  {'document': 100,\n",
       "   'probabilities': {0: 0.16002986,\n",
       "    1: 0.16003323,\n",
       "    2: 0.35988417,\n",
       "    3: 0.16002281,\n",
       "    4: 0.1600299}},\n",
       "  {'document': 101,\n",
       "   'probabilities': {0: 0.16008197,\n",
       "    1: 0.16010779,\n",
       "    2: 0.35965487,\n",
       "    3: 0.16006969,\n",
       "    4: 0.16008574}},\n",
       "  {'document': 102,\n",
       "   'probabilities': {0: 0.22509329,\n",
       "    1: 0.10003,\n",
       "    2: 0.47482902,\n",
       "    3: 0.10002015,\n",
       "    4: 0.100027524}},\n",
       "  {'document': 103,\n",
       "   'probabilities': {0: 0.35984576,\n",
       "    1: 0.16004103,\n",
       "    2: 0.16004448,\n",
       "    3: 0.16002975,\n",
       "    4: 0.16003898}},\n",
       "  {'document': 104,\n",
       "   'probabilities': {0: 0.114323385,\n",
       "    1: 0.11432901,\n",
       "    2: 0.2570412,\n",
       "    3: 0.3287705,\n",
       "    4: 0.18553591}},\n",
       "  {'document': 105,\n",
       "   'probabilities': {0: 0.22488426,\n",
       "    1: 0.10004615,\n",
       "    2: 0.100046374,\n",
       "    3: 0.43373358,\n",
       "    4: 0.14128967}},\n",
       "  {'document': 106, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 107,\n",
       "   'probabilities': {0: 0.16002336,\n",
       "    1: 0.16002853,\n",
       "    2: 0.16002686,\n",
       "    3: 0.16001783,\n",
       "    4: 0.35990337}},\n",
       "  {'document': 108,\n",
       "   'probabilities': {0: 0.2247946,\n",
       "    1: 0.47506952,\n",
       "    2: 0.100053206,\n",
       "    3: 0.100035444,\n",
       "    4: 0.10004721}},\n",
       "  {'document': 109,\n",
       "   'probabilities': {0: 0.13336577,\n",
       "    1: 0.29996774,\n",
       "    2: 0.29994005,\n",
       "    3: 0.13335907,\n",
       "    4: 0.1333674}},\n",
       "  {'document': 110,\n",
       "   'probabilities': {0: 0.2587464,\n",
       "    1: 0.43209916,\n",
       "    2: 0.072755374,\n",
       "    3: 0.16364624,\n",
       "    4: 0.072752796}},\n",
       "  {'document': 111,\n",
       "   'probabilities': {0: 0.11430286,\n",
       "    1: 0.114305295,\n",
       "    2: 0.39994338,\n",
       "    3: 0.114299245,\n",
       "    4: 0.25714922}},\n",
       "  {'document': 112,\n",
       "   'probabilities': {0: 0.10772333,\n",
       "    1: 0.31086975,\n",
       "    2: 0.31118956,\n",
       "    3: 0.08892718,\n",
       "    4: 0.18129016}},\n",
       "  {'document': 113,\n",
       "   'probabilities': {0: 0.16003492,\n",
       "    1: 0.16003744,\n",
       "    2: 0.35986653,\n",
       "    3: 0.16002597,\n",
       "    4: 0.1600351}},\n",
       "  {'document': 114,\n",
       "   'probabilities': {0: 0.13337411,\n",
       "    1: 0.13338135,\n",
       "    2: 0.29998162,\n",
       "    3: 0.133365,\n",
       "    4: 0.29989788}},\n",
       "  {'document': 115, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 116,\n",
       "   'probabilities': {0: 0.1600096,\n",
       "    1: 0.16001067,\n",
       "    2: 0.1600109,\n",
       "    3: 0.35995921,\n",
       "    4: 0.16000965}},\n",
       "  {'document': 117,\n",
       "   'probabilities': {0: 0.16002986,\n",
       "    1: 0.16003323,\n",
       "    2: 0.3598842,\n",
       "    3: 0.16002281,\n",
       "    4: 0.1600299}},\n",
       "  {'document': 118,\n",
       "   'probabilities': {0: 0.13336457,\n",
       "    1: 0.2850649,\n",
       "    2: 0.2998776,\n",
       "    3: 0.14832759,\n",
       "    4: 0.13336536}},\n",
       "  {'document': 119,\n",
       "   'probabilities': {0: 0.08116251,\n",
       "    1: 0.07274795,\n",
       "    2: 0.116324455,\n",
       "    3: 0.29334134,\n",
       "    4: 0.43642375}},\n",
       "  {'document': 120,\n",
       "   'probabilities': {0: 0.1143143,\n",
       "    1: 0.114319876,\n",
       "    2: 0.25717887,\n",
       "    3: 0.39987147,\n",
       "    4: 0.1143155}},\n",
       "  {'document': 121,\n",
       "   'probabilities': {0: 0.42234623,\n",
       "    1: 0.08892907,\n",
       "    2: 0.1998599,\n",
       "    3: 0.08891538,\n",
       "    4: 0.19994938}},\n",
       "  {'document': 122,\n",
       "   'probabilities': {0: 0.10003133,\n",
       "    1: 0.22500362,\n",
       "    2: 0.10003656,\n",
       "    3: 0.4748971,\n",
       "    4: 0.1000314}},\n",
       "  {'document': 123,\n",
       "   'probabilities': {0: 0.08892535,\n",
       "    1: 0.19982053,\n",
       "    2: 0.08893023,\n",
       "    3: 0.53339636,\n",
       "    4: 0.08892755}},\n",
       "  {'document': 124,\n",
       "   'probabilities': {0: 0.07274186,\n",
       "    1: 0.07274392,\n",
       "    2: 0.09063403,\n",
       "    3: 0.6790017,\n",
       "    4: 0.08487851}},\n",
       "  {'document': 125,\n",
       "   'probabilities': {0: 0.2635611,\n",
       "    1: 0.08154911,\n",
       "    2: 0.08164943,\n",
       "    3: 0.17275354,\n",
       "    4: 0.40048683}},\n",
       "  {'document': 126,\n",
       "   'probabilities': {0: 0.114298366,\n",
       "    1: 0.11430005,\n",
       "    2: 0.11430052,\n",
       "    3: 0.114297874,\n",
       "    4: 0.54280317}},\n",
       "  {'document': 127,\n",
       "   'probabilities': {0: 0.13343859,\n",
       "    1: 0.13344982,\n",
       "    2: 0.13345167,\n",
       "    3: 0.1334153,\n",
       "    4: 0.46624464}},\n",
       "  {'document': 128, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 129,\n",
       "   'probabilities': {0: 0.11433305,\n",
       "    1: 0.25721142,\n",
       "    2: 0.11434066,\n",
       "    3: 0.2570111,\n",
       "    4: 0.25710383}},\n",
       "  {'document': 130,\n",
       "   'probabilities': {0: 0.14991976,\n",
       "    1: 0.06670907,\n",
       "    2: 0.31672454,\n",
       "    3: 0.15004197,\n",
       "    4: 0.3166046}},\n",
       "  {'document': 131,\n",
       "   'probabilities': {0: 0.15176894,\n",
       "    1: 0.4817061,\n",
       "    2: 0.14990374,\n",
       "    3: 0.1499167,\n",
       "    4: 0.06670447}},\n",
       "  {'document': 132,\n",
       "   'probabilities': {0: 0.39954513,\n",
       "    1: 0.25734463,\n",
       "    2: 0.11438416,\n",
       "    3: 0.11435027,\n",
       "    4: 0.114375815}},\n",
       "  {'document': 133,\n",
       "   'probabilities': {0: 0.2998116,\n",
       "    1: 0.13337603,\n",
       "    2: 0.2807264,\n",
       "    3: 0.15271243,\n",
       "    4: 0.1333735}},\n",
       "  {'document': 134, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 135, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 136,\n",
       "   'probabilities': {0: 0.08002142,\n",
       "    1: 0.080024,\n",
       "    2: 0.38006204,\n",
       "    3: 0.08001648,\n",
       "    4: 0.37987605}},\n",
       "  {'document': 137,\n",
       "   'probabilities': {0: 0.16003239,\n",
       "    1: 0.16003552,\n",
       "    2: 0.359875,\n",
       "    3: 0.16002527,\n",
       "    4: 0.16003183}},\n",
       "  {'document': 138, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 139,\n",
       "   'probabilities': {0: 0.13336895,\n",
       "    1: 0.13337237,\n",
       "    2: 0.2998467,\n",
       "    3: 0.30004346,\n",
       "    4: 0.13336846}},\n",
       "  {'document': 140,\n",
       "   'probabilities': {0: 0.2997399,\n",
       "    1: 0.1333951,\n",
       "    2: 0.1333933,\n",
       "    3: 0.13337426,\n",
       "    4: 0.3000974}},\n",
       "  {'document': 141,\n",
       "   'probabilities': {0: 0.07279017,\n",
       "    1: 0.16367559,\n",
       "    2: 0.16306281,\n",
       "    3: 0.5276791,\n",
       "    4: 0.07279234}},\n",
       "  {'document': 142,\n",
       "   'probabilities': {0: 0.2247305,\n",
       "    1: 0.100056894,\n",
       "    2: 0.2249973,\n",
       "    3: 0.35016578,\n",
       "    4: 0.100049525}},\n",
       "  {'document': 143, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 144,\n",
       "   'probabilities': {0: 0.30001533,\n",
       "    1: 0.13333869,\n",
       "    2: 0.13333894,\n",
       "    3: 0.13333713,\n",
       "    4: 0.2999699}},\n",
       "  {'document': 145,\n",
       "   'probabilities': {0: 0.11432418,\n",
       "    1: 0.3998344,\n",
       "    2: 0.25720358,\n",
       "    3: 0.11431462,\n",
       "    4: 0.11432316}},\n",
       "  {'document': 146,\n",
       "   'probabilities': {0: 0.16000682,\n",
       "    1: 0.16000745,\n",
       "    2: 0.16000788,\n",
       "    3: 0.3599709,\n",
       "    4: 0.16000691}},\n",
       "  {'document': 147,\n",
       "   'probabilities': {0: 0.029640969,\n",
       "    1: 0.03142697,\n",
       "    2: 0.8379549,\n",
       "    3: 0.030940682,\n",
       "    4: 0.070036486}},\n",
       "  {'document': 148,\n",
       "   'probabilities': {0: 0.16003494,\n",
       "    1: 0.16003744,\n",
       "    2: 0.35986656,\n",
       "    3: 0.16002597,\n",
       "    4: 0.16003507}},\n",
       "  {'document': 149,\n",
       "   'probabilities': {0: 0.3598519,\n",
       "    1: 0.16004032,\n",
       "    2: 0.16004038,\n",
       "    3: 0.16002825,\n",
       "    4: 0.16003914}},\n",
       "  {'document': 150,\n",
       "   'probabilities': {0: 0.10005125,\n",
       "    1: 0.22499813,\n",
       "    2: 0.10006002,\n",
       "    3: 0.22495179,\n",
       "    4: 0.3499388}},\n",
       "  {'document': 151,\n",
       "   'probabilities': {0: 0.25449184,\n",
       "    1: 0.2544743,\n",
       "    2: 0.2545753,\n",
       "    3: 0.07276835,\n",
       "    4: 0.16369022}},\n",
       "  {'document': 152,\n",
       "   'probabilities': {0: 0.31101248,\n",
       "    1: 0.4093469,\n",
       "    2: 0.08890953,\n",
       "    3: 0.1018246,\n",
       "    4: 0.0889065}},\n",
       "  {'document': 153,\n",
       "   'probabilities': {0: 0.11431961,\n",
       "    1: 0.11432775,\n",
       "    2: 0.54271924,\n",
       "    3: 0.11431269,\n",
       "    4: 0.11432067}},\n",
       "  {'document': 154, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 155, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 156,\n",
       "   'probabilities': {0: 0.19977961,\n",
       "    1: 0.08897134,\n",
       "    2: 0.0889703,\n",
       "    3: 0.42223728,\n",
       "    4: 0.20004143}},\n",
       "  {'document': 157,\n",
       "   'probabilities': {0: 0.0727604,\n",
       "    1: 0.16349751,\n",
       "    2: 0.07276539,\n",
       "    3: 0.6182143,\n",
       "    4: 0.072762325}},\n",
       "  {'document': 158,\n",
       "   'probabilities': {0: 0.1600057,\n",
       "    1: 0.1600062,\n",
       "    2: 0.16000651,\n",
       "    3: 0.1600043,\n",
       "    4: 0.35997733}},\n",
       "  {'document': 159,\n",
       "   'probabilities': {0: 0.08077654,\n",
       "    1: 0.092293546,\n",
       "    2: 0.12693249,\n",
       "    3: 0.44648805,\n",
       "    4: 0.2535094}},\n",
       "  {'document': 160,\n",
       "   'probabilities': {0: 0.14024426,\n",
       "    1: 0.35563517,\n",
       "    2: 0.057775937,\n",
       "    3: 0.24316874,\n",
       "    4: 0.2031759}},\n",
       "  {'document': 161,\n",
       "   'probabilities': {0: 0.13335659,\n",
       "    1: 0.29989725,\n",
       "    2: 0.13335983,\n",
       "    3: 0.1333515,\n",
       "    4: 0.30003485}},\n",
       "  {'document': 162,\n",
       "   'probabilities': {0: 0.16343434,\n",
       "    1: 0.072756976,\n",
       "    2: 0.072759,\n",
       "    3: 0.6182954,\n",
       "    4: 0.07275431}},\n",
       "  {'document': 163,\n",
       "   'probabilities': {0: 0.1600206,\n",
       "    1: 0.16002522,\n",
       "    2: 0.35991615,\n",
       "    3: 0.16001646,\n",
       "    4: 0.16002157}},\n",
       "  {'document': 164,\n",
       "   'probabilities': {0: 0.08892213,\n",
       "    1: 0.19994874,\n",
       "    2: 0.27813148,\n",
       "    3: 0.34407547,\n",
       "    4: 0.08892213}},\n",
       "  {'document': 165,\n",
       "   'probabilities': {0: 0.11431713,\n",
       "    1: 0.25721142,\n",
       "    2: 0.2569917,\n",
       "    3: 0.25716242,\n",
       "    4: 0.114317335}},\n",
       "  {'document': 166,\n",
       "   'probabilities': {0: 0.114307664,\n",
       "    1: 0.25714377,\n",
       "    2: 0.11430999,\n",
       "    3: 0.25714663,\n",
       "    4: 0.25709197}},\n",
       "  {'document': 167,\n",
       "   'probabilities': {0: 0.116454914,\n",
       "    1: 0.25638494,\n",
       "    2: 0.11459145,\n",
       "    3: 0.11449162,\n",
       "    4: 0.39807707}},\n",
       "  {'document': 168,\n",
       "   'probabilities': {0: 0.1600279,\n",
       "    1: 0.1600309,\n",
       "    2: 0.16003905,\n",
       "    3: 0.1600212,\n",
       "    4: 0.35988095}},\n",
       "  {'document': 169, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 170,\n",
       "   'probabilities': {0: 0.08891554,\n",
       "    1: 0.3111279,\n",
       "    2: 0.088919245,\n",
       "    3: 0.088910185,\n",
       "    4: 0.42212716}},\n",
       "  {'document': 171,\n",
       "   'probabilities': {0: 0.47478935,\n",
       "    1: 0.100048065,\n",
       "    2: 0.21065556,\n",
       "    3: 0.114461996,\n",
       "    4: 0.100045055}},\n",
       "  {'document': 172,\n",
       "   'probabilities': {0: 0.13335638,\n",
       "    1: 0.30003637,\n",
       "    2: 0.2998994,\n",
       "    3: 0.13335098,\n",
       "    4: 0.13335691}},\n",
       "  {'document': 173,\n",
       "   'probabilities': {0: 0.10003394,\n",
       "    1: 0.22493646,\n",
       "    2: 0.10004208,\n",
       "    3: 0.10002666,\n",
       "    4: 0.47496086}},\n",
       "  {'document': 174, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 175,\n",
       "   'probabilities': {0: 0.16000913,\n",
       "    1: 0.16001031,\n",
       "    2: 0.3368709,\n",
       "    3: 0.18310036,\n",
       "    4: 0.1600093}},\n",
       "  {'document': 176,\n",
       "   'probabilities': {0: 0.11429833,\n",
       "    1: 0.11429925,\n",
       "    2: 0.11430008,\n",
       "    3: 0.5428045,\n",
       "    4: 0.11429786}},\n",
       "  {'document': 177,\n",
       "   'probabilities': {0: 0.057163298,\n",
       "    1: 0.057165746,\n",
       "    2: 0.3427896,\n",
       "    3: 0.48571783,\n",
       "    4: 0.057163518}},\n",
       "  {'document': 178,\n",
       "   'probabilities': {0: 0.13334946,\n",
       "    1: 0.13334824,\n",
       "    2: 0.13334976,\n",
       "    3: 0.13334422,\n",
       "    4: 0.46660835}},\n",
       "  {'document': 179,\n",
       "   'probabilities': {0: 0.13343237,\n",
       "    1: 0.29998,\n",
       "    2: 0.13344914,\n",
       "    3: 0.13340929,\n",
       "    4: 0.29972914}},\n",
       "  {'document': 180,\n",
       "   'probabilities': {0: 0.3499672,\n",
       "    1: 0.2248956,\n",
       "    2: 0.100053735,\n",
       "    3: 0.22503677,\n",
       "    4: 0.100046694}},\n",
       "  {'document': 181,\n",
       "   'probabilities': {0: 0.10004218,\n",
       "    1: 0.22495122,\n",
       "    2: 0.100048386,\n",
       "    3: 0.47491586,\n",
       "    4: 0.100042336}},\n",
       "  {'document': 182,\n",
       "   'probabilities': {0: 0.13333854,\n",
       "    1: 0.13333894,\n",
       "    2: 0.13333921,\n",
       "    3: 0.30000997,\n",
       "    4: 0.29997334}},\n",
       "  {'document': 183,\n",
       "   'probabilities': {0: 0.22643831,\n",
       "    1: 0.20000476,\n",
       "    2: 0.042144053,\n",
       "    3: 0.38404837,\n",
       "    4: 0.14736453}},\n",
       "  {'document': 184,\n",
       "   'probabilities': {0: 0.3196736,\n",
       "    1: 0.19135007,\n",
       "    2: 0.08893816,\n",
       "    3: 0.0889214,\n",
       "    4: 0.3111168}},\n",
       "  {'document': 185,\n",
       "   'probabilities': {0: 0.1600434,\n",
       "    1: 0.35982653,\n",
       "    2: 0.1600492,\n",
       "    3: 0.16003433,\n",
       "    4: 0.16004656}},\n",
       "  {'document': 186, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 187,\n",
       "   'probabilities': {0: 0.16002442,\n",
       "    1: 0.3599019,\n",
       "    2: 0.16003014,\n",
       "    3: 0.16001911,\n",
       "    4: 0.16002437}},\n",
       "  {'document': 188,\n",
       "   'probabilities': {0: 0.23801392,\n",
       "    1: 0.44006106,\n",
       "    2: 0.09201822,\n",
       "    3: 0.04001544,\n",
       "    4: 0.18989134}},\n",
       "  {'document': 189,\n",
       "   'probabilities': {0: 0.13341981,\n",
       "    1: 0.30000106,\n",
       "    2: 0.2997571,\n",
       "    3: 0.13339974,\n",
       "    4: 0.13342234}},\n",
       "  {'document': 190, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 191,\n",
       "   'probabilities': {0: 0.47505012,\n",
       "    1: 0.100006886,\n",
       "    2: 0.10000704,\n",
       "    3: 0.22492968,\n",
       "    4: 0.10000626}},\n",
       "  {'document': 192,\n",
       "   'probabilities': {0: 0.11430692,\n",
       "    1: 0.3999361,\n",
       "    2: 0.11431013,\n",
       "    3: 0.25713912,\n",
       "    4: 0.11430771}},\n",
       "  {'document': 193,\n",
       "   'probabilities': {0: 0.533305,\n",
       "    1: 0.08894242,\n",
       "    2: 0.19988944,\n",
       "    3: 0.08892564,\n",
       "    4: 0.0889375}},\n",
       "  {'document': 194,\n",
       "   'probabilities': {0: 0.4665996,\n",
       "    1: 0.1333517,\n",
       "    2: 0.13335264,\n",
       "    3: 0.13334616,\n",
       "    4: 0.13334991}},\n",
       "  {'document': 195,\n",
       "   'probabilities': {0: 0.072749734,\n",
       "    1: 0.2545782,\n",
       "    2: 0.07275366,\n",
       "    3: 0.16362202,\n",
       "    4: 0.4362964}},\n",
       "  {'document': 196,\n",
       "   'probabilities': {0: 0.13334845,\n",
       "    1: 0.13335024,\n",
       "    2: 0.1333503,\n",
       "    3: 0.4666021,\n",
       "    4: 0.13334896}},\n",
       "  {'document': 197,\n",
       "   'probabilities': {0: 0.3000076,\n",
       "    1: 0.29988894,\n",
       "    2: 0.13337262,\n",
       "    3: 0.13336015,\n",
       "    4: 0.1333707}},\n",
       "  {'document': 198,\n",
       "   'probabilities': {0: 0.2383546,\n",
       "    1: 0.114333056,\n",
       "    2: 0.27598366,\n",
       "    3: 0.25700077,\n",
       "    4: 0.11432789}},\n",
       "  {'document': 199, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 200, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 201,\n",
       "   'probabilities': {0: 0.16001123,\n",
       "    1: 0.16001363,\n",
       "    2: 0.16001262,\n",
       "    3: 0.16000898,\n",
       "    4: 0.35995352}},\n",
       "  {'document': 202, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 203,\n",
       "   'probabilities': {0: 0.25703084,\n",
       "    1: 0.11433217,\n",
       "    2: 0.11433223,\n",
       "    3: 0.25721267,\n",
       "    4: 0.25709212}},\n",
       "  {'document': 204,\n",
       "   'probabilities': {0: 0.2976205,\n",
       "    1: 0.2250567,\n",
       "    2: 0.10331039,\n",
       "    3: 0.14918917,\n",
       "    4: 0.22482322}},\n",
       "  {'document': 205,\n",
       "   'probabilities': {0: 0.15787944,\n",
       "    1: 0.11432589,\n",
       "    2: 0.49916196,\n",
       "    3: 0.11431184,\n",
       "    4: 0.114320874}},\n",
       "  {'document': 206,\n",
       "   'probabilities': {0: 0.08893278,\n",
       "    1: 0.42235935,\n",
       "    2: 0.20001358,\n",
       "    3: 0.08892278,\n",
       "    4: 0.19977152}},\n",
       "  {'document': 207,\n",
       "   'probabilities': {0: 0.49179947,\n",
       "    1: 0.28991726,\n",
       "    2: 0.07276624,\n",
       "    3: 0.07275428,\n",
       "    4: 0.07276278}},\n",
       "  {'document': 208,\n",
       "   'probabilities': {0: 0.16003627,\n",
       "    1: 0.35980755,\n",
       "    2: 0.16004111,\n",
       "    3: 0.16003117,\n",
       "    4: 0.16008395}},\n",
       "  {'document': 209,\n",
       "   'probabilities': {0: 0.13337429,\n",
       "    1: 0.299921,\n",
       "    2: 0.13338147,\n",
       "    3: 0.29994687,\n",
       "    4: 0.13337633}},\n",
       "  {'document': 210, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 211,\n",
       "   'probabilities': {0: 0.088931516,\n",
       "    1: 0.19986877,\n",
       "    2: 0.20006874,\n",
       "    3: 0.08892182,\n",
       "    4: 0.4222091}},\n",
       "  {'document': 212,\n",
       "   'probabilities': {0: 0.13335814,\n",
       "    1: 0.13336104,\n",
       "    2: 0.46657005,\n",
       "    3: 0.13335238,\n",
       "    4: 0.13335834}},\n",
       "  {'document': 213,\n",
       "   'probabilities': {0: 0.11432802,\n",
       "    1: 0.3998648,\n",
       "    2: 0.1143345,\n",
       "    3: 0.11431908,\n",
       "    4: 0.25715363}},\n",
       "  {'document': 214,\n",
       "   'probabilities': {0: 0.25349692,\n",
       "    1: 0.3113137,\n",
       "    2: 0.053372916,\n",
       "    3: 0.1952636,\n",
       "    4: 0.18655288}},\n",
       "  {'document': 215,\n",
       "   'probabilities': {0: 0.29062575,\n",
       "    1: 0.11242875,\n",
       "    2: 0.12176063,\n",
       "    3: 0.36283827,\n",
       "    4: 0.11234659}},\n",
       "  {'document': 216,\n",
       "   'probabilities': {0: 0.13336952,\n",
       "    1: 0.29987293,\n",
       "    2: 0.13337536,\n",
       "    3: 0.13336137,\n",
       "    4: 0.30002084}},\n",
       "  {'document': 217,\n",
       "   'probabilities': {0: 0.3625325,\n",
       "    1: 0.2374453,\n",
       "    2: 0.1752386,\n",
       "    3: 0.17474048,\n",
       "    4: 0.05004313}},\n",
       "  {'document': 218,\n",
       "   'probabilities': {0: 0.13336973,\n",
       "    1: 0.13337858,\n",
       "    2: 0.13337523,\n",
       "    3: 0.30004388,\n",
       "    4: 0.29983258}},\n",
       "  {'document': 219, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 220,\n",
       "   'probabilities': {0: 0.16002804,\n",
       "    1: 0.16003212,\n",
       "    2: 0.16003723,\n",
       "    3: 0.35987392,\n",
       "    4: 0.16002871}},\n",
       "  {'document': 221,\n",
       "   'probabilities': {0: 0.31112292,\n",
       "    1: 0.31119677,\n",
       "    2: 0.088951975,\n",
       "    3: 0.19978267,\n",
       "    4: 0.08894568}},\n",
       "  {'document': 222,\n",
       "   'probabilities': {0: 0.35999906,\n",
       "    1: 0.16000023,\n",
       "    2: 0.16000023,\n",
       "    3: 0.16000023,\n",
       "    4: 0.16000023}},\n",
       "  {'document': 223,\n",
       "   'probabilities': {0: 0.05717651,\n",
       "    1: 0.057179064,\n",
       "    2: 0.057179406,\n",
       "    3: 0.7001446,\n",
       "    4: 0.12832043}},\n",
       "  {'document': 224,\n",
       "   'probabilities': {0: 0.4665842,\n",
       "    1: 0.13335602,\n",
       "    2: 0.13335676,\n",
       "    3: 0.13334885,\n",
       "    4: 0.13335417}},\n",
       "  {'document': 225,\n",
       "   'probabilities': {0: 0.16001451,\n",
       "    1: 0.16001636,\n",
       "    2: 0.1600157,\n",
       "    3: 0.1600119,\n",
       "    4: 0.35994157}},\n",
       "  {'document': 226,\n",
       "   'probabilities': {0: 0.19971152,\n",
       "    1: 0.19990107,\n",
       "    2: 0.18730725,\n",
       "    3: 0.1017786,\n",
       "    4: 0.31130156}},\n",
       "  {'document': 227,\n",
       "   'probabilities': {0: 0.35999906,\n",
       "    1: 0.16000025,\n",
       "    2: 0.16000025,\n",
       "    3: 0.16000025,\n",
       "    4: 0.16000025}},\n",
       "  {'document': 228,\n",
       "   'probabilities': {0: 0.088927574,\n",
       "    1: 0.20005186,\n",
       "    2: 0.42226824,\n",
       "    3: 0.088919,\n",
       "    4: 0.19983333}},\n",
       "  {'document': 229,\n",
       "   'probabilities': {0: 0.114340864,\n",
       "    1: 0.25730202,\n",
       "    2: 0.25711566,\n",
       "    3: 0.25690114,\n",
       "    4: 0.11434035}},\n",
       "  {'document': 230,\n",
       "   'probabilities': {0: 0.16675945,\n",
       "    1: 0.08002981,\n",
       "    2: 0.1931994,\n",
       "    3: 0.47998443,\n",
       "    4: 0.08002688}},\n",
       "  {'document': 231,\n",
       "   'probabilities': {0: 0.061557095,\n",
       "    1: 0.07783429,\n",
       "    2: 0.061560474,\n",
       "    3: 0.66067773,\n",
       "    4: 0.13837044}},\n",
       "  {'document': 232,\n",
       "   'probabilities': {0: 0.2796993,\n",
       "    1: 0.28002706,\n",
       "    2: 0.18000776,\n",
       "    3: 0.080056585,\n",
       "    4: 0.1802093}},\n",
       "  {'document': 233,\n",
       "   'probabilities': {0: 0.35999906,\n",
       "    1: 0.16000025,\n",
       "    2: 0.16000025,\n",
       "    3: 0.16000025,\n",
       "    4: 0.16000025}},\n",
       "  {'document': 234, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 235,\n",
       "   'probabilities': {0: 0.2569084,\n",
       "    1: 0.114352554,\n",
       "    2: 0.2573722,\n",
       "    3: 0.25702393,\n",
       "    4: 0.114342935}},\n",
       "  {'document': 236,\n",
       "   'probabilities': {0: 0.42226765,\n",
       "    1: 0.08892609,\n",
       "    2: 0.08892709,\n",
       "    3: 0.3109574,\n",
       "    4: 0.088921785}},\n",
       "  {'document': 237,\n",
       "   'probabilities': {0: 0.063807525,\n",
       "    1: 0.12962484,\n",
       "    2: 0.41320783,\n",
       "    3: 0.057157766,\n",
       "    4: 0.33620203}},\n",
       "  {'document': 238,\n",
       "   'probabilities': {0: 0.16002788,\n",
       "    1: 0.16003087,\n",
       "    2: 0.16003904,\n",
       "    3: 0.16002119,\n",
       "    4: 0.35988104}},\n",
       "  {'document': 239,\n",
       "   'probabilities': {0: 0.08003332,\n",
       "    1: 0.37992766,\n",
       "    2: 0.08003784,\n",
       "    3: 0.09389432,\n",
       "    4: 0.36610684}},\n",
       "  {'document': 240,\n",
       "   'probabilities': {0: 0.088910446,\n",
       "    1: 0.08891286,\n",
       "    2: 0.08891271,\n",
       "    3: 0.6443538,\n",
       "    4: 0.088910215}},\n",
       "  {'document': 241,\n",
       "   'probabilities': {0: 0.22468619,\n",
       "    1: 0.10102561,\n",
       "    2: 0.34939614,\n",
       "    3: 0.22484167,\n",
       "    4: 0.10005037}},\n",
       "  {'document': 242,\n",
       "   'probabilities': {0: 0.1333634,\n",
       "    1: 0.46654624,\n",
       "    2: 0.13336784,\n",
       "    3: 0.13335848,\n",
       "    4: 0.13336408}},\n",
       "  {'document': 243, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 244,\n",
       "   'probabilities': {0: 0.46651796,\n",
       "    1: 0.13337448,\n",
       "    2: 0.13337578,\n",
       "    3: 0.13336149,\n",
       "    4: 0.13337028}},\n",
       "  {'document': 245,\n",
       "   'probabilities': {0: 0.16000584,\n",
       "    1: 0.35997707,\n",
       "    2: 0.16000658,\n",
       "    3: 0.1600046,\n",
       "    4: 0.16000587}},\n",
       "  {'document': 246,\n",
       "   'probabilities': {0: 0.1334125,\n",
       "    1: 0.30008468,\n",
       "    2: 0.13341247,\n",
       "    3: 0.13338386,\n",
       "    4: 0.29970655}},\n",
       "  {'document': 247, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 248, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 249,\n",
       "   'probabilities': {0: 0.1600225,\n",
       "    1: 0.16002643,\n",
       "    2: 0.3599099,\n",
       "    3: 0.16001804,\n",
       "    4: 0.16002311}},\n",
       "  {'document': 250,\n",
       "   'probabilities': {0: 0.13782759,\n",
       "    1: 0.13838638,\n",
       "    2: 0.061614513,\n",
       "    3: 0.5236949,\n",
       "    4: 0.13847665}},\n",
       "  {'document': 251,\n",
       "   'probabilities': {0: 0.19460179,\n",
       "    1: 0.08003798,\n",
       "    2: 0.0800379,\n",
       "    3: 0.56528634,\n",
       "    4: 0.08003596}},\n",
       "  {'document': 252,\n",
       "   'probabilities': {0: 0.32100493,\n",
       "    1: 0.19988504,\n",
       "    2: 0.35153103,\n",
       "    3: 0.070402,\n",
       "    4: 0.05717697}},\n",
       "  {'document': 253,\n",
       "   'probabilities': {0: 0.10004372,\n",
       "    1: 0.35005707,\n",
       "    2: 0.10004947,\n",
       "    3: 0.1000337,\n",
       "    4: 0.34981605}},\n",
       "  {'document': 254, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 255,\n",
       "   'probabilities': {0: 0.08892063,\n",
       "    1: 0.42233604,\n",
       "    2: 0.20003854,\n",
       "    3: 0.19978349,\n",
       "    4: 0.08892127}},\n",
       "  {'document': 256,\n",
       "   'probabilities': {0: 0.29982024,\n",
       "    1: 0.13337821,\n",
       "    2: 0.13337806,\n",
       "    3: 0.30004957,\n",
       "    4: 0.1333739}},\n",
       "  {'document': 257,\n",
       "   'probabilities': {0: 0.1333946,\n",
       "    1: 0.29978612,\n",
       "    2: 0.30005533,\n",
       "    3: 0.13337442,\n",
       "    4: 0.13338953}},\n",
       "  {'document': 258,\n",
       "   'probabilities': {0: 0.47853392,\n",
       "    1: 0.062324993,\n",
       "    2: 0.13055868,\n",
       "    3: 0.0571627,\n",
       "    4: 0.27141967}},\n",
       "  {'document': 259,\n",
       "   'probabilities': {0: 0.25161234,\n",
       "    1: 0.11430941,\n",
       "    2: 0.11987305,\n",
       "    3: 0.39989826,\n",
       "    4: 0.11430691}},\n",
       "  {'document': 260,\n",
       "   'probabilities': {0: 0.16002701,\n",
       "    1: 0.16002378,\n",
       "    2: 0.1600271,\n",
       "    3: 0.16001771,\n",
       "    4: 0.35990438}},\n",
       "  {'document': 261,\n",
       "   'probabilities': {0: 0.050028432,\n",
       "    1: 0.29840592,\n",
       "    2: 0.11233562,\n",
       "    3: 0.05154205,\n",
       "    4: 0.487688}},\n",
       "  {'document': 262,\n",
       "   'probabilities': {0: 0.2569437,\n",
       "    1: 0.11433234,\n",
       "    2: 0.11433417,\n",
       "    3: 0.114318214,\n",
       "    4: 0.40007156}},\n",
       "  {'document': 263, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 264,\n",
       "   'probabilities': {0: 0.25451022,\n",
       "    1: 0.34554663,\n",
       "    2: 0.25440264,\n",
       "    3: 0.07276395,\n",
       "    4: 0.07277654}},\n",
       "  {'document': 265, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 266,\n",
       "   'probabilities': {0: 0.3453643,\n",
       "    1: 0.21113202,\n",
       "    2: 0.16544539,\n",
       "    3: 0.15556876,\n",
       "    4: 0.12248957}},\n",
       "  {'document': 267,\n",
       "   'probabilities': {0: 0.114325576,\n",
       "    1: 0.25687796,\n",
       "    2: 0.40015075,\n",
       "    3: 0.11431778,\n",
       "    4: 0.11432796}},\n",
       "  {'document': 268,\n",
       "   'probabilities': {0: 0.16002625,\n",
       "    1: 0.16002949,\n",
       "    2: 0.35989618,\n",
       "    3: 0.16002074,\n",
       "    4: 0.16002731}},\n",
       "  {'document': 269,\n",
       "   'probabilities': {0: 0.114301205,\n",
       "    1: 0.11430358,\n",
       "    2: 0.25720313,\n",
       "    3: 0.11429802,\n",
       "    4: 0.39989406}},\n",
       "  {'document': 270,\n",
       "   'probabilities': {0: 0.256998,\n",
       "    1: 0.11438457,\n",
       "    2: 0.11438677,\n",
       "    3: 0.39985275,\n",
       "    4: 0.114377946}},\n",
       "  {'document': 271,\n",
       "   'probabilities': {0: 0.08891799,\n",
       "    1: 0.19981119,\n",
       "    2: 0.088923834,\n",
       "    3: 0.08891185,\n",
       "    4: 0.53343517}},\n",
       "  {'document': 272,\n",
       "   'probabilities': {0: 0.11434357,\n",
       "    1: 0.21662322,\n",
       "    2: 0.15464666,\n",
       "    3: 0.25724232,\n",
       "    4: 0.25714424}},\n",
       "  {'document': 273, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 274,\n",
       "   'probabilities': {0: 0.11432047,\n",
       "    1: 0.11432499,\n",
       "    2: 0.1143267,\n",
       "    3: 0.39995113,\n",
       "    4: 0.2570767}},\n",
       "  {'document': 275,\n",
       "   'probabilities': {0: 0.10003159,\n",
       "    1: 0.3498738,\n",
       "    2: 0.10003653,\n",
       "    3: 0.10002549,\n",
       "    4: 0.3500326}},\n",
       "  {'document': 276,\n",
       "   'probabilities': {0: 0.16266148,\n",
       "    1: 0.35733724,\n",
       "    2: 0.16000043,\n",
       "    3: 0.16000043,\n",
       "    4: 0.16000043}},\n",
       "  {'document': 277,\n",
       "   'probabilities': {0: 0.31120518,\n",
       "    1: 0.0889067,\n",
       "    2: 0.31098753,\n",
       "    3: 0.088901445,\n",
       "    4: 0.1999992}},\n",
       "  {'document': 278,\n",
       "   'probabilities': {0: 0.13338412,\n",
       "    1: 0.2999406,\n",
       "    2: 0.13339281,\n",
       "    3: 0.2998978,\n",
       "    4: 0.1333847}},\n",
       "  {'document': 279,\n",
       "   'probabilities': {0: 0.23312739,\n",
       "    1: 0.2335305,\n",
       "    2: 0.14991678,\n",
       "    3: 0.15001786,\n",
       "    4: 0.23340744}},\n",
       "  {'document': 280,\n",
       "   'probabilities': {0: 0.47496912,\n",
       "    1: 0.100050256,\n",
       "    2: 0.2249025,\n",
       "    3: 0.10003395,\n",
       "    4: 0.100044206}},\n",
       "  {'document': 281,\n",
       "   'probabilities': {0: 0.31114626,\n",
       "    1: 0.088977784,\n",
       "    2: 0.10120892,\n",
       "    3: 0.19987659,\n",
       "    4: 0.2987904}},\n",
       "  {'document': 282,\n",
       "   'probabilities': {0: 0.10002819,\n",
       "    1: 0.10003177,\n",
       "    2: 0.10003278,\n",
       "    3: 0.47490886,\n",
       "    4: 0.22499844}},\n",
       "  {'document': 283,\n",
       "   'probabilities': {0: 0.16001192,\n",
       "    1: 0.16001356,\n",
       "    2: 0.16001374,\n",
       "    3: 0.16000958,\n",
       "    4: 0.35995123}},\n",
       "  {'document': 284,\n",
       "   'probabilities': {0: 0.11430871,\n",
       "    1: 0.25710076,\n",
       "    2: 0.3999734,\n",
       "    3: 0.114304304,\n",
       "    4: 0.11431282}},\n",
       "  {'document': 285,\n",
       "   'probabilities': {0: 0.16000026,\n",
       "    1: 0.1613418,\n",
       "    2: 0.35865742,\n",
       "    3: 0.16000026,\n",
       "    4: 0.16000026}},\n",
       "  {'document': 286,\n",
       "   'probabilities': {0: 0.06157961,\n",
       "    1: 0.29232034,\n",
       "    2: 0.13841341,\n",
       "    3: 0.21545647,\n",
       "    4: 0.2922302}},\n",
       "  {'document': 287,\n",
       "   'probabilities': {0: 0.2567061,\n",
       "    1: 0.120378196,\n",
       "    2: 0.39413145,\n",
       "    3: 0.11437738,\n",
       "    4: 0.11440688}},\n",
       "  {'document': 288,\n",
       "   'probabilities': {0: 0.10003399,\n",
       "    1: 0.22487356,\n",
       "    2: 0.32641667,\n",
       "    3: 0.24864127,\n",
       "    4: 0.100034505}},\n",
       "  {'document': 289,\n",
       "   'probabilities': {0: 0.08002179,\n",
       "    1: 0.08040823,\n",
       "    2: 0.27959195,\n",
       "    3: 0.38007003,\n",
       "    4: 0.17990804}},\n",
       "  {'document': 290,\n",
       "   'probabilities': {0: 0.13334408,\n",
       "    1: 0.13334638,\n",
       "    2: 0.13334543,\n",
       "    3: 0.13334194,\n",
       "    4: 0.46662217}},\n",
       "  {'document': 291,\n",
       "   'probabilities': {0: 0.22807418,\n",
       "    1: 0.3468303,\n",
       "    2: 0.100032985,\n",
       "    3: 0.22503264,\n",
       "    4: 0.10002991}},\n",
       "  {'document': 292,\n",
       "   'probabilities': {0: 0.45691743,\n",
       "    1: 0.14162152,\n",
       "    2: 0.11390495,\n",
       "    3: 0.2375218,\n",
       "    4: 0.050034314}},\n",
       "  {'document': 293,\n",
       "   'probabilities': {0: 0.16002178,\n",
       "    1: 0.16002397,\n",
       "    2: 0.35991532,\n",
       "    3: 0.16001655,\n",
       "    4: 0.1600224}},\n",
       "  {'document': 294,\n",
       "   'probabilities': {0: 0.08005845,\n",
       "    1: 0.28011122,\n",
       "    2: 0.18000482,\n",
       "    3: 0.17985605,\n",
       "    4: 0.27996942}},\n",
       "  {'document': 295,\n",
       "   'probabilities': {0: 0.29993424,\n",
       "    1: 0.13342482,\n",
       "    2: 0.13341624,\n",
       "    3: 0.13339211,\n",
       "    4: 0.2998326}},\n",
       "  {'document': 296, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 297,\n",
       "   'probabilities': {0: 0.13339494,\n",
       "    1: 0.2999237,\n",
       "    2: 0.13340418,\n",
       "    3: 0.13337982,\n",
       "    4: 0.29989737}},\n",
       "  {'document': 298,\n",
       "   'probabilities': {0: 0.16001365,\n",
       "    1: 0.3599459,\n",
       "    2: 0.16001569,\n",
       "    3: 0.16001062,\n",
       "    4: 0.16001418}},\n",
       "  {'document': 299,\n",
       "   'probabilities': {0: 0.160035,\n",
       "    1: 0.16003777,\n",
       "    2: 0.16004494,\n",
       "    3: 0.35984644,\n",
       "    4: 0.16003586}},\n",
       "  {'document': 300,\n",
       "   'probabilities': {0: 0.16001801,\n",
       "    1: 0.16002013,\n",
       "    2: 0.16002145,\n",
       "    3: 0.16001365,\n",
       "    4: 0.3599268}},\n",
       "  {'document': 301,\n",
       "   'probabilities': {0: 0.16002944,\n",
       "    1: 0.35988164,\n",
       "    2: 0.1600343,\n",
       "    3: 0.16002367,\n",
       "    4: 0.1600309}},\n",
       "  {'document': 302,\n",
       "   'probabilities': {0: 0.23334852,\n",
       "    1: 0.15004183,\n",
       "    2: 0.14981306,\n",
       "    3: 0.16531128,\n",
       "    4: 0.30148533}},\n",
       "  {'document': 303,\n",
       "   'probabilities': {0: 0.35052317,\n",
       "    1: 0.100103304,\n",
       "    2: 0.22413656,\n",
       "    3: 0.10007191,\n",
       "    4: 0.22516507}},\n",
       "  {'document': 304,\n",
       "   'probabilities': {0: 0.1600119,\n",
       "    1: 0.16001353,\n",
       "    2: 0.16001381,\n",
       "    3: 0.16000952,\n",
       "    4: 0.3599512}},\n",
       "  {'document': 305,\n",
       "   'probabilities': {0: 0.10003834,\n",
       "    1: 0.22509903,\n",
       "    2: 0.2248738,\n",
       "    3: 0.3499499,\n",
       "    4: 0.10003893}},\n",
       "  {'document': 306,\n",
       "   'probabilities': {0: 0.16000584,\n",
       "    1: 0.35997707,\n",
       "    2: 0.16000658,\n",
       "    3: 0.1600046,\n",
       "    4: 0.16000587}},\n",
       "  {'document': 307,\n",
       "   'probabilities': {0: 0.16000958,\n",
       "    1: 0.16001064,\n",
       "    2: 0.16001087,\n",
       "    3: 0.35995921,\n",
       "    4: 0.16000964}},\n",
       "  {'document': 308,\n",
       "   'probabilities': {0: 0.114320695,\n",
       "    1: 0.11432529,\n",
       "    2: 0.11432564,\n",
       "    3: 0.25692979,\n",
       "    4: 0.4000986}},\n",
       "  {'document': 309,\n",
       "   'probabilities': {0: 0.16003287,\n",
       "    1: 0.35986924,\n",
       "    2: 0.16003707,\n",
       "    3: 0.16002591,\n",
       "    4: 0.16003491}},\n",
       "  {'document': 310,\n",
       "   'probabilities': {0: 0.19948065,\n",
       "    1: 0.31103972,\n",
       "    2: 0.3064659,\n",
       "    3: 0.09405812,\n",
       "    4: 0.088955626}},\n",
       "  {'document': 311,\n",
       "   'probabilities': {0: 0.16341922,\n",
       "    1: 0.07276016,\n",
       "    2: 0.6183149,\n",
       "    3: 0.072749466,\n",
       "    4: 0.0727563}},\n",
       "  {'document': 312,\n",
       "   'probabilities': {0: 0.64865047,\n",
       "    1: 0.05675397,\n",
       "    2: 0.054797433,\n",
       "    3: 0.18644883,\n",
       "    4: 0.053349275}},\n",
       "  {'document': 313, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 314,\n",
       "   'probabilities': {0: 0.2998698,\n",
       "    1: 0.13336621,\n",
       "    2: 0.1333661,\n",
       "    3: 0.13335495,\n",
       "    4: 0.30004293}},\n",
       "  {'document': 315,\n",
       "   'probabilities': {0: 0.16001157,\n",
       "    1: 0.16001315,\n",
       "    2: 0.16001344,\n",
       "    3: 0.16000907,\n",
       "    4: 0.35995275}},\n",
       "  {'document': 316,\n",
       "   'probabilities': {0: 0.3598914,\n",
       "    1: 0.16003123,\n",
       "    2: 0.1600308,\n",
       "    3: 0.1600198,\n",
       "    4: 0.16002677}},\n",
       "  {'document': 317,\n",
       "   'probabilities': {0: 0.3421856,\n",
       "    1: 0.23700652,\n",
       "    2: 0.13208906,\n",
       "    3: 0.21595238,\n",
       "    4: 0.07276645}},\n",
       "  {'document': 318,\n",
       "   'probabilities': {0: 0.35999906,\n",
       "    1: 0.16000023,\n",
       "    2: 0.16000023,\n",
       "    3: 0.16000023,\n",
       "    4: 0.16000023}},\n",
       "  {'document': 319,\n",
       "   'probabilities': {0: 0.29986754,\n",
       "    1: 0.1333688,\n",
       "    2: 0.3000428,\n",
       "    3: 0.13335694,\n",
       "    4: 0.13336386}},\n",
       "  {'document': 320,\n",
       "   'probabilities': {0: 0.35985488,\n",
       "    1: 0.16004089,\n",
       "    2: 0.16004089,\n",
       "    3: 0.16002792,\n",
       "    4: 0.16003543}},\n",
       "  {'document': 321,\n",
       "   'probabilities': {0: 0.321995,\n",
       "    1: 0.024262555,\n",
       "    2: 0.20607595,\n",
       "    3: 0.22326326,\n",
       "    4: 0.22440319}},\n",
       "  {'document': 322,\n",
       "   'probabilities': {0: 0.59988344,\n",
       "    1: 0.061561517,\n",
       "    2: 0.061561476,\n",
       "    3: 0.061554167,\n",
       "    4: 0.21543942}},\n",
       "  {'document': 323,\n",
       "   'probabilities': {0: 0.2778848,\n",
       "    1: 0.13335784,\n",
       "    2: 0.32205117,\n",
       "    3: 0.13335031,\n",
       "    4: 0.13335583}},\n",
       "  {'document': 324,\n",
       "   'probabilities': {0: 0.2544681,\n",
       "    1: 0.07278248,\n",
       "    2: 0.25466183,\n",
       "    3: 0.16346726,\n",
       "    4: 0.25462028}},\n",
       "  {'document': 325,\n",
       "   'probabilities': {0: 0.5482727,\n",
       "    1: 0.04213661,\n",
       "    2: 0.042654693,\n",
       "    3: 0.21982141,\n",
       "    4: 0.1471146}},\n",
       "  {'document': 326, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 327,\n",
       "   'probabilities': {0: 0.64408237,\n",
       "    1: 0.08898951,\n",
       "    2: 0.088990614,\n",
       "    3: 0.088958,\n",
       "    4: 0.08897956}},\n",
       "  {'document': 328,\n",
       "   'probabilities': {0: 0.17979723,\n",
       "    1: 0.080059275,\n",
       "    2: 0.17988262,\n",
       "    3: 0.38018727,\n",
       "    4: 0.18007357}},\n",
       "  {'document': 329,\n",
       "   'probabilities': {0: 0.114307046,\n",
       "    1: 0.11431027,\n",
       "    2: 0.25702155,\n",
       "    3: 0.40004754,\n",
       "    4: 0.1143136}},\n",
       "  {'document': 330,\n",
       "   'probabilities': {0: 0.3598518,\n",
       "    1: 0.16004033,\n",
       "    2: 0.1600404,\n",
       "    3: 0.16002828,\n",
       "    4: 0.16003917}},\n",
       "  {'document': 331,\n",
       "   'probabilities': {0: 0.10002833,\n",
       "    1: 0.100032605,\n",
       "    2: 0.10003272,\n",
       "    3: 0.5998791,\n",
       "    4: 0.10002728}},\n",
       "  {'document': 332,\n",
       "   'probabilities': {0: 0.3599446,\n",
       "    1: 0.1600157,\n",
       "    2: 0.1600156,\n",
       "    3: 0.16001049,\n",
       "    4: 0.16001356}},\n",
       "  {'document': 333,\n",
       "   'probabilities': {0: 0.1799349,\n",
       "    1: 0.080095254,\n",
       "    2: 0.2797338,\n",
       "    3: 0.20639677,\n",
       "    4: 0.25383925}},\n",
       "  {'document': 334,\n",
       "   'probabilities': {0: 0.1513284,\n",
       "    1: 0.094513245,\n",
       "    2: 0.04448916,\n",
       "    3: 0.1470365,\n",
       "    4: 0.56263274}},\n",
       "  {'document': 335,\n",
       "   'probabilities': {0: 0.2997443,\n",
       "    1: 0.13338411,\n",
       "    2: 0.13339013,\n",
       "    3: 0.13336977,\n",
       "    4: 0.30011168}},\n",
       "  {'document': 336, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 337,\n",
       "   'probabilities': {0: 0.133343,\n",
       "    1: 0.13334407,\n",
       "    2: 0.13334414,\n",
       "    3: 0.46662596,\n",
       "    4: 0.13334285}},\n",
       "  {'document': 338,\n",
       "   'probabilities': {0: 0.16000958,\n",
       "    1: 0.16001064,\n",
       "    2: 0.16001087,\n",
       "    3: 0.35995921,\n",
       "    4: 0.16000964}},\n",
       "  {'document': 339,\n",
       "   'probabilities': {0: 0.16002981,\n",
       "    1: 0.16003212,\n",
       "    2: 0.16003343,\n",
       "    3: 0.35987604,\n",
       "    4: 0.16002862}},\n",
       "  {'document': 340,\n",
       "   'probabilities': {0: 0.25704432,\n",
       "    1: 0.11434534,\n",
       "    2: 0.114346124,\n",
       "    3: 0.25704047,\n",
       "    4: 0.25722373}},\n",
       "  {'document': 341,\n",
       "   'probabilities': {0: 0.47886917,\n",
       "    1: 0.0800259,\n",
       "    2: 0.081163816,\n",
       "    3: 0.080018036,\n",
       "    4: 0.2799231}},\n",
       "  {'document': 342,\n",
       "   'probabilities': {0: 0.11430028,\n",
       "    1: 0.25710988,\n",
       "    2: 0.11430205,\n",
       "    3: 0.11429674,\n",
       "    4: 0.39999107}},\n",
       "  {'document': 343,\n",
       "   'probabilities': {0: 0.1497692,\n",
       "    1: 0.06669486,\n",
       "    2: 0.15009768,\n",
       "    3: 0.48340288,\n",
       "    4: 0.15003538}},\n",
       "  {'document': 344,\n",
       "   'probabilities': {0: 0.16003424,\n",
       "    1: 0.16004242,\n",
       "    2: 0.16003834,\n",
       "    3: 0.16002613,\n",
       "    4: 0.3598588}},\n",
       "  {'document': 345,\n",
       "   'probabilities': {0: 0.29991934,\n",
       "    1: 0.13338464,\n",
       "    2: 0.13338435,\n",
       "    3: 0.29993337,\n",
       "    4: 0.13337831}},\n",
       "  {'document': 346,\n",
       "   'probabilities': {0: 0.46653017,\n",
       "    1: 0.1333717,\n",
       "    2: 0.13337202,\n",
       "    3: 0.13335891,\n",
       "    4: 0.13336723}},\n",
       "  {'document': 347,\n",
       "   'probabilities': {0: 0.11431255,\n",
       "    1: 0.114316456,\n",
       "    2: 0.40007758,\n",
       "    3: 0.114307605,\n",
       "    4: 0.25698584}},\n",
       "  {'document': 348,\n",
       "   'probabilities': {0: 0.07275631,\n",
       "    1: 0.07276141,\n",
       "    2: 0.07275946,\n",
       "    3: 0.4364434,\n",
       "    4: 0.3452794}},\n",
       "  {'document': 349,\n",
       "   'probabilities': {0: 0.17486072,\n",
       "    1: 0.050049014,\n",
       "    2: 0.117534496,\n",
       "    3: 0.54537255,\n",
       "    4: 0.11218323}},\n",
       "  {'document': 350,\n",
       "   'probabilities': {0: 0.26848313,\n",
       "    1: 0.16321193,\n",
       "    2: 0.15762042,\n",
       "    3: 0.07094662,\n",
       "    4: 0.33973786}},\n",
       "  {'document': 351,\n",
       "   'probabilities': {0: 0.31102857,\n",
       "    1: 0.19993404,\n",
       "    2: 0.08895456,\n",
       "    3: 0.19993073,\n",
       "    4: 0.20015207}},\n",
       "  {'document': 352, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 353, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 354,\n",
       "   'probabilities': {0: 0.16001259,\n",
       "    1: 0.1600143,\n",
       "    2: 0.1600141,\n",
       "    3: 0.16000944,\n",
       "    4: 0.35994962}},\n",
       "  {'document': 355,\n",
       "   'probabilities': {0: 0.0889639,\n",
       "    1: 0.19997829,\n",
       "    2: 0.08899102,\n",
       "    3: 0.19931448,\n",
       "    4: 0.42275232}},\n",
       "  {'document': 356,\n",
       "   'probabilities': {0: 0.0889475,\n",
       "    1: 0.1998773,\n",
       "    2: 0.16023731,\n",
       "    3: 0.23971157,\n",
       "    4: 0.31122628}},\n",
       "  {'document': 357,\n",
       "   'probabilities': {0: 0.35985085,\n",
       "    1: 0.16004312,\n",
       "    2: 0.16004224,\n",
       "    3: 0.16002832,\n",
       "    4: 0.16003545}},\n",
       "  {'document': 358,\n",
       "   'probabilities': {0: 0.08892394,\n",
       "    1: 0.08892855,\n",
       "    2: 0.39899263,\n",
       "    3: 0.3342304,\n",
       "    4: 0.088924445}},\n",
       "  {'document': 359,\n",
       "   'probabilities': {0: 0.12834902,\n",
       "    1: 0.556373,\n",
       "    2: 0.19992316,\n",
       "    3: 0.058176834,\n",
       "    4: 0.057177942}},\n",
       "  {'document': 360,\n",
       "   'probabilities': {0: 0.22489408,\n",
       "    1: 0.35020986,\n",
       "    2: 0.1000712,\n",
       "    3: 0.22476147,\n",
       "    4: 0.100063406}},\n",
       "  {'document': 361,\n",
       "   'probabilities': {0: 0.16002573,\n",
       "    1: 0.16003028,\n",
       "    2: 0.35989767,\n",
       "    3: 0.16002,\n",
       "    4: 0.1600263}},\n",
       "  {'document': 362,\n",
       "   'probabilities': {0: 0.11430663,\n",
       "    1: 0.11430885,\n",
       "    2: 0.11430897,\n",
       "    3: 0.114301614,\n",
       "    4: 0.5427739}},\n",
       "  {'document': 363,\n",
       "   'probabilities': {0: 0.6175732,\n",
       "    1: 0.066701055,\n",
       "    2: 0.06670178,\n",
       "    3: 0.14978173,\n",
       "    4: 0.099242225}},\n",
       "  {'document': 364,\n",
       "   'probabilities': {0: 0.11434254,\n",
       "    1: 0.25705102,\n",
       "    2: 0.11435207,\n",
       "    3: 0.32868138,\n",
       "    4: 0.18557294}},\n",
       "  {'document': 365,\n",
       "   'probabilities': {0: 0.1600318,\n",
       "    1: 0.3598731,\n",
       "    2: 0.16003811,\n",
       "    3: 0.16002469,\n",
       "    4: 0.16003235}},\n",
       "  {'document': 366,\n",
       "   'probabilities': {0: 0.13460502,\n",
       "    1: 0.4653468,\n",
       "    2: 0.13335194,\n",
       "    3: 0.13334619,\n",
       "    4: 0.13335004}},\n",
       "  {'document': 367,\n",
       "   'probabilities': {0: 0.22491436,\n",
       "    1: 0.22486588,\n",
       "    2: 0.22499195,\n",
       "    3: 0.100052595,\n",
       "    4: 0.22517522}},\n",
       "  {'document': 368,\n",
       "   'probabilities': {0: 0.14877617,\n",
       "    1: 0.15536329,\n",
       "    2: 0.16223975,\n",
       "    3: 0.43356684,\n",
       "    4: 0.10005392}},\n",
       "  {'document': 369,\n",
       "   'probabilities': {0: 0.25938705,\n",
       "    1: 0.11198682,\n",
       "    2: 0.24479422,\n",
       "    3: 0.19203751,\n",
       "    4: 0.19179441}},\n",
       "  {'document': 370,\n",
       "   'probabilities': {0: 0.29994395,\n",
       "    1: 0.13339965,\n",
       "    2: 0.1334011,\n",
       "    3: 0.13338132,\n",
       "    4: 0.299874}},\n",
       "  {'document': 371,\n",
       "   'probabilities': {0: 0.19958594,\n",
       "    1: 0.20021258,\n",
       "    2: 0.0889677,\n",
       "    3: 0.31109846,\n",
       "    4: 0.20013534}},\n",
       "  {'document': 372, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 373,\n",
       "   'probabilities': {0: 0.1600506,\n",
       "    1: 0.3033326,\n",
       "    2: 0.21652222,\n",
       "    3: 0.16004199,\n",
       "    4: 0.16005257}},\n",
       "  {'document': 374,\n",
       "   'probabilities': {0: 0.1143539,\n",
       "    1: 0.2571498,\n",
       "    2: 0.1282966,\n",
       "    3: 0.3858433,\n",
       "    4: 0.11435644}},\n",
       "  {'document': 375,\n",
       "   'probabilities': {0: 0.35999906,\n",
       "    1: 0.16000025,\n",
       "    2: 0.16000025,\n",
       "    3: 0.16000025,\n",
       "    4: 0.16000025}},\n",
       "  {'document': 376,\n",
       "   'probabilities': {0: 0.16000527,\n",
       "    1: 0.1600059,\n",
       "    2: 0.16000599,\n",
       "    3: 0.35997763,\n",
       "    4: 0.16000524}},\n",
       "  {'document': 377,\n",
       "   'probabilities': {0: 0.31106254,\n",
       "    1: 0.18675189,\n",
       "    2: 0.18373683,\n",
       "    3: 0.118663274,\n",
       "    4: 0.1997855}},\n",
       "  {'document': 378, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 379, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 380,\n",
       "   'probabilities': {0: 0.1143537,\n",
       "    1: 0.5425862,\n",
       "    2: 0.11436491,\n",
       "    3: 0.11433965,\n",
       "    4: 0.11435557}},\n",
       "  {'document': 381, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 382,\n",
       "   'probabilities': {0: 0.1747595,\n",
       "    1: 0.61294466,\n",
       "    2: 0.112233534,\n",
       "    3: 0.05002687,\n",
       "    4: 0.050035458}},\n",
       "  {'document': 383,\n",
       "   'probabilities': {0: 0.16000026,\n",
       "    1: 0.16134223,\n",
       "    2: 0.358657,\n",
       "    3: 0.16000026,\n",
       "    4: 0.16000026}},\n",
       "  {'document': 384,\n",
       "   'probabilities': {0: 0.16002716,\n",
       "    1: 0.35989395,\n",
       "    2: 0.16003132,\n",
       "    3: 0.16002096,\n",
       "    4: 0.16002661}},\n",
       "  {'document': 385,\n",
       "   'probabilities': {0: 0.11433491,\n",
       "    1: 0.39978182,\n",
       "    2: 0.11434108,\n",
       "    3: 0.25720605,\n",
       "    4: 0.11433616}},\n",
       "  {'document': 386,\n",
       "   'probabilities': {0: 0.25703493,\n",
       "    1: 0.11432012,\n",
       "    2: 0.11431992,\n",
       "    3: 0.1143123,\n",
       "    4: 0.40001273}},\n",
       "  {'document': 387,\n",
       "   'probabilities': {0: 0.16000584,\n",
       "    1: 0.35997707,\n",
       "    2: 0.16000658,\n",
       "    3: 0.1600046,\n",
       "    4: 0.16000587}},\n",
       "  {'document': 388,\n",
       "   'probabilities': {0: 0.2570133,\n",
       "    1: 0.1869614,\n",
       "    2: 0.09542143,\n",
       "    3: 0.3380623,\n",
       "    4: 0.122541584}},\n",
       "  {'document': 389,\n",
       "   'probabilities': {0: 0.35999906,\n",
       "    1: 0.16000023,\n",
       "    2: 0.16000023,\n",
       "    3: 0.16000023,\n",
       "    4: 0.16000023}},\n",
       "  {'document': 390, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 391,\n",
       "   'probabilities': {0: 0.3500003,\n",
       "    1: 0.100064054,\n",
       "    2: 0.100060284,\n",
       "    3: 0.100042835,\n",
       "    4: 0.34983253}},\n",
       "  {'document': 392,\n",
       "   'probabilities': {0: 0.16002806,\n",
       "    1: 0.16003215,\n",
       "    2: 0.16003726,\n",
       "    3: 0.35987383,\n",
       "    4: 0.16002873}},\n",
       "  {'document': 393,\n",
       "   'probabilities': {0: 0.2999309,\n",
       "    1: 0.13336067,\n",
       "    2: 0.13336124,\n",
       "    3: 0.2999892,\n",
       "    4: 0.13335799}},\n",
       "  {'document': 394,\n",
       "   'probabilities': {0: 0.15946361,\n",
       "    1: 0.21191593,\n",
       "    2: 0.25718722,\n",
       "    3: 0.25707564,\n",
       "    4: 0.11435766}},\n",
       "  {'document': 395,\n",
       "   'probabilities': {0: 0.080030106,\n",
       "    1: 0.08003407,\n",
       "    2: 0.17983304,\n",
       "    3: 0.58007246,\n",
       "    4: 0.08003037}},\n",
       "  {'document': 396,\n",
       "   'probabilities': {0: 0.13336533,\n",
       "    1: 0.46653703,\n",
       "    2: 0.13337304,\n",
       "    3: 0.1333582,\n",
       "    4: 0.13336639}},\n",
       "  {'document': 397,\n",
       "   'probabilities': {0: 0.042126518,\n",
       "    1: 0.09877446,\n",
       "    2: 0.20624973,\n",
       "    3: 0.55816174,\n",
       "    4: 0.09468759}},\n",
       "  {'document': 398, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 399,\n",
       "   'probabilities': {0: 0.1999753,\n",
       "    1: 0.20001234,\n",
       "    2: 0.08898695,\n",
       "    3: 0.31110016,\n",
       "    4: 0.19992529}},\n",
       "  {'document': 400, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 401,\n",
       "   'probabilities': {0: 0.13335942,\n",
       "    1: 0.13336211,\n",
       "    2: 0.13336149,\n",
       "    3: 0.46655846,\n",
       "    4: 0.13335855}},\n",
       "  {'document': 402,\n",
       "   'probabilities': {0: 0.25441638,\n",
       "    1: 0.112094834,\n",
       "    2: 0.0727754,\n",
       "    3: 0.3970444,\n",
       "    4: 0.16366902}},\n",
       "  {'document': 403,\n",
       "   'probabilities': {0: 0.16001257,\n",
       "    1: 0.1600143,\n",
       "    2: 0.1600141,\n",
       "    3: 0.16000944,\n",
       "    4: 0.35994962}},\n",
       "  {'document': 404, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 405, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 406,\n",
       "   'probabilities': {0: 0.29999155,\n",
       "    1: 0.13338713,\n",
       "    2: 0.1333841,\n",
       "    3: 0.13336757,\n",
       "    4: 0.29986966}},\n",
       "  {'document': 407,\n",
       "   'probabilities': {0: 0.29358685,\n",
       "    1: 0.13337338,\n",
       "    2: 0.13986966,\n",
       "    3: 0.29980057,\n",
       "    4: 0.13336955}},\n",
       "  {'document': 408,\n",
       "   'probabilities': {0: 0.16000932,\n",
       "    1: 0.16001049,\n",
       "    2: 0.35996348,\n",
       "    3: 0.1600074,\n",
       "    4: 0.16000926}},\n",
       "  {'document': 409,\n",
       "   'probabilities': {0: 0.180109,\n",
       "    1: 0.3799367,\n",
       "    2: 0.08003579,\n",
       "    3: 0.27988714,\n",
       "    4: 0.080031335}},\n",
       "  {'document': 410,\n",
       "   'probabilities': {0: 0.32218933,\n",
       "    1: 0.26667437,\n",
       "    2: 0.044488728,\n",
       "    3: 0.21117483,\n",
       "    4: 0.15547273}},\n",
       "  {'document': 411,\n",
       "   'probabilities': {0: 0.13842703,\n",
       "    1: 0.25702372,\n",
       "    2: 0.2571631,\n",
       "    3: 0.114317596,\n",
       "    4: 0.23306857}},\n",
       "  {'document': 412,\n",
       "   'probabilities': {0: 0.089240484,\n",
       "    1: 0.3706676,\n",
       "    2: 0.08004399,\n",
       "    3: 0.18003722,\n",
       "    4: 0.28001073}},\n",
       "  {'document': 413,\n",
       "   'probabilities': {0: 0.36905694,\n",
       "    1: 0.10591048,\n",
       "    2: 0.10571198,\n",
       "    3: 0.22346519,\n",
       "    4: 0.19585535}},\n",
       "  {'document': 414,\n",
       "   'probabilities': {0: 0.16003734,\n",
       "    1: 0.16004124,\n",
       "    2: 0.16003908,\n",
       "    3: 0.16002716,\n",
       "    4: 0.35985518}},\n",
       "  {'document': 415,\n",
       "   'probabilities': {0: 0.13338096,\n",
       "    1: 0.46648207,\n",
       "    2: 0.13338736,\n",
       "    3: 0.13336922,\n",
       "    4: 0.13338034}},\n",
       "  {'document': 416,\n",
       "   'probabilities': {0: 0.08895564,\n",
       "    1: 0.42197543,\n",
       "    2: 0.16031092,\n",
       "    3: 0.23980105,\n",
       "    4: 0.08895692}},\n",
       "  {'document': 417,\n",
       "   'probabilities': {0: 0.35990435,\n",
       "    1: 0.16002727,\n",
       "    2: 0.16002625,\n",
       "    3: 0.16001832,\n",
       "    4: 0.16002387}},\n",
       "  {'document': 418, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 419,\n",
       "   'probabilities': {0: 0.25708562,\n",
       "    1: 0.11434278,\n",
       "    2: 0.11434277,\n",
       "    3: 0.114324845,\n",
       "    4: 0.39990395}},\n",
       "  {'document': 420,\n",
       "   'probabilities': {0: 0.23320521,\n",
       "    1: 0.15016635,\n",
       "    2: 0.066732645,\n",
       "    3: 0.23304088,\n",
       "    4: 0.31685492}},\n",
       "  {'document': 421,\n",
       "   'probabilities': {0: 0.13334405,\n",
       "    1: 0.13334548,\n",
       "    2: 0.30003926,\n",
       "    3: 0.13334143,\n",
       "    4: 0.29992974}},\n",
       "  {'document': 422,\n",
       "   'probabilities': {0: 0.25164157,\n",
       "    1: 0.11432115,\n",
       "    2: 0.11989216,\n",
       "    3: 0.3998277,\n",
       "    4: 0.1143174}},\n",
       "  {'document': 423,\n",
       "   'probabilities': {0: 0.13336036,\n",
       "    1: 0.4665587,\n",
       "    2: 0.13336398,\n",
       "    3: 0.13335614,\n",
       "    4: 0.13336086}},\n",
       "  {'document': 424,\n",
       "   'probabilities': {0: 0.13335687,\n",
       "    1: 0.13335967,\n",
       "    2: 0.1333619,\n",
       "    3: 0.13335155,\n",
       "    4: 0.46657}},\n",
       "  {'document': 425,\n",
       "   'probabilities': {0: 0.32537547,\n",
       "    1: 0.11433058,\n",
       "    2: 0.11432934,\n",
       "    3: 0.25718674,\n",
       "    4: 0.18877788}},\n",
       "  {'document': 426, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 427,\n",
       "   'probabilities': {0: 0.17999174,\n",
       "    1: 0.08006166,\n",
       "    2: 0.28027567,\n",
       "    3: 0.17972447,\n",
       "    4: 0.27994648}},\n",
       "  {'document': 428,\n",
       "   'probabilities': {0: 0.13335381,\n",
       "    1: 0.13335702,\n",
       "    2: 0.13335699,\n",
       "    3: 0.4665781,\n",
       "    4: 0.1333541}},\n",
       "  {'document': 429,\n",
       "   'probabilities': {0: 0.16002229,\n",
       "    1: 0.16002472,\n",
       "    2: 0.35991302,\n",
       "    3: 0.16001783,\n",
       "    4: 0.16002212}},\n",
       "  {'document': 430,\n",
       "   'probabilities': {0: 0.3531561,\n",
       "    1: 0.16108234,\n",
       "    2: 0.11432223,\n",
       "    3: 0.11431211,\n",
       "    4: 0.25712723}},\n",
       "  {'document': 431,\n",
       "   'probabilities': {0: 0.22067942,\n",
       "    1: 0.46535513,\n",
       "    2: 0.12866923,\n",
       "    3: 0.12810488,\n",
       "    4: 0.057191335}},\n",
       "  {'document': 432, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 433, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 434,\n",
       "   'probabilities': {0: 0.114302695,\n",
       "    1: 0.11430503,\n",
       "    2: 0.114305146,\n",
       "    3: 0.3999836,\n",
       "    4: 0.2571035}},\n",
       "  {'document': 435,\n",
       "   'probabilities': {0: 0.080037944,\n",
       "    1: 0.123299465,\n",
       "    2: 0.08004284,\n",
       "    3: 0.43667337,\n",
       "    4: 0.27994645}},\n",
       "  {'document': 436,\n",
       "   'probabilities': {0: 0.0906093,\n",
       "    1: 0.08003173,\n",
       "    2: 0.08003237,\n",
       "    3: 0.29084015,\n",
       "    4: 0.45848647}},\n",
       "  {'document': 437,\n",
       "   'probabilities': {0: 0.22480097,\n",
       "    1: 0.10004835,\n",
       "    2: 0.100051135,\n",
       "    3: 0.47505516,\n",
       "    4: 0.100044414}},\n",
       "  {'document': 438,\n",
       "   'probabilities': {0: 0.29996052,\n",
       "    1: 0.29993832,\n",
       "    2: 0.1333724,\n",
       "    3: 0.1333604,\n",
       "    4: 0.13336831}},\n",
       "  {'document': 439,\n",
       "   'probabilities': {0: 0.14961328,\n",
       "    1: 0.23327012,\n",
       "    2: 0.4816786,\n",
       "    3: 0.06873005,\n",
       "    4: 0.066707954}},\n",
       "  {'document': 440,\n",
       "   'probabilities': {0: 0.5211384,\n",
       "    1: 0.21533063,\n",
       "    2: 0.1403951,\n",
       "    3: 0.06156367,\n",
       "    4: 0.061572112}},\n",
       "  {'document': 441,\n",
       "   'probabilities': {0: 0.2571864,\n",
       "    1: 0.2571419,\n",
       "    2: 0.11430077,\n",
       "    3: 0.11429603,\n",
       "    4: 0.25707492}},\n",
       "  {'document': 442,\n",
       "   'probabilities': {0: 0.17989306,\n",
       "    1: 0.18001531,\n",
       "    2: 0.17997158,\n",
       "    3: 0.17985027,\n",
       "    4: 0.2802697}},\n",
       "  {'document': 443,\n",
       "   'probabilities': {0: 0.08893401,\n",
       "    1: 0.31110275,\n",
       "    2: 0.20011702,\n",
       "    3: 0.19981194,\n",
       "    4: 0.2000343}},\n",
       "  {'document': 444,\n",
       "   'probabilities': {0: 0.08892204,\n",
       "    1: 0.19985221,\n",
       "    2: 0.08892588,\n",
       "    3: 0.53337806,\n",
       "    4: 0.08892178}},\n",
       "  {'document': 445,\n",
       "   'probabilities': {0: 0.13339277,\n",
       "    1: 0.13339993,\n",
       "    2: 0.13339779,\n",
       "    3: 0.29992425,\n",
       "    4: 0.2998853}},\n",
       "  {'document': 446,\n",
       "   'probabilities': {0: 0.19997287,\n",
       "    1: 0.31121594,\n",
       "    2: 0.20003137,\n",
       "    3: 0.08894164,\n",
       "    4: 0.19983816}},\n",
       "  {'document': 447,\n",
       "   'probabilities': {0: 0.13333838,\n",
       "    1: 0.13333899,\n",
       "    2: 0.13333906,\n",
       "    3: 0.46664524,\n",
       "    4: 0.13333835}},\n",
       "  {'document': 448,\n",
       "   'probabilities': {0: 0.13339601,\n",
       "    1: 0.14644594,\n",
       "    2: 0.28714642,\n",
       "    3: 0.13337146,\n",
       "    4: 0.29964018}},\n",
       "  {'document': 449,\n",
       "   'probabilities': {0: 0.1600145,\n",
       "    1: 0.16001634,\n",
       "    2: 0.16001569,\n",
       "    3: 0.16001189,\n",
       "    4: 0.35994157}},\n",
       "  {'document': 450,\n",
       "   'probabilities': {0: 0.35990357,\n",
       "    1: 0.1600269,\n",
       "    2: 0.16002756,\n",
       "    3: 0.16001813,\n",
       "    4: 0.16002388}},\n",
       "  {'document': 451,\n",
       "   'probabilities': {0: 0.47499287,\n",
       "    1: 0.10005152,\n",
       "    2: 0.100053266,\n",
       "    3: 0.22485648,\n",
       "    4: 0.10004587}},\n",
       "  {'document': 452,\n",
       "   'probabilities': {0: 0.08002297,\n",
       "    1: 0.080026455,\n",
       "    2: 0.080026284,\n",
       "    3: 0.664544,\n",
       "    4: 0.09538037}},\n",
       "  {'document': 453, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 454,\n",
       "   'probabilities': {0: 0.16001365,\n",
       "    1: 0.3599459,\n",
       "    2: 0.1600157,\n",
       "    3: 0.16001062,\n",
       "    4: 0.16001418}},\n",
       "  {'document': 455,\n",
       "   'probabilities': {0: 0.48535046,\n",
       "    1: 0.17200114,\n",
       "    2: 0.17737484,\n",
       "    3: 0.052966386,\n",
       "    4: 0.11230715}},\n",
       "  {'document': 456,\n",
       "   'probabilities': {0: 0.27127364,\n",
       "    1: 0.33926326,\n",
       "    2: 0.05717153,\n",
       "    3: 0.060859893,\n",
       "    4: 0.27143165}},\n",
       "  {'document': 457,\n",
       "   'probabilities': {0: 0.18626323,\n",
       "    1: 0.45253497,\n",
       "    2: 0.05338033,\n",
       "    3: 0.054457944,\n",
       "    4: 0.25336352}},\n",
       "  {'document': 458,\n",
       "   'probabilities': {0: 0.13335061,\n",
       "    1: 0.13335218,\n",
       "    2: 0.13335279,\n",
       "    3: 0.4665942,\n",
       "    4: 0.13335018}},\n",
       "  {'document': 459,\n",
       "   'probabilities': {0: 0.13338386,\n",
       "    1: 0.13339159,\n",
       "    2: 0.29990035,\n",
       "    3: 0.13337258,\n",
       "    4: 0.29995164}},\n",
       "  {'document': 460, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 461, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 462,\n",
       "   'probabilities': {0: 0.3196962,\n",
       "    1: 0.03811224,\n",
       "    2: 0.09312367,\n",
       "    3: 0.51095766,\n",
       "    4: 0.038110312}},\n",
       "  {'document': 463,\n",
       "   'probabilities': {0: 0.095352195,\n",
       "    1: 0.19988374,\n",
       "    2: 0.08892357,\n",
       "    3: 0.52692103,\n",
       "    4: 0.08891944}},\n",
       "  {'document': 464,\n",
       "   'probabilities': {0: 0.18016252,\n",
       "    1: 0.3801062,\n",
       "    2: 0.17990783,\n",
       "    3: 0.08003676,\n",
       "    4: 0.17978665}},\n",
       "  {'document': 465,\n",
       "   'probabilities': {0: 0.13336757,\n",
       "    1: 0.1333749,\n",
       "    2: 0.13337187,\n",
       "    3: 0.3000364,\n",
       "    4: 0.29984924}},\n",
       "  {'document': 466,\n",
       "   'probabilities': {0: 0.1600218,\n",
       "    1: 0.16002397,\n",
       "    2: 0.3599153,\n",
       "    3: 0.16001657,\n",
       "    4: 0.16002242}},\n",
       "  {'document': 467,\n",
       "   'probabilities': {0: 0.13336925,\n",
       "    1: 0.29994124,\n",
       "    2: 0.29995608,\n",
       "    3: 0.13336203,\n",
       "    4: 0.1333714}},\n",
       "  {'document': 468,\n",
       "   'probabilities': {0: 0.16000704,\n",
       "    1: 0.16000782,\n",
       "    2: 0.35997227,\n",
       "    3: 0.16000544,\n",
       "    4: 0.1600074}},\n",
       "  {'document': 469,\n",
       "   'probabilities': {0: 0.16001642,\n",
       "    1: 0.1600199,\n",
       "    2: 0.16002028,\n",
       "    3: 0.16001362,\n",
       "    4: 0.3599298}},\n",
       "  {'document': 470,\n",
       "   'probabilities': {0: 0.4800944,\n",
       "    1: 0.08004081,\n",
       "    2: 0.080038704,\n",
       "    3: 0.18004033,\n",
       "    4: 0.17978576}},\n",
       "  {'document': 471,\n",
       "   'probabilities': {0: 0.16002797,\n",
       "    1: 0.1600297,\n",
       "    2: 0.1600297,\n",
       "    3: 0.16002093,\n",
       "    4: 0.35989177}},\n",
       "  {'document': 472,\n",
       "   'probabilities': {0: 0.06669154,\n",
       "    1: 0.06669443,\n",
       "    2: 0.14979291,\n",
       "    3: 0.06668571,\n",
       "    4: 0.6501354}},\n",
       "  {'document': 473,\n",
       "   'probabilities': {0: 0.25438133,\n",
       "    1: 0.07276626,\n",
       "    2: 0.072767265,\n",
       "    3: 0.43643674,\n",
       "    4: 0.16364838}},\n",
       "  {'document': 474, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 475,\n",
       "   'probabilities': {0: 0.34415588,\n",
       "    1: 0.3453148,\n",
       "    2: 0.074120924,\n",
       "    3: 0.16364129,\n",
       "    4: 0.072767094}},\n",
       "  {'document': 476,\n",
       "   'probabilities': {0: 0.22470544,\n",
       "    1: 0.100071296,\n",
       "    2: 0.35011357,\n",
       "    3: 0.22504678,\n",
       "    4: 0.10006288}},\n",
       "  {'document': 477,\n",
       "   'probabilities': {0: 0.13338515,\n",
       "    1: 0.1345086,\n",
       "    2: 0.2990842,\n",
       "    3: 0.29963642,\n",
       "    4: 0.1333856}},\n",
       "  {'document': 478,\n",
       "   'probabilities': {0: 0.25702077,\n",
       "    1: 0.114333324,\n",
       "    2: 0.11433428,\n",
       "    3: 0.32879198,\n",
       "    4: 0.18551967}},\n",
       "  {'document': 479,\n",
       "   'probabilities': {0: 0.16002294,\n",
       "    1: 0.16002634,\n",
       "    2: 0.3599083,\n",
       "    3: 0.16001815,\n",
       "    4: 0.16002427}},\n",
       "  {'document': 480, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 481, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 482, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 483,\n",
       "   'probabilities': {0: 0.16000932,\n",
       "    1: 0.16001049,\n",
       "    2: 0.35996348,\n",
       "    3: 0.1600074,\n",
       "    4: 0.16000926}},\n",
       "  {'document': 484,\n",
       "   'probabilities': {0: 0.29986286,\n",
       "    1: 0.13341117,\n",
       "    2: 0.13341223,\n",
       "    3: 0.13338432,\n",
       "    4: 0.2999294}},\n",
       "  {'document': 485,\n",
       "   'probabilities': {0: 0.13336596,\n",
       "    1: 0.13336876,\n",
       "    2: 0.1333708,\n",
       "    3: 0.46652815,\n",
       "    4: 0.13336635}},\n",
       "  {'document': 486,\n",
       "   'probabilities': {0: 0.16000958,\n",
       "    1: 0.16001064,\n",
       "    2: 0.16001087,\n",
       "    3: 0.35995921,\n",
       "    4: 0.16000964}},\n",
       "  {'document': 487,\n",
       "   'probabilities': {0: 0.13335252,\n",
       "    1: 0.13335596,\n",
       "    2: 0.13337405,\n",
       "    3: 0.13334821,\n",
       "    4: 0.4665693}},\n",
       "  {'document': 488,\n",
       "   'probabilities': {0: 0.16002463,\n",
       "    1: 0.16002756,\n",
       "    2: 0.16002992,\n",
       "    3: 0.16001907,\n",
       "    4: 0.35989884}},\n",
       "  {'document': 489,\n",
       "   'probabilities': {0: 0.16001193,\n",
       "    1: 0.16001356,\n",
       "    2: 0.16001382,\n",
       "    3: 0.16000955,\n",
       "    4: 0.3599512}},\n",
       "  {'document': 490,\n",
       "   'probabilities': {0: 0.16000932,\n",
       "    1: 0.16001049,\n",
       "    2: 0.35996348,\n",
       "    3: 0.1600074,\n",
       "    4: 0.16000926}},\n",
       "  {'document': 491, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 492,\n",
       "   'probabilities': {0: 0.16002227,\n",
       "    1: 0.1600247,\n",
       "    2: 0.35991305,\n",
       "    3: 0.16001782,\n",
       "    4: 0.16002211}},\n",
       "  {'document': 493,\n",
       "   'probabilities': {0: 0.06670696,\n",
       "    1: 0.14984661,\n",
       "    2: 0.31670865,\n",
       "    3: 0.40002927,\n",
       "    4: 0.066708475}},\n",
       "  {'document': 494,\n",
       "   'probabilities': {0: 0.05337152,\n",
       "    1: 0.19235241,\n",
       "    2: 0.5147197,\n",
       "    3: 0.053360607,\n",
       "    4: 0.18619575}},\n",
       "  {'document': 495,\n",
       "   'probabilities': {0: 0.114299454,\n",
       "    1: 0.114301756,\n",
       "    2: 0.11430134,\n",
       "    3: 0.39999977,\n",
       "    4: 0.25709766}},\n",
       "  {'document': 496,\n",
       "   'probabilities': {0: 0.15786657,\n",
       "    1: 0.11432061,\n",
       "    2: 0.49918637,\n",
       "    3: 0.11430935,\n",
       "    4: 0.11431709}},\n",
       "  {'document': 497,\n",
       "   'probabilities': {0: 0.22466339,\n",
       "    1: 0.1001141,\n",
       "    2: 0.22509065,\n",
       "    3: 0.35003927,\n",
       "    4: 0.10009258}},\n",
       "  {'document': 498,\n",
       "   'probabilities': {0: 0.10346688,\n",
       "    1: 0.40070373,\n",
       "    2: 0.1405269,\n",
       "    3: 0.25199813,\n",
       "    4: 0.10330437}},\n",
       "  {'document': 499,\n",
       "   'probabilities': {0: 0.37371257,\n",
       "    1: 0.18005364,\n",
       "    2: 0.08637586,\n",
       "    3: 0.27980882,\n",
       "    4: 0.08004913}},\n",
       "  {'document': 500,\n",
       "   'probabilities': {0: 0.16003342,\n",
       "    1: 0.16003738,\n",
       "    2: 0.16004075,\n",
       "    3: 0.35985303,\n",
       "    4: 0.16003543}},\n",
       "  {'document': 501,\n",
       "   'probabilities': {0: 0.22589357,\n",
       "    1: 0.0889337,\n",
       "    2: 0.1999895,\n",
       "    3: 0.3962537,\n",
       "    4: 0.08892953}},\n",
       "  {'document': 502,\n",
       "   'probabilities': {0: 0.11962324,\n",
       "    1: 0.11988869,\n",
       "    2: 0.25357196,\n",
       "    3: 0.38679287,\n",
       "    4: 0.12012325}},\n",
       "  {'document': 503,\n",
       "   'probabilities': {0: 0.29984257,\n",
       "    1: 0.13337107,\n",
       "    2: 0.1333709,\n",
       "    3: 0.30004853,\n",
       "    4: 0.1333669}},\n",
       "  {'document': 504,\n",
       "   'probabilities': {0: 0.08893841,\n",
       "    1: 0.08894721,\n",
       "    2: 0.1999307,\n",
       "    3: 0.19977269,\n",
       "    4: 0.42241105}},\n",
       "  {'document': 505,\n",
       "   'probabilities': {0: 0.16004182,\n",
       "    1: 0.1600504,\n",
       "    2: 0.16004886,\n",
       "    3: 0.35981238,\n",
       "    4: 0.16004656}},\n",
       "  {'document': 506,\n",
       "   'probabilities': {0: 0.049919836,\n",
       "    1: 0.453742,\n",
       "    2: 0.05255969,\n",
       "    3: 0.27922487,\n",
       "    4: 0.16455355}},\n",
       "  {'document': 507,\n",
       "   'probabilities': {0: 0.38951156,\n",
       "    1: 0.115212485,\n",
       "    2: 0.26667356,\n",
       "    3: 0.114299566,\n",
       "    4: 0.11430282}},\n",
       "  {'document': 508,\n",
       "   'probabilities': {0: 0.35989794,\n",
       "    1: 0.16002809,\n",
       "    2: 0.16002835,\n",
       "    3: 0.16002078,\n",
       "    4: 0.16002488}},\n",
       "  {'document': 509,\n",
       "   'probabilities': {0: 0.105758496,\n",
       "    1: 0.22349136,\n",
       "    2: 0.2825825,\n",
       "    3: 0.22356652,\n",
       "    4: 0.16460113}},\n",
       "  {'document': 510,\n",
       "   'probabilities': {0: 0.25433096,\n",
       "    1: 0.07275972,\n",
       "    2: 0.16375344,\n",
       "    3: 0.34547162,\n",
       "    4: 0.16368423}},\n",
       "  {'document': 511,\n",
       "   'probabilities': {0: 0.16003738,\n",
       "    1: 0.16004129,\n",
       "    2: 0.16003911,\n",
       "    3: 0.16002719,\n",
       "    4: 0.35985506}},\n",
       "  {'document': 512,\n",
       "   'probabilities': {0: 0.1600387,\n",
       "    1: 0.35984862,\n",
       "    2: 0.1600437,\n",
       "    3: 0.16002934,\n",
       "    4: 0.16003965}},\n",
       "  {'document': 513,\n",
       "   'probabilities': {0: 0.4000172,\n",
       "    1: 0.11431726,\n",
       "    2: 0.25704503,\n",
       "    3: 0.11430679,\n",
       "    4: 0.11431371}},\n",
       "  {'document': 514, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 515,\n",
       "   'probabilities': {0: 0.067178205,\n",
       "    1: 0.5668946,\n",
       "    2: 0.06672405,\n",
       "    3: 0.14964065,\n",
       "    4: 0.14956257}},\n",
       "  {'document': 516,\n",
       "   'probabilities': {0: 0.13341473,\n",
       "    1: 0.2998165,\n",
       "    2: 0.13342834,\n",
       "    3: 0.29992503,\n",
       "    4: 0.13341543}},\n",
       "  {'document': 517, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 518,\n",
       "   'probabilities': {0: 0.16002007,\n",
       "    1: 0.16002287,\n",
       "    2: 0.1600229,\n",
       "    3: 0.35991356,\n",
       "    4: 0.16002056}},\n",
       "  {'document': 519,\n",
       "   'probabilities': {0: 0.25692913,\n",
       "    1: 0.114429586,\n",
       "    2: 0.2573311,\n",
       "    3: 0.11438298,\n",
       "    4: 0.25692722}},\n",
       "  {'document': 520,\n",
       "   'probabilities': {0: 0.13336012,\n",
       "    1: 0.13336404,\n",
       "    2: 0.1333689,\n",
       "    3: 0.46654618,\n",
       "    4: 0.13336077}},\n",
       "  {'document': 521,\n",
       "   'probabilities': {0: 0.09755221,\n",
       "    1: 0.3111317,\n",
       "    2: 0.08892301,\n",
       "    3: 0.41347435,\n",
       "    4: 0.08891876}},\n",
       "  {'document': 522,\n",
       "   'probabilities': {0: 0.16266559,\n",
       "    1: 0.35733312,\n",
       "    2: 0.16000043,\n",
       "    3: 0.16000043,\n",
       "    4: 0.16000043}},\n",
       "  {'document': 523,\n",
       "   'probabilities': {0: 0.13335994,\n",
       "    1: 0.13336283,\n",
       "    2: 0.2999225,\n",
       "    3: 0.29999492,\n",
       "    4: 0.13335983}},\n",
       "  {'document': 524,\n",
       "   'probabilities': {0: 0.16002984,\n",
       "    1: 0.1600332,\n",
       "    2: 0.3598842,\n",
       "    3: 0.16002281,\n",
       "    4: 0.1600299}},\n",
       "  {'document': 525,\n",
       "   'probabilities': {0: 0.19999312,\n",
       "    1: 0.08900328,\n",
       "    2: 0.08900262,\n",
       "    3: 0.19984736,\n",
       "    4: 0.42215368}},\n",
       "  {'document': 526, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 527,\n",
       "   'probabilities': {0: 0.1998497,\n",
       "    1: 0.08895328,\n",
       "    2: 0.20002414,\n",
       "    3: 0.42222694,\n",
       "    4: 0.08894594}},\n",
       "  {'document': 528,\n",
       "   'probabilities': {0: 0.31099194,\n",
       "    1: 0.08893569,\n",
       "    2: 0.19991224,\n",
       "    3: 0.3112271,\n",
       "    4: 0.08893304}},\n",
       "  {'document': 529,\n",
       "   'probabilities': {0: 0.10001986,\n",
       "    1: 0.10002212,\n",
       "    2: 0.1000221,\n",
       "    3: 0.10552696,\n",
       "    4: 0.594409}},\n",
       "  {'document': 530,\n",
       "   'probabilities': {0: 0.17983979,\n",
       "    1: 0.17974411,\n",
       "    2: 0.28058195,\n",
       "    3: 0.17989077,\n",
       "    4: 0.17994338}},\n",
       "  {'document': 531, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 532,\n",
       "   'probabilities': {0: 0.32012585,\n",
       "    1: 0.16003427,\n",
       "    2: 0.1997849,\n",
       "    3: 0.16002308,\n",
       "    4: 0.16003194}},\n",
       "  {'document': 533,\n",
       "   'probabilities': {0: 0.094479725,\n",
       "    1: 0.14718726,\n",
       "    2: 0.39257917,\n",
       "    3: 0.27103892,\n",
       "    4: 0.09471496}},\n",
       "  {'document': 534,\n",
       "   'probabilities': {0: 0.16002797,\n",
       "    1: 0.1600297,\n",
       "    2: 0.16002968,\n",
       "    3: 0.16002093,\n",
       "    4: 0.35989174}},\n",
       "  {'document': 535,\n",
       "   'probabilities': {0: 0.19979286,\n",
       "    1: 0.20014848,\n",
       "    2: 0.19984081,\n",
       "    3: 0.08892697,\n",
       "    4: 0.31129086}},\n",
       "  {'document': 536,\n",
       "   'probabilities': {0: 0.06367438,\n",
       "    1: 0.047070663,\n",
       "    2: 0.78994584,\n",
       "    3: 0.052239615,\n",
       "    4: 0.04706949}},\n",
       "  {'document': 537,\n",
       "   'probabilities': {0: 0.25690663,\n",
       "    1: 0.11436096,\n",
       "    2: 0.11435925,\n",
       "    3: 0.2570489,\n",
       "    4: 0.25732425}},\n",
       "  {'document': 538,\n",
       "   'probabilities': {0: 0.16000026,\n",
       "    1: 0.16134202,\n",
       "    2: 0.35865718,\n",
       "    3: 0.16000026,\n",
       "    4: 0.16000026}},\n",
       "  {'document': 539,\n",
       "   'probabilities': {0: 0.1333763,\n",
       "    1: 0.13338187,\n",
       "    2: 0.29998735,\n",
       "    3: 0.2998782,\n",
       "    4: 0.13337629}},\n",
       "  {'document': 540,\n",
       "   'probabilities': {0: 0.10003351,\n",
       "    1: 0.2249723,\n",
       "    2: 0.10003737,\n",
       "    3: 0.47492263,\n",
       "    4: 0.10003419}},\n",
       "  {'document': 541,\n",
       "   'probabilities': {0: 0.16000682,\n",
       "    1: 0.16000745,\n",
       "    2: 0.1600079,\n",
       "    3: 0.3599709,\n",
       "    4: 0.16000691}},\n",
       "  {'document': 542,\n",
       "   'probabilities': {0: 0.17827709,\n",
       "    1: 0.13338183,\n",
       "    2: 0.2999926,\n",
       "    3: 0.2549703,\n",
       "    4: 0.13337821}},\n",
       "  {'document': 543,\n",
       "   'probabilities': {0: 0.16350445,\n",
       "    1: 0.07279634,\n",
       "    2: 0.3458059,\n",
       "    3: 0.34510234,\n",
       "    4: 0.072790995}},\n",
       "  {'document': 544,\n",
       "   'probabilities': {0: 0.16001259,\n",
       "    1: 0.1600143,\n",
       "    2: 0.1600141,\n",
       "    3: 0.16000944,\n",
       "    4: 0.35994962}},\n",
       "  {'document': 545,\n",
       "   'probabilities': {0: 0.2457943,\n",
       "    1: 0.23133652,\n",
       "    2: 0.17833027,\n",
       "    3: 0.25889105,\n",
       "    4: 0.085647844}},\n",
       "  {'document': 546,\n",
       "   'probabilities': {0: 0.16000932,\n",
       "    1: 0.16001049,\n",
       "    2: 0.35996348,\n",
       "    3: 0.1600074,\n",
       "    4: 0.16000926}},\n",
       "  {'document': 547,\n",
       "   'probabilities': {0: 0.35999906,\n",
       "    1: 0.16000025,\n",
       "    2: 0.16000025,\n",
       "    3: 0.16000025,\n",
       "    4: 0.16000025}},\n",
       "  {'document': 548,\n",
       "   'probabilities': {0: 0.16002427,\n",
       "    1: 0.16002731,\n",
       "    2: 0.16002753,\n",
       "    3: 0.16001852,\n",
       "    4: 0.35990238}},\n",
       "  {'document': 549, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 550,\n",
       "   'probabilities': {0: 0.13336402,\n",
       "    1: 0.29999825,\n",
       "    2: 0.29991534,\n",
       "    3: 0.13335827,\n",
       "    4: 0.13336414}},\n",
       "  {'document': 551,\n",
       "   'probabilities': {0: 0.35984078,\n",
       "    1: 0.1600447,\n",
       "    2: 0.16004433,\n",
       "    3: 0.16003004,\n",
       "    4: 0.16004016}},\n",
       "  {'document': 552,\n",
       "   'probabilities': {0: 0.11538396,\n",
       "    1: 0.39889768,\n",
       "    2: 0.1143111,\n",
       "    3: 0.11430272,\n",
       "    4: 0.25710455}},\n",
       "  {'document': 553,\n",
       "   'probabilities': {0: 0.081539296,\n",
       "    1: 0.296734,\n",
       "    2: 0.1727399,\n",
       "    3: 0.41259837,\n",
       "    4: 0.03638843}},\n",
       "  {'document': 554,\n",
       "   'probabilities': {0: 0.05791019,\n",
       "    1: 0.315402,\n",
       "    2: 0.07691608,\n",
       "    3: 0.29641917,\n",
       "    4: 0.25335255}},\n",
       "  {'document': 555,\n",
       "   'probabilities': {0: 0.16004327,\n",
       "    1: 0.16004333,\n",
       "    2: 0.16004437,\n",
       "    3: 0.16003142,\n",
       "    4: 0.35983762}},\n",
       "  {'document': 556,\n",
       "   'probabilities': {0: 0.2250303,\n",
       "    1: 0.1888235,\n",
       "    2: 0.22503433,\n",
       "    3: 0.26110238,\n",
       "    4: 0.10000947}},\n",
       "  {'document': 557,\n",
       "   'probabilities': {0: 0.13338315,\n",
       "    1: 0.29998216,\n",
       "    2: 0.2998796,\n",
       "    3: 0.1333723,\n",
       "    4: 0.13338278}},\n",
       "  {'document': 558,\n",
       "   'probabilities': {0: 0.29986086,\n",
       "    1: 0.29997176,\n",
       "    2: 0.13339715,\n",
       "    3: 0.133379,\n",
       "    4: 0.13339122}},\n",
       "  {'document': 559,\n",
       "   'probabilities': {0: 0.16002943,\n",
       "    1: 0.35988173,\n",
       "    2: 0.16003428,\n",
       "    3: 0.16002367,\n",
       "    4: 0.16003089}},\n",
       "  {'document': 560,\n",
       "   'probabilities': {0: 0.16000527,\n",
       "    1: 0.1600059,\n",
       "    2: 0.16000599,\n",
       "    3: 0.35997763,\n",
       "    4: 0.16000524}},\n",
       "  {'document': 561,\n",
       "   'probabilities': {0: 0.06668804,\n",
       "    1: 0.23329912,\n",
       "    2: 0.06669171,\n",
       "    3: 0.48329863,\n",
       "    4: 0.15002248}},\n",
       "  {'document': 562,\n",
       "   'probabilities': {0: 0.16000028,\n",
       "    1: 0.16000028,\n",
       "    2: 0.35999885,\n",
       "    3: 0.16000028,\n",
       "    4: 0.16000028}},\n",
       "  {'document': 563,\n",
       "   'probabilities': {0: 0.1600145,\n",
       "    1: 0.16001633,\n",
       "    2: 0.16001569,\n",
       "    3: 0.16001189,\n",
       "    4: 0.3599416}},\n",
       "  {'document': 564, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 565,\n",
       "   'probabilities': {0: 0.16003695,\n",
       "    1: 0.16004005,\n",
       "    2: 0.16003907,\n",
       "    3: 0.3598462,\n",
       "    4: 0.16003779}},\n",
       "  {'document': 566,\n",
       "   'probabilities': {0: 0.04707971,\n",
       "    1: 0.6355427,\n",
       "    2: 0.04708256,\n",
       "    3: 0.105779625,\n",
       "    4: 0.16451548}},\n",
       "  {'document': 567,\n",
       "   'probabilities': {0: 0.35987666,\n",
       "    1: 0.1600345,\n",
       "    2: 0.16003415,\n",
       "    3: 0.16002406,\n",
       "    4: 0.16003059}},\n",
       "  {'document': 568,\n",
       "   'probabilities': {0: 0.13334998,\n",
       "    1: 0.13335225,\n",
       "    2: 0.13335487,\n",
       "    3: 0.46659258,\n",
       "    4: 0.13335036}},\n",
       "  {'document': 569,\n",
       "   'probabilities': {0: 0.11431501,\n",
       "    1: 0.11431824,\n",
       "    2: 0.25699958,\n",
       "    3: 0.11430813,\n",
       "    4: 0.40005904}},\n",
       "  {'document': 570,\n",
       "   'probabilities': {0: 0.06669405,\n",
       "    1: 0.38115704,\n",
       "    2: 0.066696644,\n",
       "    3: 0.4187591,\n",
       "    4: 0.06669317}},\n",
       "  {'document': 571,\n",
       "   'probabilities': {0: 0.29986155,\n",
       "    1: 0.2999918,\n",
       "    2: 0.13338938,\n",
       "    3: 0.1333724,\n",
       "    4: 0.13338485}},\n",
       "  {'document': 572, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 573,\n",
       "   'probabilities': {0: 0.06158188,\n",
       "    1: 0.19035825,\n",
       "    2: 0.21540293,\n",
       "    3: 0.31731683,\n",
       "    4: 0.21534012}},\n",
       "  {'document': 574, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 575,\n",
       "   'probabilities': {0: 0.13337697,\n",
       "    1: 0.2999164,\n",
       "    2: 0.2999617,\n",
       "    3: 0.13336638,\n",
       "    4: 0.13337857}},\n",
       "  {'document': 576,\n",
       "   'probabilities': {0: 0.29982588,\n",
       "    1: 0.13337652,\n",
       "    2: 0.1333761,\n",
       "    3: 0.3000507,\n",
       "    4: 0.1333708}},\n",
       "  {'document': 577,\n",
       "   'probabilities': {0: 0.16000958,\n",
       "    1: 0.16001064,\n",
       "    2: 0.16001087,\n",
       "    3: 0.35995921,\n",
       "    4: 0.16000964}},\n",
       "  {'document': 578,\n",
       "   'probabilities': {0: 0.2569518,\n",
       "    1: 0.1143544,\n",
       "    2: 0.11435796,\n",
       "    3: 0.2572228,\n",
       "    4: 0.257113}},\n",
       "  {'document': 579,\n",
       "   'probabilities': {0: 0.17453453,\n",
       "    1: 0.118362345,\n",
       "    2: 0.48227862,\n",
       "    3: 0.17478889,\n",
       "    4: 0.05003564}},\n",
       "  {'document': 580,\n",
       "   'probabilities': {0: 0.3969527,\n",
       "    1: 0.23337615,\n",
       "    2: 0.14362629,\n",
       "    3: 0.076025054,\n",
       "    4: 0.15001984}},\n",
       "  {'document': 581,\n",
       "   'probabilities': {0: 0.2516637,\n",
       "    1: 0.2570185,\n",
       "    2: 0.11989265,\n",
       "    3: 0.25710112,\n",
       "    4: 0.11432402}},\n",
       "  {'document': 582,\n",
       "   'probabilities': {0: 0.11432539,\n",
       "    1: 0.40000194,\n",
       "    2: 0.2570303,\n",
       "    3: 0.11431613,\n",
       "    4: 0.11432623}},\n",
       "  {'document': 583, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 584,\n",
       "   'probabilities': {0: 0.16002974,\n",
       "    1: 0.35987818,\n",
       "    2: 0.16003473,\n",
       "    3: 0.16002499,\n",
       "    4: 0.16003233}},\n",
       "  {'document': 585,\n",
       "   'probabilities': {0: 0.18650061,\n",
       "    1: 0.16348605,\n",
       "    2: 0.25467965,\n",
       "    3: 0.16358556,\n",
       "    4: 0.23174813}},\n",
       "  {'document': 586, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 587, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 588,\n",
       "   'probabilities': {0: 0.299899,\n",
       "    1: 0.13339183,\n",
       "    2: 0.13339242,\n",
       "    3: 0.13337344,\n",
       "    4: 0.2999433}},\n",
       "  {'document': 589,\n",
       "   'probabilities': {0: 0.117328316,\n",
       "    1: 0.19329692,\n",
       "    2: 0.322476,\n",
       "    3: 0.10004635,\n",
       "    4: 0.26685238}},\n",
       "  {'document': 590,\n",
       "   'probabilities': {0: 0.3596337,\n",
       "    1: 0.16009788,\n",
       "    2: 0.16010328,\n",
       "    3: 0.1600724,\n",
       "    4: 0.16009268}},\n",
       "  {'document': 591,\n",
       "   'probabilities': {0: 0.1143005,\n",
       "    1: 0.114302196,\n",
       "    2: 0.11430233,\n",
       "    3: 0.5427943,\n",
       "    4: 0.11430072}},\n",
       "  {'document': 592,\n",
       "   'probabilities': {0: 0.19983906,\n",
       "    1: 0.2714656,\n",
       "    2: 0.05718787,\n",
       "    3: 0.4143243,\n",
       "    4: 0.057183124}},\n",
       "  {'document': 593,\n",
       "   'probabilities': {0: 0.13976866,\n",
       "    1: 0.14071205,\n",
       "    2: 0.1894257,\n",
       "    3: 0.3900679,\n",
       "    4: 0.14002563}},\n",
       "  {'document': 594,\n",
       "   'probabilities': {0: 0.13334897,\n",
       "    1: 0.13335085,\n",
       "    2: 0.13335145,\n",
       "    3: 0.29998893,\n",
       "    4: 0.29995987}},\n",
       "  {'document': 595,\n",
       "   'probabilities': {0: 0.3488625,\n",
       "    1: 0.16186762,\n",
       "    2: 0.11794757,\n",
       "    3: 0.25698614,\n",
       "    4: 0.11433615}},\n",
       "  {'document': 596,\n",
       "   'probabilities': {0: 0.053203963,\n",
       "    1: 0.25274485,\n",
       "    2: 0.40402597,\n",
       "    3: 0.0936839,\n",
       "    4: 0.19634129}},\n",
       "  {'document': 597,\n",
       "   'probabilities': {0: 0.11432621,\n",
       "    1: 0.114334606,\n",
       "    2: 0.25711277,\n",
       "    3: 0.39989784,\n",
       "    4: 0.11432857}},\n",
       "  {'document': 598,\n",
       "   'probabilities': {0: 0.31090245,\n",
       "    1: 0.311143,\n",
       "    2: 0.08894494,\n",
       "    3: 0.20007147,\n",
       "    4: 0.08893819}},\n",
       "  {'document': 599,\n",
       "   'probabilities': {0: 0.13741009,\n",
       "    1: 0.050206076,\n",
       "    2: 0.40011394,\n",
       "    3: 0.112461425,\n",
       "    4: 0.29980844}},\n",
       "  {'document': 600,\n",
       "   'probabilities': {0: 0.10001643,\n",
       "    1: 0.22501314,\n",
       "    2: 0.10001851,\n",
       "    3: 0.47493547,\n",
       "    4: 0.100016415}},\n",
       "  {'document': 601,\n",
       "   'probabilities': {0: 0.06157478,\n",
       "    1: 0.21487299,\n",
       "    2: 0.36965108,\n",
       "    3: 0.1385575,\n",
       "    4: 0.21534365}},\n",
       "  {'document': 602,\n",
       "   'probabilities': {0: 0.08894525,\n",
       "    1: 0.19997978,\n",
       "    2: 0.19996595,\n",
       "    3: 0.19978744,\n",
       "    4: 0.31132162}},\n",
       "  {'document': 603,\n",
       "   'probabilities': {0: 0.29980958,\n",
       "    1: 0.13342144,\n",
       "    2: 0.13342611,\n",
       "    3: 0.2999266,\n",
       "    4: 0.13341627}},\n",
       "  {'document': 604,\n",
       "   'probabilities': {0: 0.30009866,\n",
       "    1: 0.13336731,\n",
       "    2: 0.13336992,\n",
       "    3: 0.13335663,\n",
       "    4: 0.2998075}},\n",
       "  {'document': 605,\n",
       "   'probabilities': {0: 0.35992026,\n",
       "    1: 0.16002274,\n",
       "    2: 0.16002245,\n",
       "    3: 0.16001529,\n",
       "    4: 0.16001928}},\n",
       "  {'document': 606,\n",
       "   'probabilities': {0: 0.100043535,\n",
       "    1: 0.10003962,\n",
       "    2: 0.10004669,\n",
       "    3: 0.5998338,\n",
       "    4: 0.10003639}},\n",
       "  {'document': 607,\n",
       "   'probabilities': {0: 0.13335693,\n",
       "    1: 0.13336107,\n",
       "    2: 0.13336089,\n",
       "    3: 0.14799342,\n",
       "    4: 0.45192772}},\n",
       "  {'document': 608,\n",
       "   'probabilities': {0: 0.3998207,\n",
       "    1: 0.25724298,\n",
       "    2: 0.114316806,\n",
       "    3: 0.11430678,\n",
       "    4: 0.11431275}},\n",
       "  {'document': 609,\n",
       "   'probabilities': {0: 0.17983073,\n",
       "    1: 0.28023216,\n",
       "    2: 0.17985946,\n",
       "    3: 0.2800201,\n",
       "    4: 0.08005757}},\n",
       "  {'document': 610,\n",
       "   'probabilities': {0: 0.10003423,\n",
       "    1: 0.47502586,\n",
       "    2: 0.10003857,\n",
       "    3: 0.10002609,\n",
       "    4: 0.22487521}},\n",
       "  {'document': 611,\n",
       "   'probabilities': {0: 0.16002989,\n",
       "    1: 0.16003324,\n",
       "    2: 0.35988414,\n",
       "    3: 0.16002281,\n",
       "    4: 0.16002992}},\n",
       "  {'document': 612,\n",
       "   'probabilities': {0: 0.08003624,\n",
       "    1: 0.17980185,\n",
       "    2: 0.08004267,\n",
       "    3: 0.56128514,\n",
       "    4: 0.09883413}},\n",
       "  {'document': 613,\n",
       "   'probabilities': {0: 0.1333791,\n",
       "    1: 0.13338597,\n",
       "    2: 0.30001628,\n",
       "    3: 0.29983756,\n",
       "    4: 0.13338108}},\n",
       "  {'document': 614,\n",
       "   'probabilities': {0: 0.23555645,\n",
       "    1: 0.17914277,\n",
       "    2: 0.16964325,\n",
       "    3: 0.26254824,\n",
       "    4: 0.15310927}},\n",
       "  {'document': 615,\n",
       "   'probabilities': {0: 0.35985225,\n",
       "    1: 0.16004047,\n",
       "    2: 0.16004062,\n",
       "    3: 0.16002852,\n",
       "    4: 0.16003814}},\n",
       "  {'document': 616,\n",
       "   'probabilities': {0: 0.16006431,\n",
       "    1: 0.3597559,\n",
       "    2: 0.16007039,\n",
       "    3: 0.16004397,\n",
       "    4: 0.16006543}},\n",
       "  {'document': 617,\n",
       "   'probabilities': {0: 0.19599345,\n",
       "    1: 0.19999759,\n",
       "    2: 0.0933495,\n",
       "    3: 0.3112784,\n",
       "    4: 0.19938104}},\n",
       "  {'document': 618,\n",
       "   'probabilities': {0: 0.16266187,\n",
       "    1: 0.35733685,\n",
       "    2: 0.16000043,\n",
       "    3: 0.16000043,\n",
       "    4: 0.16000043}},\n",
       "  {'document': 619,\n",
       "   'probabilities': {0: 0.057168756,\n",
       "    1: 0.16181138,\n",
       "    2: 0.12858272,\n",
       "    3: 0.5867175,\n",
       "    4: 0.06571967}},\n",
       "  {'document': 620, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 621,\n",
       "   'probabilities': {0: 0.088901006,\n",
       "    1: 0.08890247,\n",
       "    2: 0.088902086,\n",
       "    3: 0.09282062,\n",
       "    4: 0.64047384}},\n",
       "  {'document': 622,\n",
       "   'probabilities': {0: 0.2617654,\n",
       "    1: 0.17265017,\n",
       "    2: 0.10622197,\n",
       "    3: 0.28672722,\n",
       "    4: 0.17263521}},\n",
       "  {'document': 623,\n",
       "   'probabilities': {0: 0.23292379,\n",
       "    1: 0.06670473,\n",
       "    2: 0.40028125,\n",
       "    3: 0.06669226,\n",
       "    4: 0.23339793}},\n",
       "  {'document': 624, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 625,\n",
       "   'probabilities': {0: 0.25436687,\n",
       "    1: 0.25474334,\n",
       "    2: 0.07278683,\n",
       "    3: 0.16340286,\n",
       "    4: 0.25470012}},\n",
       "  {'document': 626,\n",
       "   'probabilities': {0: 0.35985914,\n",
       "    1: 0.16003942,\n",
       "    2: 0.16003913,\n",
       "    3: 0.16002727,\n",
       "    4: 0.16003507}},\n",
       "  {'document': 627,\n",
       "   'probabilities': {0: 0.11429151,\n",
       "    1: 0.11429222,\n",
       "    2: 0.114292406,\n",
       "    3: 0.5428322,\n",
       "    4: 0.11429166}},\n",
       "  {'document': 628,\n",
       "   'probabilities': {0: 0.16000527,\n",
       "    1: 0.1600059,\n",
       "    2: 0.16000599,\n",
       "    3: 0.35997763,\n",
       "    4: 0.16000524}},\n",
       "  {'document': 629,\n",
       "   'probabilities': {0: 0.20502222,\n",
       "    1: 0.19500248,\n",
       "    2: 0.31114614,\n",
       "    3: 0.08891211,\n",
       "    4: 0.19991706}},\n",
       "  {'document': 630,\n",
       "   'probabilities': {0: 0.17145091,\n",
       "    1: 0.28789717,\n",
       "    2: 0.23395236,\n",
       "    3: 0.13571168,\n",
       "    4: 0.17098783}},\n",
       "  {'document': 631,\n",
       "   'probabilities': {0: 0.3598433,\n",
       "    1: 0.16004433,\n",
       "    2: 0.16004334,\n",
       "    3: 0.160031,\n",
       "    4: 0.16003798}},\n",
       "  {'document': 632,\n",
       "   'probabilities': {0: 0.0858598,\n",
       "    1: 0.05716019,\n",
       "    2: 0.6715836,\n",
       "    3: 0.12823798,\n",
       "    4: 0.05715846}},\n",
       "  {'document': 633,\n",
       "   'probabilities': {0: 0.08558852,\n",
       "    1: 0.27996746,\n",
       "    2: 0.08006383,\n",
       "    3: 0.17977186,\n",
       "    4: 0.37460834}},\n",
       "  {'document': 634,\n",
       "   'probabilities': {0: 0.28235275,\n",
       "    1: 0.10581788,\n",
       "    2: 0.2112674,\n",
       "    3: 0.17704585,\n",
       "    4: 0.2235161}},\n",
       "  {'document': 635,\n",
       "   'probabilities': {0: 0.18699127,\n",
       "    1: 0.2119795,\n",
       "    2: 0.13884893,\n",
       "    3: 0.27505258,\n",
       "    4: 0.1871277}},\n",
       "  {'document': 636,\n",
       "   'probabilities': {0: 0.088916086,\n",
       "    1: 0.08892015,\n",
       "    2: 0.3110482,\n",
       "    3: 0.20000896,\n",
       "    4: 0.31110662}},\n",
       "  {'document': 637,\n",
       "   'probabilities': {0: 0.3240241,\n",
       "    1: 0.31805936,\n",
       "    2: 0.08550056,\n",
       "    3: 0.1866623,\n",
       "    4: 0.08575372}},\n",
       "  {'document': 638,\n",
       "   'probabilities': {0: 0.08890431,\n",
       "    1: 0.08890683,\n",
       "    2: 0.088906676,\n",
       "    3: 0.64437664,\n",
       "    4: 0.08890553}},\n",
       "  {'document': 639,\n",
       "   'probabilities': {0: 0.19977117,\n",
       "    1: 0.20003118,\n",
       "    2: 0.27182955,\n",
       "    3: 0.12807624,\n",
       "    4: 0.20029186}},\n",
       "  {'document': 640,\n",
       "   'probabilities': {0: 0.12834945,\n",
       "    1: 0.12829106,\n",
       "    2: 0.05719341,\n",
       "    3: 0.34301394,\n",
       "    4: 0.3431521}},\n",
       "  {'document': 641,\n",
       "   'probabilities': {0: 0.18101355,\n",
       "    1: 0.16043533,\n",
       "    2: 0.14132206,\n",
       "    3: 0.14175525,\n",
       "    4: 0.37547383}},\n",
       "  {'document': 642,\n",
       "   'probabilities': {0: 0.1600272,\n",
       "    1: 0.16003025,\n",
       "    2: 0.35989267,\n",
       "    3: 0.16002057,\n",
       "    4: 0.16002929}},\n",
       "  {'document': 643, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 644,\n",
       "   'probabilities': {0: 0.16001861,\n",
       "    1: 0.16002151,\n",
       "    2: 0.16002177,\n",
       "    3: 0.16001473,\n",
       "    4: 0.35992336}},\n",
       "  {'document': 645,\n",
       "   'probabilities': {0: 0.16002625,\n",
       "    1: 0.16002947,\n",
       "    2: 0.35989627,\n",
       "    3: 0.16002072,\n",
       "    4: 0.1600273}},\n",
       "  {'document': 646,\n",
       "   'probabilities': {0: 0.16002242,\n",
       "    1: 0.35990864,\n",
       "    2: 0.16002695,\n",
       "    3: 0.16001794,\n",
       "    4: 0.16002402}},\n",
       "  {'document': 647,\n",
       "   'probabilities': {0: 0.25436315,\n",
       "    1: 0.25458223,\n",
       "    2: 0.072762154,\n",
       "    3: 0.34553367,\n",
       "    4: 0.07275881}},\n",
       "  {'document': 648,\n",
       "   'probabilities': {0: 0.08890905,\n",
       "    1: 0.08891129,\n",
       "    2: 0.08891272,\n",
       "    3: 0.623456,\n",
       "    4: 0.10981095}},\n",
       "  {'document': 649,\n",
       "   'probabilities': {0: 0.2504319,\n",
       "    1: 0.1973927,\n",
       "    2: 0.19131076,\n",
       "    3: 0.07310612,\n",
       "    4: 0.28775856}},\n",
       "  {'document': 650,\n",
       "   'probabilities': {0: 0.21734685,\n",
       "    1: 0.3174015,\n",
       "    2: 0.07276409,\n",
       "    3: 0.31972748,\n",
       "    4: 0.072760075}},\n",
       "  {'document': 651,\n",
       "   'probabilities': {0: 0.13335526,\n",
       "    1: 0.30000558,\n",
       "    2: 0.13335945,\n",
       "    3: 0.13335097,\n",
       "    4: 0.29992872}},\n",
       "  {'document': 652,\n",
       "   'probabilities': {0: 0.07173144,\n",
       "    1: 0.12996432,\n",
       "    2: 0.07185486,\n",
       "    3: 0.53455937,\n",
       "    4: 0.19189003}},\n",
       "  {'document': 653, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 654,\n",
       "   'probabilities': {0: 0.100022495,\n",
       "    1: 0.10002561,\n",
       "    2: 0.22482482,\n",
       "    3: 0.4751046,\n",
       "    4: 0.10002247}},\n",
       "  {'document': 655,\n",
       "   'probabilities': {0: 0.19982688,\n",
       "    1: 0.16790652,\n",
       "    2: 0.12850705,\n",
       "    3: 0.4465781,\n",
       "    4: 0.057181455}},\n",
       "  {'document': 656, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 657,\n",
       "   'probabilities': {0: 0.29974908,\n",
       "    1: 0.13339087,\n",
       "    2: 0.13339213,\n",
       "    3: 0.3000837,\n",
       "    4: 0.13338423}},\n",
       "  {'document': 658,\n",
       "   'probabilities': {0: 0.100048445,\n",
       "    1: 0.22485784,\n",
       "    2: 0.22492103,\n",
       "    3: 0.3501253,\n",
       "    4: 0.10004739}},\n",
       "  {'document': 659, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 660,\n",
       "   'probabilities': {0: 0.3616335,\n",
       "    1: 0.036368947,\n",
       "    2: 0.1604293,\n",
       "    3: 0.19977386,\n",
       "    4: 0.24179436}},\n",
       "  {'document': 661,\n",
       "   'probabilities': {0: 0.1600057,\n",
       "    1: 0.1600062,\n",
       "    2: 0.16000651,\n",
       "    3: 0.1600043,\n",
       "    4: 0.35997733}},\n",
       "  {'document': 662,\n",
       "   'probabilities': {0: 0.13337648,\n",
       "    1: 0.13338229,\n",
       "    2: 0.13338344,\n",
       "    3: 0.13336651,\n",
       "    4: 0.46649128}},\n",
       "  {'document': 663,\n",
       "   'probabilities': {0: 0.3273424,\n",
       "    1: 0.334406,\n",
       "    2: 0.13830991,\n",
       "    3: 0.061575945,\n",
       "    4: 0.13836579}},\n",
       "  {'document': 664, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 665,\n",
       "   'probabilities': {0: 0.13335973,\n",
       "    1: 0.29989827,\n",
       "    2: 0.1333628,\n",
       "    3: 0.3000187,\n",
       "    4: 0.1333605}},\n",
       "  {'document': 666,\n",
       "   'probabilities': {0: 0.16000958,\n",
       "    1: 0.16001064,\n",
       "    2: 0.16001087,\n",
       "    3: 0.35995921,\n",
       "    4: 0.16000964}},\n",
       "  {'document': 667,\n",
       "   'probabilities': {0: 0.13337338,\n",
       "    1: 0.4665094,\n",
       "    2: 0.13337922,\n",
       "    3: 0.13336429,\n",
       "    4: 0.13337366}},\n",
       "  {'document': 668,\n",
       "   'probabilities': {0: 0.16231535,\n",
       "    1: 0.23654285,\n",
       "    2: 0.2773468,\n",
       "    3: 0.13815355,\n",
       "    4: 0.18564148}},\n",
       "  {'document': 669,\n",
       "   'probabilities': {0: 0.100068115,\n",
       "    1: 0.47546148,\n",
       "    2: 0.22434196,\n",
       "    3: 0.10005718,\n",
       "    4: 0.10007127}},\n",
       "  {'document': 670,\n",
       "   'probabilities': {0: 0.16004942,\n",
       "    1: 0.16006115,\n",
       "    2: 0.16005453,\n",
       "    3: 0.35978863,\n",
       "    4: 0.1600463}},\n",
       "  {'document': 671,\n",
       "   'probabilities': {0: 0.16001365,\n",
       "    1: 0.3599459,\n",
       "    2: 0.16001569,\n",
       "    3: 0.16001062,\n",
       "    4: 0.16001418}},\n",
       "  {'document': 672, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 673,\n",
       "   'probabilities': {0: 0.16007158,\n",
       "    1: 0.16008085,\n",
       "    2: 0.16008303,\n",
       "    3: 0.16005462,\n",
       "    4: 0.35970995}},\n",
       "  {'document': 674, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 675,\n",
       "   'probabilities': {0: 0.16002801,\n",
       "    1: 0.16002972,\n",
       "    2: 0.1600297,\n",
       "    3: 0.16002095,\n",
       "    4: 0.35989165}},\n",
       "  {'document': 676,\n",
       "   'probabilities': {0: 0.25702757,\n",
       "    1: 0.25714794,\n",
       "    2: 0.25719076,\n",
       "    3: 0.114313446,\n",
       "    4: 0.114320256}},\n",
       "  {'document': 677,\n",
       "   'probabilities': {0: 0.26682696,\n",
       "    1: 0.13301578,\n",
       "    2: 0.28356788,\n",
       "    3: 0.19997124,\n",
       "    4: 0.11661813}},\n",
       "  {'document': 678, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 679,\n",
       "   'probabilities': {0: 0.114308394,\n",
       "    1: 0.11431084,\n",
       "    2: 0.25704393,\n",
       "    3: 0.40002847,\n",
       "    4: 0.11430834}},\n",
       "  {'document': 680,\n",
       "   'probabilities': {0: 0.1383562,\n",
       "    1: 0.13834825,\n",
       "    2: 0.29261497,\n",
       "    3: 0.29221758,\n",
       "    4: 0.13846304}},\n",
       "  {'document': 681,\n",
       "   'probabilities': {0: 0.3137247,\n",
       "    1: 0.17296223,\n",
       "    2: 0.15181789,\n",
       "    3: 0.15941255,\n",
       "    4: 0.20208259}},\n",
       "  {'document': 682,\n",
       "   'probabilities': {0: 0.16002986,\n",
       "    1: 0.16003321,\n",
       "    2: 0.35988417,\n",
       "    3: 0.16002281,\n",
       "    4: 0.16002992}},\n",
       "  {'document': 683,\n",
       "   'probabilities': {0: 0.22485583,\n",
       "    1: 0.10005566,\n",
       "    2: 0.10915421,\n",
       "    3: 0.10003973,\n",
       "    4: 0.4658946}},\n",
       "  {'document': 684,\n",
       "   'probabilities': {0: 0.114356734,\n",
       "    1: 0.1143662,\n",
       "    2: 0.11436738,\n",
       "    3: 0.25695467,\n",
       "    4: 0.39995503}},\n",
       "  {'document': 685,\n",
       "   'probabilities': {0: 0.0800323,\n",
       "    1: 0.18006289,\n",
       "    2: 0.08003594,\n",
       "    3: 0.18003443,\n",
       "    4: 0.47983447}},\n",
       "  {'document': 686,\n",
       "   'probabilities': {0: 0.16001642,\n",
       "    1: 0.1600199,\n",
       "    2: 0.16002028,\n",
       "    3: 0.16001362,\n",
       "    4: 0.35992977}},\n",
       "  {'document': 687, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 688,\n",
       "   'probabilities': {0: 0.16002329,\n",
       "    1: 0.16002654,\n",
       "    2: 0.16002786,\n",
       "    3: 0.16001807,\n",
       "    4: 0.3599042}},\n",
       "  {'document': 689,\n",
       "   'probabilities': {0: 0.13335994,\n",
       "    1: 0.1333633,\n",
       "    2: 0.29993206,\n",
       "    3: 0.13335371,\n",
       "    4: 0.29999098}},\n",
       "  {'document': 690,\n",
       "   'probabilities': {0: 0.13335845,\n",
       "    1: 0.29989377,\n",
       "    2: 0.13336122,\n",
       "    3: 0.30002746,\n",
       "    4: 0.1333591}},\n",
       "  {'document': 691,\n",
       "   'probabilities': {0: 0.11438224,\n",
       "    1: 0.2573751,\n",
       "    2: 0.3995127,\n",
       "    3: 0.11435236,\n",
       "    4: 0.1143776}},\n",
       "  {'document': 692,\n",
       "   'probabilities': {0: 0.16000682,\n",
       "    1: 0.16000745,\n",
       "    2: 0.16000788,\n",
       "    3: 0.3599709,\n",
       "    4: 0.16000691}},\n",
       "  {'document': 693,\n",
       "   'probabilities': {0: 0.35985088,\n",
       "    1: 0.16004117,\n",
       "    2: 0.16004239,\n",
       "    3: 0.16002794,\n",
       "    4: 0.16003759}},\n",
       "  {'document': 694,\n",
       "   'probabilities': {0: 0.16003597,\n",
       "    1: 0.16004206,\n",
       "    2: 0.16004154,\n",
       "    3: 0.35984394,\n",
       "    4: 0.16003646}},\n",
       "  {'document': 695, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 696,\n",
       "   'probabilities': {0: 0.16004339,\n",
       "    1: 0.35982665,\n",
       "    2: 0.16004917,\n",
       "    3: 0.16003431,\n",
       "    4: 0.16004652}},\n",
       "  {'document': 697,\n",
       "   'probabilities': {0: 0.39981505,\n",
       "    1: 0.11433948,\n",
       "    2: 0.11433843,\n",
       "    3: 0.11432414,\n",
       "    4: 0.2571829}},\n",
       "  {'document': 698,\n",
       "   'probabilities': {0: 0.16003585,\n",
       "    1: 0.35985678,\n",
       "    2: 0.16004238,\n",
       "    3: 0.1600279,\n",
       "    4: 0.16003706}},\n",
       "  {'document': 699, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 700,\n",
       "   'probabilities': {0: 0.19609074,\n",
       "    1: 0.1270634,\n",
       "    2: 0.081916705,\n",
       "    3: 0.24877565,\n",
       "    4: 0.34615353}},\n",
       "  {'document': 701, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 702,\n",
       "   'probabilities': {0: 0.16000682,\n",
       "    1: 0.16000745,\n",
       "    2: 0.1600079,\n",
       "    3: 0.3599709,\n",
       "    4: 0.16000693}},\n",
       "  {'document': 703,\n",
       "   'probabilities': {0: 0.11431132,\n",
       "    1: 0.11431681,\n",
       "    2: 0.25699872,\n",
       "    3: 0.40006173,\n",
       "    4: 0.11431138}},\n",
       "  {'document': 704,\n",
       "   'probabilities': {0: 0.16002251,\n",
       "    1: 0.16002645,\n",
       "    2: 0.3599099,\n",
       "    3: 0.16001804,\n",
       "    4: 0.16002312}},\n",
       "  {'document': 705,\n",
       "   'probabilities': {0: 0.13334176,\n",
       "    1: 0.13334277,\n",
       "    2: 0.13334301,\n",
       "    3: 0.13333997,\n",
       "    4: 0.46663246}},\n",
       "  {'document': 706,\n",
       "   'probabilities': {0: 0.2521099,\n",
       "    1: 0.2064218,\n",
       "    2: 0.091876216,\n",
       "    3: 0.3428647,\n",
       "    4: 0.10672743}},\n",
       "  {'document': 707,\n",
       "   'probabilities': {0: 0.22444291,\n",
       "    1: 0.35036397,\n",
       "    2: 0.10007415,\n",
       "    3: 0.10005,\n",
       "    4: 0.22506899}},\n",
       "  {'document': 708,\n",
       "   'probabilities': {0: 0.13336441,\n",
       "    1: 0.13336778,\n",
       "    2: 0.13336822,\n",
       "    3: 0.29992613,\n",
       "    4: 0.29997346}},\n",
       "  {'document': 709,\n",
       "   'probabilities': {0: 0.26299205,\n",
       "    1: 0.17984322,\n",
       "    2: 0.39711133,\n",
       "    3: 0.08002306,\n",
       "    4: 0.08003036}},\n",
       "  {'document': 710,\n",
       "   'probabilities': {0: 0.1600116,\n",
       "    1: 0.16001327,\n",
       "    2: 0.16001314,\n",
       "    3: 0.35995007,\n",
       "    4: 0.1600119}},\n",
       "  {'document': 711,\n",
       "   'probabilities': {0: 0.16000915,\n",
       "    1: 0.16001032,\n",
       "    2: 0.33677354,\n",
       "    3: 0.18319772,\n",
       "    4: 0.16000931}},\n",
       "  {'document': 712, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 713,\n",
       "   'probabilities': {0: 0.07276286,\n",
       "    1: 0.25446296,\n",
       "    2: 0.072767146,\n",
       "    3: 0.25457406,\n",
       "    4: 0.3454329}},\n",
       "  {'document': 714,\n",
       "   'probabilities': {0: 0.13335355,\n",
       "    1: 0.46658656,\n",
       "    2: 0.13335711,\n",
       "    3: 0.13334899,\n",
       "    4: 0.13335383}},\n",
       "  {'document': 715,\n",
       "   'probabilities': {0: 0.08002632,\n",
       "    1: 0.08002912,\n",
       "    2: 0.17988333,\n",
       "    3: 0.18000062,\n",
       "    4: 0.48006058}},\n",
       "  {'document': 716,\n",
       "   'probabilities': {0: 0.16002338,\n",
       "    1: 0.35990766,\n",
       "    2: 0.16002676,\n",
       "    3: 0.16001841,\n",
       "    4: 0.16002378}},\n",
       "  {'document': 717,\n",
       "   'probabilities': {0: 0.13335824,\n",
       "    1: 0.13336113,\n",
       "    2: 0.13336512,\n",
       "    3: 0.13335231,\n",
       "    4: 0.46656317}},\n",
       "  {'document': 718,\n",
       "   'probabilities': {0: 0.40364033,\n",
       "    1: 0.044472955,\n",
       "    2: 0.27549815,\n",
       "    3: 0.17657709,\n",
       "    4: 0.099811465}},\n",
       "  {'document': 719,\n",
       "   'probabilities': {0: 0.031747222,\n",
       "    1: 0.06854985,\n",
       "    2: 0.10758657,\n",
       "    3: 0.22884503,\n",
       "    4: 0.5632713}},\n",
       "  {'document': 720,\n",
       "   'probabilities': {0: 0.27993292,\n",
       "    1: 0.08003235,\n",
       "    2: 0.36692238,\n",
       "    3: 0.19308378,\n",
       "    4: 0.08002856}},\n",
       "  {'document': 721,\n",
       "   'probabilities': {0: 0.16001955,\n",
       "    1: 0.35992557,\n",
       "    2: 0.16002153,\n",
       "    3: 0.1600144,\n",
       "    4: 0.16001898}},\n",
       "  {'document': 722, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 723, 'probabilities': {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}},\n",
       "  {'document': 724,\n",
       "   'probabilities': {0: 0.114322975,\n",
       "    1: 0.1143265,\n",
       "    2: 0.114327185,\n",
       "    3: 0.39997166,\n",
       "    4: 0.25705168}},\n",
       "  {'document': 725,\n",
       "   'probabilities': {0: 0.11432513,\n",
       "    1: 0.11432967,\n",
       "    2: 0.4000392,\n",
       "    3: 0.25698125,\n",
       "    4: 0.11432474}},\n",
       "  {'document': 726,\n",
       "   'probabilities': {0: 0.45904985,\n",
       "    1: 0.13333876,\n",
       "    2: 0.14093594,\n",
       "    3: 0.13333718,\n",
       "    4: 0.13333824}},\n",
       "  {'document': 727,\n",
       "   'probabilities': {0: 0.31108192,\n",
       "    1: 0.08893962,\n",
       "    2: 0.08893916,\n",
       "    3: 0.20007172,\n",
       "    4: 0.31096762}},\n",
       "  {'document': 728,\n",
       "   'probabilities': {0: 0.08001573,\n",
       "    1: 0.28002846,\n",
       "    2: 0.08001829,\n",
       "    3: 0.38008785,\n",
       "    4: 0.17984964}},\n",
       "  {'document': 729,\n",
       "   'probabilities': {0: 0.16002886,\n",
       "    1: 0.16003358,\n",
       "    2: 0.16003494,\n",
       "    3: 0.35987356,\n",
       "    4: 0.16002902}},\n",
       "  {'document': 730,\n",
       "   'probabilities': {0: 0.19986437,\n",
       "    1: 0.12854332,\n",
       "    2: 0.05717503,\n",
       "    3: 0.48588884,\n",
       "    4: 0.12852846}},\n",
       "  {'document': 731,\n",
       "   'probabilities': {0: 0.03781725,\n",
       "    1: 0.041037973,\n",
       "    2: 0.10586463,\n",
       "    3: 0.73368084,\n",
       "    4: 0.08159928}},\n",
       "  {'document': 732,\n",
       "   'probabilities': {0: 0.042807974,\n",
       "    1: 0.030844236,\n",
       "    2: 0.4684646,\n",
       "    3: 0.27340198,\n",
       "    4: 0.1844812}},\n",
       "  {'document': 733,\n",
       "   'probabilities': {0: 0.1600057,\n",
       "    1: 0.1600062,\n",
       "    2: 0.16000651,\n",
       "    3: 0.1600043,\n",
       "    4: 0.35997733}},\n",
       "  {'document': 734,\n",
       "   'probabilities': {0: 0.16001192,\n",
       "    1: 0.16001356,\n",
       "    2: 0.16001374,\n",
       "    3: 0.16000958,\n",
       "    4: 0.3599512}},\n",
       "  {'document': 735,\n",
       "   'probabilities': {0: 0.3498976,\n",
       "    1: 0.100039065,\n",
       "    2: 0.22505398,\n",
       "    3: 0.22497432,\n",
       "    4: 0.100035004}}])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA using priors\n",
    "\n",
    "def create_eta(priors, etadict, ntopics):\n",
    "    eta = np.full(shape=(ntopics, len(etadict)), fill_value=1) # create a (ntopics, nterms) matrix and fill with 1\n",
    "    for word, topic in priors.items(): # for each word in the list of priors\n",
    "        keyindex = [index for index,term in etadict.items() if term==word] # look up the word in the dictionary\n",
    "        if (len(keyindex)>0): # if it's in the dictionary\n",
    "            eta[topic,keyindex[0]] = 1e7  # put a large number in there\n",
    "    eta = np.divide(eta, eta.sum(axis=0)) # normalize so that the probabilities sum to 1 over all topics\n",
    "    return eta\n",
    "\n",
    "apriori_original = {\n",
    "    'war':0,'peace':0,'military':0,\n",
    "    'tax':1,'cut':1,'business':1, \n",
    "    'economy':2, 'employment':2, 'growth':2, 'unemployment':2,\n",
    "    'virus':3, 'pandemic':3, 'virus':3,'coronavirus':3,\n",
    "    'media':4, 'news':4\n",
    "}\n",
    "\n",
    "eta = create_eta(apriori_original, dictionary, 5)\n",
    "test_eta(eta, dictionary, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d858593",
   "metadata": {},
   "source": [
    "### 4B) SENTIMENT ANALYSIS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b716197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "# instructions\n",
    "# pip install spacytextblob\n",
    "# python3 -m textblob.download_corpora\n",
    "# python3 -m spacy download en_core_web_sm\n",
    "# pip install spacytextblob\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "texts = df['text_preproc']\n",
    "polarities = []\n",
    "subjectivities = []\n",
    "\n",
    "for text in texts:\n",
    "    doc = nlp(text)\n",
    "    polarity = doc._.blob.polarity\n",
    "    polarities.append(polarity)\n",
    "    subjectivity = doc._.blob.subjectivity\n",
    "    subjectivities.append(subjectivity)\n",
    "\n",
    "df['sentiment'] = polarities\n",
    "df['subjectivity'] = subjectivities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "cbb1e401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0380952380952381,\n",
       " -0.10208333333333333,\n",
       " 0.010822510822510817,\n",
       " 0.0,\n",
       " 0.08484848484848484,\n",
       " -0.005858134920634924,\n",
       " 0.0,\n",
       " 0.15,\n",
       " 0.0,\n",
       " -0.07556689342403626,\n",
       " 0.3111111111111111,\n",
       " -0.09523809523809525,\n",
       " -0.12600250626566414,\n",
       " 0.0,\n",
       " 0.23129251700680273,\n",
       " -0.09206349206349207,\n",
       " 0.0,\n",
       " -0.13268170426065165,\n",
       " -0.05238095238095238,\n",
       " -0.15555555555555556,\n",
       " 0.1461309523809524,\n",
       " -0.09531250000000001,\n",
       " 0.6,\n",
       " -0.13333333333333333,\n",
       " -0.03336309523809524,\n",
       " 0.17417184265010355,\n",
       " 0.0,\n",
       " 0.0058730158730158745,\n",
       " 0.0,\n",
       " -0.010243055555555552,\n",
       " 0.051713124274099874,\n",
       " -0.078482350982351,\n",
       " 0.0011904761904761739,\n",
       " -0.0035714285714285865,\n",
       " -0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.17024531024531026,\n",
       " -0.007395833333333331,\n",
       " 0.0,\n",
       " -0.020199692780337945,\n",
       " 0.0,\n",
       " 0.10952380952380951,\n",
       " 0.0037946428571428714,\n",
       " 0.11499999999999996,\n",
       " 0.125,\n",
       " 0.0,\n",
       " -0.15000000000000002,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.25,\n",
       " 0.5,\n",
       " -0.16666666666666666,\n",
       " 0.0,\n",
       " -0.05,\n",
       " 0.01666666666666667,\n",
       " -0.125,\n",
       " 0.0,\n",
       " -0.2125,\n",
       " -0.025,\n",
       " 0.08750000000000002,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.03571428571428571,\n",
       " 0.0,\n",
       " -0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.31249999999999994,\n",
       " -0.6,\n",
       " -0.04999999999999999,\n",
       " 0.5,\n",
       " -0.10571428571428572,\n",
       " 0.14019607843137255,\n",
       " -0.16666666666666666,\n",
       " 0.13823529411764707,\n",
       " -0.16666666666666666,\n",
       " 0.0,\n",
       " 0.22727272727272727,\n",
       " -0.195,\n",
       " 0.05625,\n",
       " -0.016666666666666666,\n",
       " 0.21000000000000002,\n",
       " -0.125,\n",
       " 0.0,\n",
       " 0.16,\n",
       " 0.018571428571428572,\n",
       " 0.2,\n",
       " 0.0,\n",
       " -0.05555555555555555,\n",
       " -0.06904761904761905,\n",
       " 0.006111111111111111,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.35,\n",
       " 0.0,\n",
       " 0.2683333333333333,\n",
       " 0.0,\n",
       " 0.4666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05,\n",
       " -0.15000000000000002,\n",
       " 0.0,\n",
       " -0.057727272727272724,\n",
       " -0.1111111111111111,\n",
       " -0.050427350427350436,\n",
       " -0.32222222222222224,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.13843537414965987,\n",
       " 0.042187499999999996,\n",
       " 0.3,\n",
       " 0.3,\n",
       " 0.0,\n",
       " 0.375,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.12083333333333335,\n",
       " 0.3125,\n",
       " 0.0,\n",
       " 0.375,\n",
       " -0.05,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.047333333333333324,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.14375,\n",
       " -0.21000000000000002,\n",
       " -0.1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.08333333333333333,\n",
       " -0.014814814814814836,\n",
       " 0.0,\n",
       " -0.03333333333333333,\n",
       " -0.09569327731092438,\n",
       " 0.0,\n",
       " -0.07777777777777778,\n",
       " 0.0,\n",
       " -0.35,\n",
       " -0.07142857142857144,\n",
       " -0.03125,\n",
       " 0.0,\n",
       " -0.16666666666666666,\n",
       " 0.0,\n",
       " -0.3333333333333333,\n",
       " 0.1,\n",
       " 0.16142857142857142,\n",
       " -0.09166666666666667,\n",
       " 0.375,\n",
       " 0.4,\n",
       " -0.4833333333333333,\n",
       " -0.125,\n",
       " 0.0,\n",
       " -0.25,\n",
       " 0.2,\n",
       " 0.4,\n",
       " -0.25,\n",
       " -0.2722222222222222,\n",
       " 0.25,\n",
       " -0.04126984126984126,\n",
       " -0.25,\n",
       " -0.35,\n",
       " -0.03333333333333333,\n",
       " 0.13636363636363635,\n",
       " 0.0,\n",
       " -0.05,\n",
       " -0.18333333333333335,\n",
       " 0.18333333333333335,\n",
       " 0.0,\n",
       " 0.1538095238095238,\n",
       " 0.3,\n",
       " 0.0,\n",
       " 0.016666666666666663,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.9,\n",
       " -0.07333333333333335,\n",
       " -0.08125000000000004,\n",
       " 0.11214285714285714,\n",
       " -0.38020833333333337,\n",
       " 0.4,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.35,\n",
       " 0.2,\n",
       " 0.04166666666666666,\n",
       " 0.0646103896103896,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.15,\n",
       " -0.0982142857142857,\n",
       " 0.0,\n",
       " 0.43333333333333335,\n",
       " -0.09999999999999999,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.375,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.2857142857142857,\n",
       " -0.030000000000000006,\n",
       " 0.4333333333333333,\n",
       " -0.3035714285714286,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.05000000000000001,\n",
       " 0.12937500000000002,\n",
       " -0.16666666666666666,\n",
       " -0.1,\n",
       " 0.0,\n",
       " -0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.016666666666666663,\n",
       " 0.2857142857142857,\n",
       " -0.125,\n",
       " -0.1,\n",
       " 0.0,\n",
       " -0.03333333333333333,\n",
       " -0.46296296296296297,\n",
       " 0.0,\n",
       " 0.05333333333333332,\n",
       " -0.23125,\n",
       " 0.4,\n",
       " 0.10714285714285714,\n",
       " 0.0,\n",
       " 0.23541666666666666,\n",
       " -0.06296296296296296,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.06904761904761905,\n",
       " 0.0,\n",
       " 0.4,\n",
       " -0.08333333333333333,\n",
       " -0.05,\n",
       " 0.003911564625850342,\n",
       " -0.008333333333333331,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4583333333333333,\n",
       " -0.16666666666666666,\n",
       " 0.22727272727272727,\n",
       " 0.0,\n",
       " -0.75,\n",
       " 0.05,\n",
       " -0.25,\n",
       " -0.04857142857142858,\n",
       " 0.5,\n",
       " 0.043452380952380944,\n",
       " 0.0,\n",
       " 0.15000000000000002,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.012857142857142855,\n",
       " 0.0,\n",
       " 0.27777777777777773,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07500000000000001,\n",
       " -0.11597222222222221,\n",
       " 0.16,\n",
       " -0.06666666666666667,\n",
       " 0.125,\n",
       " 0.5,\n",
       " 0.275,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.1,\n",
       " 0.12962962962962965,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.14285714285714285,\n",
       " 0.35119047619047616,\n",
       " -0.5833333333333333,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.17,\n",
       " 0.09444444444444443,\n",
       " -0.4,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.3,\n",
       " 0.25,\n",
       " 0.0,\n",
       " -0.05,\n",
       " 0.0,\n",
       " 0.09897435897435897,\n",
       " 0.014285714285714287,\n",
       " 0.0,\n",
       " -0.10694444444444441,\n",
       " 0.019047619047619067,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.15,\n",
       " -0.15,\n",
       " 0.011111111111111118,\n",
       " -0.26666666666666666,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.14583333333333331,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.3,\n",
       " 0.1,\n",
       " 0.0125,\n",
       " 0.0,\n",
       " -0.09444444444444444,\n",
       " -0.4,\n",
       " 0.10000000000000002,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.06666666666666667,\n",
       " 0.06666666666666667,\n",
       " -0.05308641975308643,\n",
       " 0.10833333333333334,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2785714285714286,\n",
       " 0.07952380952380951,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.011111111111111105,\n",
       " -0.13333333333333333,\n",
       " 0.125,\n",
       " 0.14081632653061227,\n",
       " 0.2833333333333333,\n",
       " 0.3333333333333333,\n",
       " -0.35,\n",
       " 0.1,\n",
       " 0.021428571428571422,\n",
       " -0.021666666666666667,\n",
       " 0.25,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.125,\n",
       " -0.1,\n",
       " 0.22000000000000003,\n",
       " 0.2857142857142857,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.011904761904761899,\n",
       " 0.0,\n",
       " 0.5,\n",
       " -0.23076923076923078,\n",
       " -0.06666666666666667,\n",
       " 0.19285714285714284,\n",
       " -0.03750000000000001,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.21666666666666665,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.15,\n",
       " -0.3044444444444444,\n",
       " -0.25,\n",
       " 0.05000000000000001,\n",
       " 0.0,\n",
       " -0.24375,\n",
       " 0.225,\n",
       " 0.0,\n",
       " -0.19,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.125,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.17714285714285713,\n",
       " 0.03333333333333333,\n",
       " 0.0,\n",
       " -0.21999999999999997,\n",
       " 0.007773109243697477,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.25,\n",
       " -0.3333333333333333,\n",
       " 0.0,\n",
       " 0.05,\n",
       " 0.07142857142857142,\n",
       " -0.19444444444444445,\n",
       " -0.5,\n",
       " -0.25,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.0,\n",
       " -0.18095238095238095,\n",
       " -0.05000000000000001,\n",
       " -0.6,\n",
       " -0.45,\n",
       " 0.025,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.075,\n",
       " -0.65,\n",
       " 0.08333333333333334,\n",
       " -0.11666666666666668,\n",
       " 0.10875000000000001,\n",
       " 0.021428571428571422,\n",
       " -0.0380952380952381,\n",
       " -0.5,\n",
       " -0.049999999999999996,\n",
       " 0.10000000000000002,\n",
       " 0.6,\n",
       " 0.005729166666666674,\n",
       " -0.15,\n",
       " 0.0,\n",
       " 0.1,\n",
       " -0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.29285714285714287,\n",
       " -0.3499999999999999,\n",
       " -0.007142857142857141,\n",
       " 0.29625,\n",
       " 0.6,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.06842105263157897,\n",
       " -0.25,\n",
       " -0.1,\n",
       " 0.0,\n",
       " 0.16,\n",
       " 0.05,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.16666666666666669,\n",
       " 0.0,\n",
       " -0.038095238095238106,\n",
       " 0.011111111111111108,\n",
       " 0.3125,\n",
       " 0.3,\n",
       " 0.25,\n",
       " 0.0,\n",
       " -0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.025,\n",
       " 0.1,\n",
       " 0.175,\n",
       " -0.22698412698412698,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.05,\n",
       " 0.4666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.24666666666666667,\n",
       " 0.2222222222222222,\n",
       " 0.5,\n",
       " -0.35,\n",
       " 0.016666666666666666,\n",
       " 0.027551020408163263,\n",
       " 0.15000000000000002,\n",
       " -0.09999999999999998,\n",
       " 0.0,\n",
       " 0.11041666666666666,\n",
       " 0.4,\n",
       " 0.11499999999999999,\n",
       " -0.08333333333333333,\n",
       " 0.0,\n",
       " 0.05,\n",
       " 0.0,\n",
       " 0.07083333333333333,\n",
       " -0.175,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.07142857142857142,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.3888888888888889,\n",
       " -0.08333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1375,\n",
       " 0.1,\n",
       " -0.26041666666666663,\n",
       " 0.45,\n",
       " 0.0,\n",
       " -0.7999999999999999,\n",
       " 0.2660119047619048,\n",
       " 0.0,\n",
       " 0.42857142857142855,\n",
       " 0.0,\n",
       " -0.36388888888888893,\n",
       " 0.5,\n",
       " 0.6,\n",
       " 0.0,\n",
       " -0.3,\n",
       " -0.16666666666666666,\n",
       " 0.01809523809523809,\n",
       " 0.0,\n",
       " 0.18571428571428572,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.02500000000000001,\n",
       " 0.0623015873015873,\n",
       " -0.08333333333333334,\n",
       " -0.08333333333333333,\n",
       " -0.25,\n",
       " 0.4,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.16666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.15,\n",
       " -0.6,\n",
       " 0.5,\n",
       " -0.15,\n",
       " 0.05714285714285714,\n",
       " 0.4166666666666667,\n",
       " 0.0,\n",
       " -0.0875,\n",
       " -0.2777777777777778,\n",
       " 0.2857142857142857,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.12,\n",
       " 0.10119047619047618,\n",
       " 0.07142857142857142,\n",
       " 0.037500000000000006,\n",
       " 0.0,\n",
       " 0.6,\n",
       " -0.29583333333333334,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.04183673469387755,\n",
       " -0.025,\n",
       " -0.5375,\n",
       " -0.0625,\n",
       " -0.11904761904761904,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -0.16666666666666666,\n",
       " 0.0,\n",
       " -0.12083333333333335,\n",
       " 0.010476190476190477,\n",
       " 0.2857142857142857,\n",
       " -0.25,\n",
       " 0.0,\n",
       " -0.1,\n",
       " 0.08333333333333333,\n",
       " 0.375,\n",
       " -0.16666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.2722222222222222,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.005553075396825397,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10277777777777779,\n",
       " 0.3,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.14166666666666666,\n",
       " -0.043371212121212116,\n",
       " 0.08266666666666667,\n",
       " 0.3333333333333333,\n",
       " 0.26761904761904765,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.2729166666666667,\n",
       " -0.011538461538461536,\n",
       " -0.05555555555555557,\n",
       " 0.015277777777777784,\n",
       " -0.01333333333333333,\n",
       " 0.022312925170068026,\n",
       " -0.03909632034632034,\n",
       " 0.13333333333333333,\n",
       " 0.017857142857142873,\n",
       " 0.0,\n",
       " -0.17500000000000002,\n",
       " -0.3,\n",
       " 0.04900181214134703,\n",
       " 0.2,\n",
       " -0.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.16517857142857142,\n",
       " -0.1285714285714286,\n",
       " -0.027141203703703695,\n",
       " -0.10285714285714287,\n",
       " 0.0,\n",
       " -0.107703081232493,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.08333333333333333,\n",
       " 0.0,\n",
       " 0.09285714285714287,\n",
       " 0.12430555555555556,\n",
       " 0.0,\n",
       " 0.008749999999999997,\n",
       " 0.24166666666666667,\n",
       " 0.0,\n",
       " 0.20510204081632652,\n",
       " -0.029999999999999992,\n",
       " 0.5416666666666666,\n",
       " -0.375,\n",
       " 0.0,\n",
       " 0.06881720430107527,\n",
       " 0.1964285714285714,\n",
       " 0.8,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.05036231884057973,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.016666666666666663,\n",
       " -0.001910522273425506,\n",
       " 0.0,\n",
       " -0.13333333333333333,\n",
       " -0.25,\n",
       " 0.07301587301587302,\n",
       " 0.35,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.8,\n",
       " -0.16,\n",
       " 0.0,\n",
       " -0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.15,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.053221288515406175,\n",
       " -0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.1875,\n",
       " 0.16839544513457555,\n",
       " 0.16468253968253968,\n",
       " -0.5,\n",
       " -0.13333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1285714285714286,\n",
       " 0.0,\n",
       " -0.33888888888888885,\n",
       " 0.05,\n",
       " -0.16666666666666666,\n",
       " 0.03333333333333334,\n",
       " 0.1416666666666667,\n",
       " -0.6,\n",
       " -0.05,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.5,\n",
       " -0.30000000000000004,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.09791666666666667,\n",
       " 0.0,\n",
       " -0.03560606060606061,\n",
       " -0.18095238095238095,\n",
       " -0.07916666666666668,\n",
       " -0.1,\n",
       " 0.0,\n",
       " 0.275]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "a816a928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>736.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.513025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.124815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.472604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.575759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  736.000000\n",
       "mean     0.513025\n",
       "std      0.124815\n",
       "min      0.000000\n",
       "25%      0.472604\n",
       "50%      0.500000\n",
       "75%      0.575759\n",
       "max      1.000000"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarities = [(polarity + 1) / 2 for polarity in polarities]\n",
    "polarities_df = pd.DataFrame(polarities)\n",
    "polarities_df.iloc[50:60,:]\n",
    "polarities_df.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "97cb9433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 20: administration assisted normalizing relations UAE Israel, historical event celebrated Nobel Peace US history. administration lowered cost prescription drugs. Republicans Democrats pockets pharmaceutical companies decades. started fixing effects Biden-Clinton crime bill destroyed black communities. Regardless COVID vaccine arrives, unprecedented steps taken administration buy, at-risk, hundreds millions doses instrumental 3 different entities Stage 3 trials right now. Without federal government's assistance, nearly close. turned tide manufacturing jobs pouring China. recent times ignored money large corporations ensuring offshore manufacturing jobs. progress opioid crisis, ignored federal level took office.\n",
      "Topic polarities: [0.5768518518518518]\n",
      "Document 21: 39 years us war. lmao lowest unemployment blacks hispanics history, small businesses owned blacks increased 400% 2017 2018 tax cuts,He forgave debt HBCUs affected Katrinahttps://www.washingtonpost.com/news/grade-point/wp/2018/03/15/education-department-forgives-322-million-in-loans-to-help-historically-black-colleges-recover-from-hurricanes/Program puts 800 billion minoritiy retirements.https://www.whitehouse.gov/briefings-statements/remarks-president-trump-signing-executive-order-establishing-white-house-opportunity-revitalizatioCompensation native Americans lost land 1900.https://thehill.com/changing-america/respect/diversity-inclusion/476049-trump-signed-three-bills-affecting-nativeSet legislation fight sex trafficking appointed former victim (black woman) newest member U.S. Advisory Council Human Trafficking.https://m.theepochtimes.com/trump-creates-new-position-dedicated-to-fighting-human-trafficking_3223105.htmlLowering penalty none violent crimes.https://www.nytimes.com/2018/11/14/us/politics/prison-sentencing-trump.htmlBillions help urban development led Ben Carson!https://www.politico.com/states/new-york/albany/story/2019/01/31/trump-administration-imposes-monitor-on-nycha-city-pledges-22b-over-10-years-831349Trump RESTORES funding HBCU (Historically Black Colleges Universities)https://apnews.com/c4834e48841d97c5a93312b1bf75302aHe awards Jesse Jackson work black community.\n",
      "Topic polarities: [0.45234375]\n",
      "Document 22: wars armed conflicts. started conflicts, him. succeeded bringing peace Korea east, let's honest, things worse, least Bill Clinton did.\n",
      "Topic polarities: [0.48333333333333334]\n",
      "Document 23: Biggest impact talk avoiding war Iran. US drone shot down, military planned bombing attack response. personally stopped realized strike killed dozens Iranians. rhetoric hear career politicians parties, pretty else office let military proceed strike.\n",
      "Topic polarities: [0.485]\n",
      "Document 24: Disclosure - he's idiot social policy generally can't stand deplorable ethical standards. achievements worth noting. helped broker fully normalized relations UAE Israel. big east. BBC.com/news/world-us-canada-54092960 big enough Norwegian politician nominated Nobel peace role normalized relations stated judged facts action \"on behaves sometimes.\" likely win one, I'd love leftist heads explode did. handle righteous right gloating though... Reducing dependence China matching tarrifs Chinese government increasingly expansionist flouting human rights agree with. stupid calling Xi Xinping great leader, he's maintained military pivot Asia offering assurance allied Asian nations. actions line verbal praise Chinese leadership. \"accomplishments\" bad depending political views right. instance, religious freedom policies steps backward US domestic social issues (he's anti-abortion). \"religion\" US currently synonymous Christian. needs change helped domestic side. But, international stage publicly called Nigeria killing Christians conference call Nigeria's criticized China persecution Uighur Muslims. He's consistently strong backing religious freedoms international front. https://www.reuters.com/article/us-usa-religion-un/at-un-trump-pushes-religious-freedom-at-event-slamming-china-over-uighurs-idUSKBN1W82BJ domestic social policies suck. He's divisive need. biggest weakness he's knee-jerk combative childish de-escalate. However, foreign policy gets mentioned US media unless stupid bad.\n",
      "Topic polarities: [0.49339037698412697]\n"
     ]
    }
   ],
   "source": [
    "for i in range(20,25):\n",
    "    print(f\"Document {i}: {df.body[i]}\")\n",
    "    polarities_ = polarities_df.iloc[i].tolist()\n",
    "    print(f\"Topic polarities: {polarities_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "c7351413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>733.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.756498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.062529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.735833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.787881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  733.000000\n",
       "mean     0.756498\n",
       "std      0.062529\n",
       "min      0.500000\n",
       "25%      0.735833\n",
       "50%      0.750000\n",
       "75%      0.787881\n",
       "max      1.000000"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarities_df.describe()\n",
    "\n",
    "#len(df_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ddd919",
   "metadata": {},
   "source": [
    "### 5) LOGISTIC REGRESSION ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "9eb3e325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>586.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.511387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.122879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.464237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.575762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  586.000000\n",
       "mean     0.511387\n",
       "std      0.122879\n",
       "min      0.000000\n",
       "25%      0.464237\n",
       "50%      0.500000\n",
       "75%      0.575762\n",
       "max      1.000000"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# get the list of index labels to drop from df_reg\n",
    "drop_index = df_reg[df_reg.isna().any(axis=1)].index.tolist() + polarities_df[polarities_df.isna().any(axis=1)].index.tolist()\n",
    "\n",
    "# drop the same index labels from subjectivities\n",
    "polarities_df.drop(drop_index, inplace=True)\n",
    "df_reg.drop(drop_index, inplace=True)\n",
    "\n",
    "# define X and y variables\n",
    "X = pd.DataFrame(df_reg.iloc[:, 1:].values)\n",
    "X = X.rename(columns={0: \"topic 1\", 1: \"topic 2\", 2: \"topic 3\", 3: \"topic 4\", 4: \"topic 5\", 5: \"topic 6\", 6: \"topic 7\",7:\"topic 8\"})\n",
    "y = pd.DataFrame(polarities_df.values)\n",
    "\n",
    "# split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "443cb063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.690596\n",
      "         Iterations 4\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      0   No. Observations:                  586\n",
      "Model:                          Logit   Df Residuals:                      580\n",
      "Method:                           MLE   Df Model:                            5\n",
      "Date:                Mon, 20 Feb 2023   Pseudo R-squ.:               -0.002538\n",
      "Time:                        16:22:38   Log-Likelihood:                -404.69\n",
      "converged:                       True   LL-Null:                       -403.66\n",
      "Covariance Type:            nonrobust   LLR p-value:                     1.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "topic 1        0.4310      0.789      0.547      0.585      -1.114       1.976\n",
      "topic 2        0.1234      0.579      0.213      0.831      -1.011       1.258\n",
      "topic 3        0.5377      0.734      0.732      0.464      -0.902       1.977\n",
      "topic 4       -0.1967      0.629     -0.313      0.755      -1.430       1.036\n",
      "topic 5       -0.3660      0.608     -0.602      0.547      -1.558       0.826\n",
      "topic 6       -0.1342      0.613     -0.219      0.827      -1.335       1.067\n",
      "==============================================================================\n",
      "Pseudo R-squared value: -0.002538140375217157\n",
      "Mean absolute error: 0.0883\n",
      "Mean squared error: 0.0180\n"
     ]
    }
   ],
   "source": [
    "# fit logistic regression model on the training set\n",
    "model = sm.Logit(y_train, X_train).fit()\n",
    "\n",
    "# print model summary\n",
    "print(model.summary())\n",
    "\n",
    "# predict the labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the Pseudo R-squared value using pandas\n",
    "df = pd.DataFrame({'y_true': y_test.iloc[:,0], 'y_pred': y_pred})\n",
    "#df['resid'] = df['y_true'] - df['y_pred']\n",
    "#df['resid_sq'] = df['resid'] ** 2\n",
    "\n",
    "#null_deviance = sum((y_test - y_test.mean()) ** 2)\n",
    "##print(null_deviance)\n",
    "#model_deviance = sum((y_test - y_pred) ** 2)\n",
    "#pseudo_r2 = 1 - (model_deviance / null_deviance)\n",
    "\n",
    "pseudo_r2 = 1 - (model.llf/ model.llnull)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# print performance metrics\n",
    "print(\"Pseudo R-squared value:\", pseudo_r2)\n",
    "print('Mean absolute error: {:.4f}'.format(mae))\n",
    "print('Mean squared error: {:.4f}'.format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "bc5efe66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_search.py:910: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n",
      "/var/folders/09/2tcgbyys1xg463jzt7ddwy2m0000gn/T/ipykernel_68892/2497914088.py:45: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf_best.fit(X_train, y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 0.0448\n",
      "Mean squared error: 0.0045\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# get the list of index labels to drop from df_reg\n",
    "drop_index = df_reg[df_reg.isna().any(axis=1)].index.tolist()\n",
    "\n",
    "# drop the same index labels from subjectivities\n",
    "polarities_df.drop(drop_index, inplace=True)\n",
    "df_reg.drop(drop_index, inplace=True)\n",
    "\n",
    "# define X and y variables\n",
    "X = pd.DataFrame(df_reg.iloc[:, 1:].values)\n",
    "X = X.rename(columns={0: \"topic 1\", 1: \"topic 2\", 2: \"topic 3\", 3: \"topic 4\", 4: \"topic 5\", 5: \"topic 6\"})\n",
    "y = pd.DataFrame(polarities_df.values)\n",
    "\n",
    "# split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# initialize random forest regressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# perform grid search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# get best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# initialize random forest regressor with best hyperparameters\n",
    "rf_best = RandomForestRegressor(**best_params, random_state=42)\n",
    "\n",
    "# fit the model to the training data\n",
    "rf_best.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test data\n",
    "y_pred = rf_best.predict(X_test)\n",
    "\n",
    "# calculate performance metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# print performance metrics\n",
    "print('Mean absolute error: {:.4f}'.format(mae))\n",
    "print('Mean squared error: {:.4f}'.format(mse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2e44a3",
   "metadata": {},
   "source": [
    "### 6) POSSIBLE EXTENSIONS "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
