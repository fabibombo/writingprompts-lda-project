{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3156a16",
   "metadata": {},
   "source": [
    "## Introduction to Text Mining and NLP: Term Paper\n",
    "\n",
    "---------------------------\n",
    "Ramón Talvi\n",
    "\n",
    "David Vallmanya\n",
    "\n",
    "Irene Villalonga\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc928607",
   "metadata": {},
   "source": [
    "In this term paper we will use Reddit's API to extract a huge number of texts. We will focus on a community that provides creative writing prompts to inspire users to come up with mainly short stories. The prompt represents the main post and the comments to the prompt are the stories (in our case the documents). The main idea is to find the most prevalent topics in creative writing prompts on reddit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d0387",
   "metadata": {},
   "source": [
    "### 1) WEB SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd0cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import datetime\n",
    "from praw.models import MoreComments\n",
    "\n",
    "secret_token = \"iIrB6d7nO_pdIDDqeTfc3p9l5F_xEw\"\n",
    "client_id = \"QjdGILGBn4Tu9RXu0zyPvg\"\n",
    "username = \"FabiBombo\"\n",
    "password = \"Samba2323\"\n",
    "\n",
    "reddit = praw.Reddit(client_id=client_id,\n",
    "                     client_secret=secret_token, password=password,\n",
    "                     user_agent='MyBot/0.0.1', username=username)\n",
    "\n",
    "#subreddit = reddit.subreddit('python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e828890",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_submission = 2000\n",
    "limit_comments = 800\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "subreddit = reddit.subreddit('WritingPrompts')\n",
    "\n",
    "hot_python = subreddit.top(limit=limit_submission, time_filter=\"all\")\n",
    "\n",
    "count_posts = 0\n",
    "count_comments = 0\n",
    "count_errors = 0\n",
    "\n",
    "for submission in hot_python:\n",
    "    if not submission.stickied:\n",
    "        \n",
    "        count_posts += 1\n",
    "        \n",
    "        p_awards = 0\n",
    "        for award in submission.all_awardings:\n",
    "             p_awards += 1\n",
    "        \n",
    "        for i in range(limit_comments+1):\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                if submission.comments[i] == None:\n",
    "                    continue\n",
    "                \n",
    "                if isinstance(submission.comments[i], MoreComments):\n",
    "                    continue\n",
    "                    \n",
    "                if submission.comments[i].stickied == False:\n",
    "                    \n",
    "                                  \n",
    "                    c_author = submission.comments[i].author\n",
    "                    if c_author == None: \n",
    "                        c_author = \"NA\"\n",
    "                    else:\n",
    "                        c_author = c_author.name\n",
    "                    \n",
    "                    c_replies = 0\n",
    "                    for reply in submission.comments[i].replies:\n",
    "                        c_replies += 1\n",
    "                        \n",
    "                    c_awards = 0\n",
    "                    for award in submission.comments[i].all_awardings:\n",
    "                        c_awards += 1\n",
    "                    \n",
    "                    \n",
    "                    df_comment = pd.DataFrame({\n",
    "                        'post_id': submission.id,\n",
    "                        'comment_id': submission.comments[i].id,\n",
    "                        'subreddit': submission.subreddit.display_name,\n",
    "                        'post_title': submission.title,\n",
    "                        'post_ups': submission.ups,\n",
    "                        'post_upvote_ratio': submission.upvote_ratio,\n",
    "                        'post_num_comments': submission.num_comments,\n",
    "                        'post_time': datetime.datetime.fromtimestamp(submission.created_utc),\n",
    "                        'post_downs': submission.downs,\n",
    "                        'post_score': submission.score,\n",
    "                        'post_awards': p_awards,\n",
    "                        'comment_awards': c_awards,\n",
    "                        'comment_author': c_author,\n",
    "                        'body': submission.comments[i].body,\n",
    "                        'comment_replies': c_replies,\n",
    "                        'comment_ups': submission.comments[i].ups,\n",
    "                        'comment_downs': submission.comments[i].downs,\n",
    "                        'comment_time' : datetime.datetime.fromtimestamp(submission.comments[i].created_utc),\n",
    "                        'comment_score': submission.comments[i].score\n",
    "                    }, index=[count_comments])\n",
    "                    \n",
    "                    df = pd.concat([df, df_comment])\n",
    "                    \n",
    "                    print(\"Post: \"+str(count_posts) + \"    Comment:\"+ str(count_comments) + \"    Inpost comment:\" + str(i))\n",
    "                    \n",
    "                    count_comments += 1\n",
    "                    \n",
    "            except IndexError as e:\n",
    "                count_errors += 1\n",
    "                continue\n",
    "\n",
    "print(count_errors)\n",
    "df.to_csv(\"writingprompts_extract.csv\", sep=';')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad99a606",
   "metadata": {},
   "source": [
    "### 2) PRE PROCESSING \n",
    "In this section we are going to first load and clean the Data Frame. Then we will carry out the usual steps in Text Mining preprocessing: tokenization, lemmatizing, remove punctuation, unify, remove stopwords, stemming. Finally we will obtain the Document Term Matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c74f9480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42606686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27960 entries, 0 to 27959\n",
      "Data columns (total 20 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Unnamed: 0         27960 non-null  object \n",
      " 1   post_id            27957 non-null  object \n",
      " 2   comment_id         27957 non-null  object \n",
      " 3   subreddit          27957 non-null  object \n",
      " 4   post_title         27957 non-null  object \n",
      " 5   post_ups           27957 non-null  float64\n",
      " 6   post_upvote_ratio  27956 non-null  float64\n",
      " 7   post_num_comments  27956 non-null  float64\n",
      " 8   post_time          27956 non-null  object \n",
      " 9   post_downs         27956 non-null  float64\n",
      " 10  post_score         27956 non-null  float64\n",
      " 11  post_awards        27956 non-null  float64\n",
      " 12  comment_awards     27956 non-null  float64\n",
      " 13  comment_author     24882 non-null  object \n",
      " 14  body               27956 non-null  object \n",
      " 15  comment_replies    27955 non-null  float64\n",
      " 16  comment_ups        27955 non-null  float64\n",
      " 17  comment_downs      27955 non-null  float64\n",
      " 18  comment_time       27955 non-null  object \n",
      " 19  comment_score      27955 non-null  float64\n",
      "dtypes: float64(11), object(9)\n",
      "memory usage: 4.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load dataframe\n",
    "\n",
    "readin = '/Users/irenevillalonga/Desktop/Text Mining & NLP/Term paper/'\n",
    "#spitout= '/Users/irenevillalonga/Desktop/Text Mining & NLP/Term paper/'\n",
    "filename =\"writingprompts_extract.csv\"\n",
    "\n",
    "df = pd.read_csv(os.path.join(readin, filename), sep=';', encoding='utf-8')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f35740e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 27396 entries, 0 to 27959\n",
      "Data columns (total 20 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Unnamed: 0         27396 non-null  object \n",
      " 1   post_id            27393 non-null  object \n",
      " 2   comment_id         27393 non-null  object \n",
      " 3   subreddit          27393 non-null  object \n",
      " 4   post_title         27393 non-null  object \n",
      " 5   post_ups           27393 non-null  float64\n",
      " 6   post_upvote_ratio  27392 non-null  float64\n",
      " 7   post_num_comments  27392 non-null  float64\n",
      " 8   post_time          27392 non-null  object \n",
      " 9   post_downs         27392 non-null  float64\n",
      " 10  post_score         27392 non-null  float64\n",
      " 11  post_awards        27392 non-null  float64\n",
      " 12  comment_awards     27392 non-null  float64\n",
      " 13  comment_author     24882 non-null  object \n",
      " 14  body               27392 non-null  object \n",
      " 15  comment_replies    27391 non-null  float64\n",
      " 16  comment_ups        27391 non-null  float64\n",
      " 17  comment_downs      27391 non-null  float64\n",
      " 18  comment_time       27391 non-null  object \n",
      " 19  comment_score      27391 non-null  float64\n",
      "dtypes: float64(11), object(9)\n",
      "memory usage: 4.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Remove NA\n",
    "\n",
    "df = df.loc[df['body'] != '[removed]']\n",
    "df = df.dropna(how='all')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4f09f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: [WP] The year is 1910. Adolf Hitler, a struggling artist, has fought off dozens of assasination attemps by well meaning time travelers, but this one is different. This traveller doesn't want to kill Hitler, he wants to teach him to paint. He pulls off his hood to reveal the frizzy afro of Bob Ross.\n",
      "   \n",
      "Text: There he sat, twirling his personal, stylized mustache. It was avant garde, just like he wanted to be. The man, as he was so, just wanted a place in this world for his art. He continues to stare at the easel, thinking. \n",
      "\n",
      "After a while he felt a firm, calming hand on his shoulder. He sighed, hanging his head wearily. \"Are you yet another man come to end my life, if you can even see it that way?\" The hand didn't answer, as it had no mouth. However, it's owner did, speaking the soft, assuaging tones that had come to make him famous. \n",
      "\n",
      "\"No sir. I've seen too much death and war to want to do another such thing. Instead, I have come as a tutor. Here, grab that 2 inch brush and dip it in some titanium white and prussian blue.\"\n",
      "\n",
      "Hitler did such a thing, and the man behind him nodded. \"Good. Now, mix them together, until you have a rather nice pale blue...\" \n",
      "\n",
      "Adolf did so, his brush strokes trembling across the pallette. \"Easy there tiger, try to keep yourself calm, now. Painting is all about being steady, confident.\" Adolf nodded again, and went this time, albeit a bit slower, and mixed another selection. After he had done this the stranger patted his shoulder. \n",
      "\"Good, now let's see you paint a nice, open sky.\"\n",
      "\"But how? I can barely paint the ground, let allow what lies above it!\" \n",
      "Sighing, the man grabbed a firm hold of his arm and lifted it up. \n",
      "\n",
      "\"All you have to do is nice, tiny Xs, like so..\" \n",
      "A portion, the top left hand corner was soon filled with a nice layer of blue. \n",
      "\"Now go ahead, try it.\" \n",
      "Adolf sighed and attempted this, and, to his surprise, mimicked the man's stroke almost perfectly.\n",
      "\n",
      "\"Ah! There ya go! Now, wash that brush off in your water and beat the devil out of it on your wood there...\" \n",
      "\n",
      "At this point, adolf couldn't help but turn around in surprise. \"You want me to beat my easel with it..?\" \n",
      "\n",
      "The afro'd figure behind him nodded, pulling off his woolen hood. \"Yes sir I Do. Go on. It won't hurt it.\" \n",
      "\n",
      "\"Tell me who you are first, then maybe I will...\" \n",
      "The figure smiled a bright smile, as a squirrel popped out of his hair.\n",
      "\"Why, I'm Bob. Bob Ross. And I heard you wanted to be a painter.\"\n",
      "\n",
      "------------\n",
      "Just popping in to say thank you the beautiful person who have me a gold.\n"
     ]
    }
   ],
   "source": [
    "print(\"Title:\", df.post_title[0])\n",
    "print(\"   \")\n",
    "print(\"Text:\", df.body[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6693aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto serviria si cada texto tuviera un titulo.. Como tratamos esto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b2e826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk                                                             # Natural language Toolkit\n",
    "from nltk.stem import SnowballStemmer                                   # Porter's II Stemmer\n",
    "from nltk import word_tokenize                                          # Document tokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "porter=SnowballStemmer(\"english\")\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def abbr_or_lower(word):\n",
    "    if re.match('([A-Z]+[a-z]*){2,}', word):\n",
    "        return word\n",
    "    else:\n",
    "        return word.lower()\n",
    "\n",
    "def tokenize(text, modulation):\n",
    "    tokens = re.split(r'\\W+', text)\n",
    "    stems = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        lowers=abbr_or_lower(token)\n",
    "        if lowers not in stop_words:\n",
    "            if re.search('[a-zA-Z]', lowers):\n",
    "                if modulation==1:\n",
    "                    stems.append(porter.stem(lowers))\n",
    "                if modulation==2:\n",
    "                    stems.append(lmtzr.lemmatize(lowers))\n",
    "                if modulation==0:\n",
    "                    stems.append(lowers)\n",
    "                stems.append(\" \")\n",
    "    return \"\".join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab70564b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 27396/27396 [00:05<00:00, 4586.22it/s]\n",
      "100%|████████████████████████████████████| 27396/27396 [01:13<00:00, 372.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with text plus titles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mod=1 #=1 means stemming, =2 means lemmatizing, =0 just lowercase\n",
    "tit_preproc = (\n",
    "    df.post_title\n",
    "    .astype(str)\n",
    "    .progress_apply(lambda row: tokenize(row, mod))\n",
    ")\n",
    "\n",
    "text_preproc = (\n",
    "    df.body\n",
    "    .astype(str)\n",
    "    .progress_apply(lambda row: tokenize(row, mod))\n",
    ")\n",
    "\n",
    "df_proc = df\n",
    "df_proc[\"titles_lemm\"]=tit_preproc\n",
    "df_proc[\"text_with_head_lemm\"]=text_preproc\n",
    "\n",
    "print(\"done with text plus titles\")\n",
    "\n",
    "df_proc = df_proc.dropna(subset=['titles_lemm']).reset_index(drop=True)\n",
    "\n",
    "df_proc  = df_proc[['text_with_head_lemm', 'titles_lemm']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6be5ab27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sat twirl person styliz mustach avant gard like want man want place world art continu stare easel think felt firm calm hand shoulder sigh hang head wearili yet anoth man come end life even see way hand answer mouth howev owner speak soft assuag tone come make famous sir seen much death war want anoth thing instead come tutor grab inch brush dip titanium white prussian blue hitler thing man behind nod good mix togeth rather nice pale blue adolf brush stroke trembl across pallett easi tiger tri keep calm paint steadi confid adolf nod went time albeit bit slower mix anoth select done stranger pat shoulder good let see paint nice open sky bare paint ground let allow lie sigh man grab firm hold arm lift nice tini xs like portion top left hand corner soon fill nice layer blue go ahead tri adolf sigh attempt surpris mimick man stroke almost perfect ah ya go wash brush water beat devil wood point adolf help turn around surpris want beat easel afro figur behind nod pull woolen hood yes sir go hurt tell first mayb figur smile bright smile squirrel pop hair bob bob ross heard want painter pop say thank beauti person gold \n"
     ]
    }
   ],
   "source": [
    "for article in df.text_with_head_lemm[:1]:\n",
    "    print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "672462b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Titles = df_proc.titles_lemm.tolist()\n",
    "Texts = df_proc.text_with_head_lemm.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "024cc1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/irenevillalonga/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of DT matrix are (27396, 3041945)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "cv = CountVectorizer(ngram_range = (1,2))\n",
    "cv.fit(Texts)\n",
    "vectorized_text=cv.transform(Texts)\n",
    "names=np.array(cv.get_feature_names())\n",
    "print(\"dimensions of DT matrix are\", vectorized_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea3f59fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_titles=cv.transform(Titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc4f0464",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall=vectorized_text+4*vectorized_titles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca40202e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<27396x3041945 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 12018018 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af1683d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<27396x3041945 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11246895 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df219b97",
   "metadata": {},
   "source": [
    "### 3) POST PROCESSING\n",
    "In this section we are going to use both the term frequency and the inverse document frequency to reweight the terms in the document term matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513f5eb0",
   "metadata": {},
   "source": [
    "### 4) LDA IMPLEMENTATION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d858593",
   "metadata": {},
   "source": [
    "### 5) LDA RESULT EVALUATION "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
